{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" As usual, we start by importing the relevant packages\"\"\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler # scales variables to be mean=0,sd=1\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import LassoLarsCV\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.metrics import f1_score, accuracy_score # Are not actively used, but could be a valuable extension\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Since, regularized regression, as a training method, is able to detect 'useless' variables there is little need for a \n",
    "    rigorous pre-selection of variables. This function creates interaction terms of every variable. Further, it creates \n",
    "    second-order polynomials for each variable.\"\"\"\n",
    "\n",
    "def CombineAttributes(data, var_list):\n",
    "    for i in var_list:\n",
    "        for j in var_list:\n",
    "            if i == j:\n",
    "                name = str(i)+ '_square'\n",
    "                data[name] = data.loc[:, i] * data.loc[:, i]\n",
    "\n",
    "            else:\n",
    "                name =  str(i)+ '_' +str(j)\n",
    "                data[name] = data.loc[:, i] * data.loc[:, j]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Import the relevant data and clean it\"\"\"\n",
    "\n",
    "dataframe_1 = pd.read_excel(r'C:\\Users\\mariu\\Desktop\\Project\\Historical Data\\Data_mai.xlsx')\n",
    "dataframe = dataframe_1.dropna().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Declare X and Y variable\n",
    "\n",
    "X_name = ['place_list', 'incumbent', 'woman', 'doctor', 'year', 'federal_election', 'artistocracy', 'google', 'google_zero',\n",
    "         'google_b1000', 'google_b100', 'google_million', 'population', 'share_students', 'unemployment', 'share_old','CDU', 'SPD', \n",
    "         'Linke', 'FDP', 'Grüne', 'AfD', 'share_youth', 'share_migrants', 'share_pupils', 'FW', 'local_list'\n",
    "         ]\n",
    "\n",
    "X = dataframe[['place_list', 'incumbent', 'woman', 'doctor', 'year', 'federal_election', 'artistocracy', 'google', 'google_zero', \n",
    "               'google_b1000', 'google_b100', 'google_million', 'population', 'share_students', 'unemployment', 'share_old','CDU', \n",
    "               'SPD', 'Linke', 'FDP', 'Grüne', 'AfD', 'share_youth', 'share_migrants', 'share_pupils', 'FW', 'local_list'\n",
    "              ]]\n",
    "y = dataframe['votes']\n",
    "\n",
    "\n",
    "\n",
    "# Make use of the interaction function from above\n",
    "CombineAttributes(X, X_name)\n",
    "\n",
    "\n",
    "\n",
    "# Split dataset, not necessary when predicting with the 2019 data -> train on historical data test on contemporaneous\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(    Actual     Predicted\n",
       " 0     3223   6666.117111\n",
       " 1     1822   1173.859576\n",
       " 2     1383   4325.283645\n",
       " 3      950   2195.536819\n",
       " 4     3304    746.358312\n",
       " 5     3940   2979.885279\n",
       " 6     2865   4624.280492\n",
       " 7      937    103.606557\n",
       " 8     1106   1291.268410\n",
       " 9     2605   3014.800495\n",
       " 10   19524  16212.859423\n",
       " 11    7440   4862.435291\n",
       " 12   38146  26617.361598\n",
       " 13    3442   2686.072907\n",
       " 14   41900  24032.520061\n",
       " 15   10742  14327.022289\n",
       " 16    1818   1799.039872\n",
       " 17   22724  25565.015607\n",
       " 18    6504   6233.586953\n",
       " 19   15619  13978.311911\n",
       " 20    6798   5715.078601\n",
       " 21    1897   2349.664891\n",
       " 22    4857   6333.165479\n",
       " 23    1394   2716.036356\n",
       " 24    4956   5358.145238,\n",
       " array([ 1.67183525e+01, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -1.86133121e+02, -2.82252107e+01,  0.00000000e+00, -3.20790662e-04,\n",
       "         0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -1.01023828e-02, -0.00000000e+00, -0.00000000e+00,  1.87334311e+02,\n",
       "         0.00000000e+00,  3.68554051e+02,  0.00000000e+00,  0.00000000e+00,\n",
       "        -9.23307368e+03, -0.00000000e+00,  9.58280503e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  5.13090714e+00,\n",
       "        -0.00000000e+00, -1.28466526e+02, -5.18565768e+01, -9.07423207e-02,\n",
       "         4.21462928e-01, -0.00000000e+00,  3.35189414e-06,  1.58996124e+02,\n",
       "        -1.93981085e+00, -1.87203443e-01, -0.00000000e+00, -2.99290213e-04,\n",
       "        -3.75599000e+01, -3.67888422e+00, -5.37298374e+00,  1.06900840e+02,\n",
       "         1.71216955e+02,  8.98914500e+01, -8.67444428e+01,  2.96308884e+02,\n",
       "        -1.62423432e+01, -5.59008731e-01,  4.75100790e+02, -2.13625154e+02,\n",
       "         3.99763508e+01, -1.26933298e+02, -3.70299169e+01, -0.00000000e+00,\n",
       "        -6.98893441e+01,  3.62274242e+01,  2.82588420e+00, -1.57349019e+01,\n",
       "        -0.00000000e+00, -2.07194684e-04,  0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00,  2.13150063e-02, -0.00000000e+00,\n",
       "         4.15171007e+02, -1.29958961e+02, -0.00000000e+00, -0.00000000e+00,\n",
       "         1.47346718e+02, -8.26030246e+01,  1.44765287e+02,  0.00000000e+00,\n",
       "         9.49473829e+01, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  1.30562465e+02, -2.25437888e+02,  0.00000000e+00,\n",
       "        -9.01780502e+01, -1.15362846e+00, -9.13106113e+01, -0.00000000e+00,\n",
       "         7.04585800e-05,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00,  7.27179694e-03,  2.30940564e+03, -0.00000000e+00,\n",
       "         4.43893141e+01, -3.34700343e+01, -0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -6.04255515e+00,  1.05991078e+00,\n",
       "         0.00000000e+00, -0.00000000e+00, -7.34538948e+01,  0.00000000e+00,\n",
       "         5.78313593e+01,  6.80472675e+01, -0.00000000e+00, -0.00000000e+00,\n",
       "        -2.93367400e-01,  0.00000000e+00,  0.00000000e+00, -8.97795590e-05,\n",
       "        -0.00000000e+00,  0.00000000e+00,  6.39171847e+00,  0.00000000e+00,\n",
       "         3.97405748e-03,  0.00000000e+00,  2.58474402e+01,  1.26506352e+00,\n",
       "         0.00000000e+00,  1.59443748e+00, -0.00000000e+00, -2.64997992e+02,\n",
       "        -3.96146569e+02,  0.00000000e+00,  1.30925099e+01,  0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  9.91179421e-02,\n",
       "        -3.07752365e-01, -1.02967541e+00, -4.79026237e-01, -3.27529203e-02,\n",
       "        -7.51998838e-02,  9.51855973e+00, -2.93021981e-08,  6.99612150e-01,\n",
       "         3.45554100e-01,  6.36694073e-02,  2.60534517e+00, -1.54430925e-07,\n",
       "        -1.90495113e+01, -1.63634641e-01, -2.06116860e-01,  1.42511610e+00,\n",
       "         3.01772607e+00,  4.52457659e+00,  2.13539811e+00, -4.96904587e+00,\n",
       "        -5.68067850e-01, -5.21368403e-02, -1.97420083e+01, -2.75475900e+01,\n",
       "         2.84301422e+00,  1.70163939e+00,  4.57250120e-02,  8.65594282e+01,\n",
       "         1.12168379e+02,  4.49169575e+01, -5.72965119e-03, -8.85957464e+00,\n",
       "        -0.00000000e+00,  7.65802146e-06,  2.43291424e+02,  2.55248476e+02,\n",
       "         2.87805880e+02, -2.20139451e+01,  4.12194410e-04,  2.12356522e+03,\n",
       "         2.33195151e+01,  8.24237199e+00, -6.80280681e+02, -3.09298410e+02,\n",
       "        -4.53177538e+02, -5.34703352e+02, -1.60296142e+02, -4.92561667e+03,\n",
       "         1.76218350e+01,  3.67190247e+02, -1.07837003e+03,  0.00000000e+00,\n",
       "         0.00000000e+00, -2.64743189e+01, -0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00,  7.96785907e-01, -5.36899183e+01,  0.00000000e+00,\n",
       "        -1.85306842e+01,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00, -5.33693440e-02, -0.00000000e+00, -8.47723275e+02,\n",
       "         6.92773061e+02, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -0.00000000e+00,  0.00000000e+00, -7.27570588e+02,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "         1.39828619e-07, -4.90818928e-04, -9.34152294e-05,  1.10267927e-04,\n",
       "        -2.48615201e-08, -1.48663432e-05,  1.41608663e+01,  1.81414561e-12,\n",
       "         0.00000000e+00, -3.23289595e+00, -3.42228069e+01,  1.15384486e-04,\n",
       "        -7.97504525e-10, -4.69284745e-04, -2.38064544e-05, -2.42967235e-06,\n",
       "        -1.91245604e-04,  2.76893890e-04, -3.78156294e-04,  2.88512121e-04,\n",
       "         1.48642226e-03,  5.88816621e-04, -4.57663954e-06,  7.48883674e-04,\n",
       "         7.50341336e-04,  5.28392022e-04,  5.06011254e-04, -1.42247026e+02,\n",
       "         0.00000000e+00,  0.00000000e+00, -5.79439009e+02, -9.09250270e-01,\n",
       "        -2.36701179e+02,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00, -0.00000000e+00,  0.00000000e+00, -1.03317427e-02,\n",
       "        -0.00000000e+00, -1.75769710e+01,  3.02222774e+01,  0.00000000e+00,\n",
       "        -6.22317458e+02,  0.00000000e+00,  0.00000000e+00, -4.53631082e+02,\n",
       "        -0.00000000e+00, -6.61991747e+01,  0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00, -6.64426593e+00, -1.96036123e+01,\n",
       "        -1.76263586e+02,  1.74559052e+01, -6.55173856e-02, -2.62472522e+02,\n",
       "         0.00000000e+00,  3.37300163e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  3.18780496e-03,  0.00000000e+00,\n",
       "        -0.00000000e+00,  9.00003546e+01,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -0.00000000e+00, -5.95720175e+00, -0.00000000e+00,\n",
       "        -2.76133995e+01,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00, -1.28090766e+01, -7.84037930e+02, -3.62704827e+01,\n",
       "         5.05724737e+00, -1.07282438e-01, -3.13837425e+02, -0.00000000e+00,\n",
       "         3.06172356e+01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  2.78860183e-03,  0.00000000e+00,  1.48525107e+02,\n",
       "        -1.96432617e+00,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "        -2.15784124e+02,  0.00000000e+00, -0.00000000e+00,  5.36517984e+01,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -3.69733541e+01,  0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        -2.11431550e-01, -8.32430755e+01,  0.00000000e+00, -1.13140807e-04,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "        -7.53778795e-03, -0.00000000e+00, -0.00000000e+00, -1.30618021e+02,\n",
       "        -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -2.11056003e-04,\n",
       "        -4.78989380e-03, -4.08075976e-03, -3.84186788e-03,  3.00305872e-06,\n",
       "        -4.04154887e-04,  4.16085033e-03,  2.52990547e-10,  1.07232694e-02,\n",
       "        -2.89733477e-03, -3.63396135e-03,  1.02432017e-02,  5.37606882e-09,\n",
       "         3.11518366e-01,  9.52088209e-04, -5.04908562e-04,  1.88953276e-02,\n",
       "        -2.91848395e-04, -1.55624711e-02, -4.43940258e-03,  1.91421213e-02,\n",
       "        -3.36910238e-03,  8.05652095e-05,  1.40134937e-02,  2.04405101e-01,\n",
       "        -3.11115720e-04, -1.06235347e-02, -7.51395390e+02, -0.00000000e+00,\n",
       "         1.59808990e+03,  0.00000000e+00, -2.66681459e+01,  6.79084243e+02,\n",
       "        -0.00000000e+00, -6.33618097e-04, -0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -0.00000000e+00,  7.89883003e-02, -0.00000000e+00,\n",
       "        -0.00000000e+00,  1.14977718e+03,  0.00000000e+00, -2.29763880e+04,\n",
       "         0.00000000e+00, -0.00000000e+00,  1.23363596e+04, -0.00000000e+00,\n",
       "         4.73239011e+02, -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "         3.07251260e+03, -1.88249716e+01,  0.00000000e+00, -6.07695950e+01,\n",
       "         5.88291409e+01, -4.14359883e-01,  2.18423263e+00, -4.02459318e+02,\n",
       "        -3.72112291e-05, -0.00000000e+00, -6.97332109e+01, -6.34319495e+01,\n",
       "        -0.00000000e+00, -1.17513641e-03, -5.87890503e+03,  2.97193080e+02,\n",
       "        -7.13383942e+01, -1.36623184e+03,  3.43484094e+02,  4.96535619e+01,\n",
       "         0.00000000e+00, -1.62246074e+02,  4.70429152e+03, -8.30296811e+00,\n",
       "        -3.57048734e+03, -0.00000000e+00,  1.90737344e+02,  4.13922497e+02,\n",
       "        -1.97942832e+00, -2.72320343e+01,  7.34627449e+01,  2.17120642e+01,\n",
       "         4.58719582e-01,  1.39914379e+01,  6.93746789e+02, -1.66955720e-06,\n",
       "         3.74027619e+01, -1.47742241e-01, -6.92515436e+01,  0.00000000e+00,\n",
       "        -6.02326880e-04,  1.09251159e+03,  1.38334296e+01, -7.54732999e+00,\n",
       "         1.89316980e+02, -1.68219029e+02,  5.49867662e+01, -5.42716539e+01,\n",
       "         4.01398560e+02,  5.67965983e+01, -3.91034715e+00,  2.53018681e+03,\n",
       "        -1.70581941e+03,  1.86708341e+02,  2.08930142e+01, -1.82293084e+02,\n",
       "        -7.60159502e+02, -1.35604128e+03,  3.39849772e+02, -1.83168377e+00,\n",
       "        -3.44650506e+02, -0.00000000e+00,  1.89945792e-03,  0.00000000e+00,\n",
       "         4.29834517e+02,  8.32452795e+02, -0.00000000e+00,  3.00544949e-02,\n",
       "         0.00000000e+00, -1.04617977e+02,  5.73084350e+02, -1.50580891e+03,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  1.47380678e+02,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -2.21775339e+02, -0.00000000e+00,\n",
       "        -2.53347850e+02,  0.00000000e+00,  7.38773282e-01,  1.76391371e+02,\n",
       "         0.00000000e+00,  9.91761693e-04, -0.00000000e+00,  5.96074906e+01,\n",
       "         2.36121437e+01, -0.00000000e+00,  4.22086857e-03, -0.00000000e+00,\n",
       "         7.39890447e+02, -5.80140256e+01,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -1.76582901e+02, -1.10255158e+04, -0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -3.17945086e+01,  0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00,  3.61574423e+00, -1.78028938e+02,  0.00000000e+00,\n",
       "         1.27538391e-03,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00, -1.03762602e-02,  0.00000000e+00,  2.05020169e+02,\n",
       "        -2.07404580e+01,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -2.29730840e+02,\n",
       "        -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         1.25151255e+02, -0.00000000e+00,  2.34971837e+02, -0.00000000e+00,\n",
       "         2.26751285e+00, -1.81890264e+02,  0.00000000e+00,  7.87274648e-04,\n",
       "         8.52990528e+01, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        -4.74513144e-03, -0.00000000e+00,  1.83170355e+02, -1.07022142e+02,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  2.11602250e+02, -1.14668060e+04,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -3.63237393e+02,\n",
       "         5.04596651e+02,  8.53727152e+01, -6.56185487e+01, -8.12487272e+00,\n",
       "         2.68270995e+02, -0.00000000e+00,  2.84061424e-04, -0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  1.81152619e-02,\n",
       "         2.28234122e+04,  1.91103549e+01,  5.60534625e+02,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -1.82149583e+04,\n",
       "         0.00000000e+00,  5.98126935e+02,  0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  5.13131159e+01,  0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00,  9.41597690e-01, -4.86539600e+03,\n",
       "         0.00000000e+00,  3.38668834e-04, -0.00000000e+00, -0.00000000e+00,\n",
       "        -3.07803502e+02, -0.00000000e+00, -1.97222433e-03, -0.00000000e+00,\n",
       "         5.38506909e+03, -1.52981389e+02,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "         1.50248713e+02, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  3.39265281e+00, -1.26433846e+02, -1.22305659e+00,\n",
       "         1.18426526e+01,  2.72240936e-01, -3.17922507e+00, -8.28338936e+02,\n",
       "        -7.88169314e-06, -1.03480436e+01, -6.57277746e+01,  4.76973795e+01,\n",
       "         3.63661928e+00, -4.06867342e-04,  7.94260279e+02, -6.89700882e+00,\n",
       "        -3.99151631e+00,  1.24838247e+02,  1.26851280e+02,  1.43767038e+01,\n",
       "         1.00455769e+02,  5.23433622e+01,  9.46309627e+01, -2.37213295e+00,\n",
       "         1.92442849e+02, -1.22624258e+03,  1.38049845e+01,  1.03702611e+02,\n",
       "         6.35897161e+01, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -2.53519647e+01,  1.35419709e+02, -0.00000000e+00,  8.73774943e-04,\n",
       "         0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         3.27796215e-02, -0.00000000e+00, -0.00000000e+00,  1.71751963e+03,\n",
       "         0.00000000e+00, -5.31175558e+03,  0.00000000e+00, -5.78423783e+03,\n",
       "         0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00,  5.57284928e+03,  0.00000000e+00, -5.46550833e+00,\n",
       "         0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  8.12497731e+01,\n",
       "        -9.18132311e+02, -0.00000000e+00,  4.99630747e-04, -0.00000000e+00,\n",
       "         0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  2.65290850e-01,\n",
       "         0.00000000e+00, -0.00000000e+00, -3.80429388e+02,  0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00, -1.11434091e+02, -0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00, -0.00000000e+00, -9.27304827e+00,  4.38554090e+02,\n",
       "        -0.00000000e+00,  8.10923330e+01,  3.51463295e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00,  2.79247812e-04, -1.70826975e+02, -5.18223845e+02,\n",
       "        -6.04392249e+02, -0.00000000e+00, -1.30274929e-02, -0.00000000e+00,\n",
       "         0.00000000e+00, -9.61238219e+01,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -7.24279548e+01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  2.14397480e+02,  1.58405781e+03,  0.00000000e+00,\n",
       "        -0.00000000e+00,  3.41256805e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         3.83520823e-04,  0.00000000e+00,  0.00000000e+00, -4.61715450e+02,\n",
       "        -0.00000000e+00, -1.75069753e-02,  0.00000000e+00, -9.42864509e+01,\n",
       "        -3.09363811e+02,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  2.07934041e+01,\n",
       "         0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  8.47155331e+03]),\n",
       " 0.8395037460722031)"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" Here, a simple Lasso regression. Using default value 1.\"\n",
    "\n",
    "lasso_reg = Lasso()\n",
    "lasso_reg.fit(X_train, y_train)\n",
    "lasso_pred = lasso_reg.predict(X)\n",
    "comparison_lasso = pd.DataFrame({'Actual': y, 'Predicted': lasso_pred})\n",
    "comparison_lasso.head(25), lasso_reg.coef_, lasso_reg.score(X_test, y_test) # Score is the R_sqrd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" However, it makes more sense to use cross-validation, since the parameter alpha should be selected \n",
    "    by the data. Further, normalize the data to restrict the influence of outliers.\"\"\"\n",
    "\n",
    "res_lasso_cv = LassoCV(cv=10, n_alphas=10, normalize = True).fit(X, y)\n",
    "res_lasso_cv.score(X,y), res_lasso_cv.coef_, res_lasso_cv.alpha_\n",
    "lasso_comp_pred = res_lasso_cv.predict(X_2019)\n",
    "dataframe_2019['Votes_predicted_computer_lasso'] = lasso_comp_pred # Add the totally computerized forecast to the dataset\n",
    "pred_coef_comp = res_lasso_cv.coef_\n",
    "X2 = X.columns.values\n",
    "list_coefficients = pd.DataFrame({'Variable': X2, 'Coefficient': pred_coef_comp}) # Create a dataset with the estimated coefficients\n",
    "list_coefficients.to_excel(r'C:\\Users\\mariu\\Desktop\\Project\\Coefficients.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.222e+01, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.929e+00, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.935e+00, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=5.101e+00, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=5.101e+00, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=5.101e+00, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=5.101e+00, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:337: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 21 iterations, alpha=5.097e+00, previous alpha=5.083e+00, with an active set of 16 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=2.860e+01, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.430e+01, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.430e+01, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.340e+01, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.340e+01, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=8.334e+00, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=8.286e+00, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=7.099e+00, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=7.099e+00, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=7.070e+00, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=7.070e+00, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=6.677e+00, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=6.618e+00, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:337: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 21 iterations, alpha=6.788e+00, previous alpha=6.608e+00, with an active set of 12 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=3.087e+01, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.424e+01, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.424e+01, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=7.122e+00, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=7.122e+00, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=7.122e+00, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:337: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 17 iterations, alpha=7.122e+00, previous alpha=7.116e+00, with an active set of 10 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=2.806e+01, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.329e+01, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.329e+01, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=6.693e+00, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=6.646e+00, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=6.646e+00, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=6.561e+00, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:337: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 18 iterations, alpha=6.645e+00, previous alpha=6.561e+00, with an active set of 13 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=2.861e+01, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.313e+01, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.313e+01, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=7.323e+00, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=6.810e+00, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=6.564e+00, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=6.564e+00, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=6.564e+00, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=5.962e+00, with an active set of 14 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=5.962e+00, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=5.650e+00, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=5.650e+00, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=5.650e+00, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=5.379e+00, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=4.377e+00, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=4.377e+00, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:337: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 23 iterations, alpha=4.404e+00, previous alpha=4.339e+00, with an active set of 18 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=2.974e+01, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=2.974e+01, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.487e+01, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.487e+01, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.367e+01, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:337: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 7 iterations, alpha=1.366e+01, previous alpha=1.352e+01, with an active set of 6 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=2.964e+01, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.366e+01, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.366e+01, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.366e+01, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=6.830e+00, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=6.830e+00, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=6.830e+00, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=6.830e+00, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:337: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 19 iterations, alpha=6.829e+00, previous alpha=6.800e+00, with an active set of 12 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=5.857e+01, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=3.025e+01, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.069e+01, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.069e+01, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.069e+01, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.069e+01, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.069e+01, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:337: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 11 iterations, alpha=2.069e+01, previous alpha=2.044e+01, with an active set of 10 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=2.941e+01, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=2.941e+01, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.057e+01, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.471e+01, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.370e+01, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.370e+01, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=9.717e+00, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=9.689e+00, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=8.525e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=8.473e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=7.202e+00, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=7.202e+00, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:337: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 15 iterations, alpha=7.188e+00, previous alpha=7.176e+00, with an active set of 10 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=7.277e+00, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=7.277e+00, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=7.277e+00, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=7.217e+00, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=7.153e+00, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=6.258e+00, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=6.258e+00, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=4.194e+00, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=4.194e+00, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=4.031e+00, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=4.031e+00, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=3.854e+00, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=3.854e+00, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=3.826e+00, with an active set of 21 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=3.826e+00, with an active set of 21 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:337: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 24 iterations, alpha=3.826e+00, previous alpha=3.825e+00, with an active set of 21 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=2.807e+01, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.302e+01, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.302e+01, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=7.168e+00, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=6.509e+00, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=6.509e+00, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=6.509e+00, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=6.509e+00, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=5.903e+00, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=5.477e+00, with an active set of 15 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=5.477e+00, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=5.477e+00, with an active set of 15 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=5.236e+00, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=3.937e+00, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8420038310366738,\n",
       " array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         2.61802003e-01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -1.84661363e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -4.34963490e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -1.31546032e+02, -2.14880611e+02,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  3.65023253e-02,  9.95984602e+02,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  1.72539030e-03,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -1.67908562e+01,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -1.29159885e-04,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  1.43995438e-09,  2.51577674e-02,  0.00000000e+00,\n",
       "         0.00000000e+00,  1.77143152e-03,  0.00000000e+00, -4.11509202e-03,\n",
       "         0.00000000e+00,  2.00941425e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  4.09520342e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         1.27386282e+01,  0.00000000e+00, -4.73748543e+01,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -1.80012341e+03,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00]),\n",
       " 3.825750653568829)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not really an improvement\n",
    "\n",
    "\"\"\" Using LARS algorithm while cross validate the \"alpha\" parameter. (In the original paper it's actually called lambda) \n",
    "    Not sure if that's really helpful -> check that again\"\"\"\n",
    "\n",
    "res_lasso_lars_cv = LassoLarsCV(cv=10, max_n_alphas=1000).fit(X_train, y_train)\n",
    "res_lasso_lars_cv.score(X,y), res_lasso_lars_cv.coef_, res_lasso_lars_cv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   43.9s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:  5.8min finished\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:465: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'estimator__alpha': 63.153874537438504},\n",
       " 0.7629899526844288,\n",
       "       Actual    Prediction\n",
       " 5162     957   1776.311131\n",
       " 1396    1744   3247.999899\n",
       " 7352    1152    431.303231\n",
       " 1140   25158  17823.554192\n",
       " 2299     639   1405.778216\n",
       " 2755    2254   1671.826822\n",
       " 5507    1452   1576.333755\n",
       " 384     4119   2416.135917\n",
       " 5439    4841   4124.611075\n",
       " 1841    3797   1468.528702\n",
       " 5705    4364   2526.601034\n",
       " 4148    1488  -1593.540934\n",
       " 3755   20091  20217.296976\n",
       " 6946    1377   2031.063490\n",
       " 3880    6361   7891.312754\n",
       " 474     2396   1517.840525\n",
       " 1818    1258   1646.511947\n",
       " 5617    6732   6563.504691\n",
       " 2327    2051   5343.491053\n",
       " 6205    7085  12553.029336\n",
       " 3808   12387  17058.663158\n",
       " 5071     237    560.164591\n",
       " 444     7678   1348.811910\n",
       " 5368    1241   1623.801744\n",
       " 2403    6656  11992.172352\n",
       " 2554    1365   3525.695513\n",
       " 3113   17544  16533.857953\n",
       " 7424     851  -3089.165220\n",
       " 5900   15574  11871.280047\n",
       " 4253    1472   3892.319393\n",
       " 7577    2612  17684.184817\n",
       " 6838    2269   2121.256352\n",
       " 3144   35869  31738.249549\n",
       " 221     1584   3020.121282\n",
       " 4632   30602  29986.249877)"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Now, I'm trying to manualize the process. First, a grid of possible alpha(lambda) values is constructed. Standardize and regress \n",
    "    in a pipeline. Basically, this means you always standardize automatically before regressing when calling the pipeline. \n",
    "    Next, apply grid search to tune the hyperparameter alpha. Fit trainings data\"\"\"\n",
    "\n",
    "param_grid = {'estimator__alpha': np.logspace(.001, 3, num=20, endpoint=False)}\n",
    "\n",
    "lasso_pipe = Pipeline([('scale', StandardScaler()), ('estimator', Lasso())])\n",
    "\n",
    "lin_cv = GridSearchCV(estimator=lasso_pipe,\n",
    "                      param_grid = param_grid,\n",
    "                      n_jobs=-1,\n",
    "                      verbose=2,\n",
    "                      cv=10)\n",
    "lin_cv.fit(X_train, y_train)\n",
    "final_prediction = lin_cv.best_estimator_.predict(X_test)\n",
    "comparison_prediction = pd.DataFrame({'Actual': y_test, 'Prediction': final_prediction})\n",
    "lin_cv.best_params_, lin_cv.best_score_, comparison_prediction.head(35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                       Variable  Coefficient\n",
       " 0                    place_list    -0.000000\n",
       " 1                     incumbent     0.000000\n",
       " 2                         woman    -0.000000\n",
       " 3                        doctor     0.000000\n",
       " 4                          year    -0.000000\n",
       " 5              federal_election    -0.000000\n",
       " 6                  artistocracy     0.000000\n",
       " 7                        google    -0.000337\n",
       " 8                   google_zero     0.000000\n",
       " 9                  google_b1000    -0.000000\n",
       " 10                  google_b100     0.000000\n",
       " 11               google_million     0.000000\n",
       " 12                   population    -0.037683\n",
       " 13               share_students    -0.000000\n",
       " 14                 unemployment     0.000000\n",
       " 15                    share_old     0.000000\n",
       " 16                          CDU    -0.000000\n",
       " 17                          SPD     0.000000\n",
       " 18                        Linke     0.000000\n",
       " 19                          FDP     0.000000\n",
       " 20                        Grüne    -0.000000\n",
       " 21                          AfD     0.000000\n",
       " 22                  share_youth     0.000000\n",
       " 23               share_migrants     0.000000\n",
       " 24                 share_pupils    -0.000000\n",
       " 25                           FW     0.000000\n",
       " 26                   local_list     0.000000\n",
       " 27            place_list_square     4.882465\n",
       " 28         place_list_incumbent   -38.175260\n",
       " 29             place_list_woman     0.000000\n",
       " 30            place_list_doctor     0.000000\n",
       " 31              place_list_year    -0.185969\n",
       " 32  place_list_federal_election    -6.774981\n",
       " 33      place_list_artistocracy     0.000000\n",
       " 34            place_list_google     0.000003\n",
       " 35       place_list_google_zero     4.421230\n",
       " 36      place_list_google_b1000    -0.000000\n",
       " 37       place_list_google_b100    -0.000000\n",
       " 38    place_list_google_million    -0.000000\n",
       " 39        place_list_population     0.000073\n",
       " 40    place_list_share_students   -56.558539\n",
       " 41      place_list_unemployment     5.191469\n",
       " 42         place_list_share_old    -1.749929\n",
       " 43               place_list_CDU   -36.731534\n",
       " 44               place_list_SPD   -19.353947\n",
       " 45             place_list_Linke    35.723236\n",
       " 46               place_list_FDP    20.544319\n",
       " 47             place_list_Grüne   -62.761407\n",
       " 48               place_list_AfD    -0.000000\n",
       " 49       place_list_share_youth     4.443510\n",
       " 50    place_list_share_migrants     0.000000\n",
       " 51      place_list_share_pupils     0.000000\n",
       " 52                place_list_FW     0.000000\n",
       " 53        place_list_local_list     0.000000\n",
       " 54         incumbent_place_list    -0.000000\n",
       " 55             incumbent_square     0.000000\n",
       " 56              incumbent_woman    -0.000000\n",
       " 57             incumbent_doctor     0.000000\n",
       " 58               incumbent_year     3.780319\n",
       " 59   incumbent_federal_election     0.000000,\n",
       " array([  5816.31039751,    768.73050396,   3055.67729733, ...,\n",
       "        -10470.20931433, -10743.41362987,  -7742.96335792]))"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" Doing what Pipeline does by hand to retrieve the exact coefficients\"\n",
    "\n",
    "X_train_scaled = StandardScaler().fit_transform(X_train)\n",
    "lasso_regress = Lasso(alpha = 63.153874537438504) # from the Grid search before\n",
    "lasso_regress.fit(X_train, y_train)\n",
    "pred_coef = lasso_regress.coef_\n",
    "X2 = X.columns.values\n",
    "list_coefficients = pd.DataFrame({'Variable': X2, 'Coefficient': pred_coef})\n",
    "list_coefficients.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'estimator__alpha': 708.027294346627},\n",
       " array([6711.89434254, 6664.10284244, 6616.31134234, ...,  941.48416104,\n",
       "         896.94224829,  852.40033555]))"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" We're now turning to make the 2019 (out-of-sample) prediction. We train on the entire historical dataset and predict on\n",
    "    the new data. Old data: still X and y\"\"\"\n",
    "\n",
    "\n",
    "dataframe_2019 = pd.read_excel(r'C:\\Users\\mariu\\Desktop\\Project\\2019_Data\\Data_BW_2019.xlsx')\n",
    "dataframe_2019 = dataframe_2019\n",
    "X_2019 = dataframe_2019[['place_list', 'incumbent', 'woman', 'doctor', 'year', 'federal_election', 'artistocracy', 'google', 'google_zero', \n",
    "               'google_b1000', 'google_b100', 'google_million', 'population', 'share_students', 'unemployment', 'share_old','CDU', \n",
    "               'SPD', 'Linke', 'FDP', 'Grüne', 'AfD', 'share_youth', 'share_migrants', 'share_pupils', 'FW', 'local_list'\n",
    "              ]]\n",
    "\n",
    "X_2019_2 = X_2019.dropna()\n",
    "\n",
    "CombineAttributes(X_2019_2, X_name)\n",
    "\n",
    "#lin_cv.fit(X, y) # Use all historic data\n",
    "\n",
    "final_prediction = lin_cv.best_estimator_.predict(X_2019_2) # predict with new set of variables\n",
    "lin_cv.best_params_, final_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Lastly, we collect the estimations and generate the data set\"\"\"\n",
    "\n",
    "\n",
    "X_2019_2['votes'] = final_prediction\n",
    "#X_2019_2.to_excel(r'C:\\Users\\mariu\\Desktop\\Project\\Lasso_predictions_whole_data.xlsx')\n",
    "data_w_pred = pd.merge(dataframe_2019, X_2019_2, left_index=True, right_index=True)\n",
    "final_data = pd.DataFrame({'pred_votes': data_w_pred['votes'], 'Name': data_w_pred['Name_total'],\n",
    "                          'city': data_w_pred['city'], 'party': data_w_pred['party'], 'place_list': dataframe_2019['place_list']})\n",
    "final_data.to_excel(r'C:\\Users\\mariu\\Desktop\\Project\\Lasso_prediction.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_w_pred.to_excel(r'C:\\Users\\mariu\\Desktop\\Project\\Lasso_prediction_full_set.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

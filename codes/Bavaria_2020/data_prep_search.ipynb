{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.parse import quote\n",
    "from time import sleep\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayern_hist = pd.read_excel(r'C:\\Users\\mariu\\Documents\\Project Local Elections\\Bayern_historisch.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayern_hist_2 = bayern_hist.drop(['Unnamed: 6', 'Unnamed: 7', 'Unnamed: 8', 'Unnamed: 9'], 1)\n",
    "bayern_hist_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayern_hist_2['Doctor'] = bayern_hist_2['Name'].str.contains('Dr.', regex=False)\n",
    "bayern_hist_2.iloc[:,1:] = bayern_hist_2.iloc[:,1:].replace({True:1,False:0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_name(data):\n",
    "    \n",
    "    \"\"\" Here, we are going to separate\n",
    "        first and last name as well as any title \"\"\"\n",
    "\n",
    "    new = data['Name'].str.split(\",\", expand = True)\n",
    "    data[\"Nachname\"]= new[0]\n",
    "    data[\"Vorname\"]= new[1]\n",
    "    data.Nachname = data.Nachname.str.replace('Dr.', ' ')\n",
    "\n",
    "pre_name(bayern_hist_2)\n",
    "\n",
    "def combine_name(data):\n",
    "\n",
    "    \"\"\" Combine separate columns to get googleable name \"\"\"\n",
    "\n",
    "    return data.assign(Name_compl=data.agg('{0[Vorname]} {0[Nachname]}'.format, axis=1))\n",
    "\n",
    "# Split dataframe to make waiting times bearable\n",
    "bayern_hist_3 = combine_name(bayern_hist_2)\n",
    "bayern_hist_3['Jahr'] = bayern_hist_3['Jahr'].apply(str)\n",
    "bayern_hist_4 = pd.DataFrame(data=bayern_hist_3[0:4000])\n",
    "bayern_hist_5 = pd.DataFrame(data=bayern_hist_3[4000:8000].reset_index(drop = True))\n",
    "bayern_hist_6 = pd.DataFrame(data=bayern_hist_3[8000:12000].reset_index(drop = True))\n",
    "bayern_hist_7 = pd.DataFrame(data=bayern_hist_3[12000:16000].reset_index(drop = True))\n",
    "bayern_hist_8 = pd.DataFrame(data=bayern_hist_3[16000:].reset_index(drop = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid being blocked by Ecosia (Electronical ID)\n",
    "headers = {\n",
    "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'\n",
    "    }\n",
    "\n",
    "def beautiful_soup(names, year, city):\n",
    "    \n",
    "    \"\"\" Script to scrape ecosia on a large scale. \n",
    "        As a goal, we retrieve the number of search results.\"\"\"\n",
    "\n",
    "    results_next = []\n",
    "    new_core = []\n",
    "\n",
    "    for i in range(len(names)):\n",
    "\n",
    "        page_link = 'https://www.ecosia.org/search?q=' + names[i] + ' ' + year[i] + ' ' + city[i] # Search query\n",
    "\n",
    "        sleep(2)\n",
    "\n",
    "        page_response = requests.get(page_link, headers = headers) # Results page\n",
    "\n",
    "        page_content = BeautifulSoup(page_response.content, \"html.parser\")\n",
    "        text = page_content.get_text() # Pure text of website\n",
    "\n",
    "        core = re.findall(r\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n(.*)\", text) # Regex for search result structure\n",
    "\n",
    "        new_core.append(core[27])\n",
    "\n",
    "        results_next.append([names[i], year[i], new_core[i]])\n",
    "\n",
    "    data_google = pd.DataFrame(data=results_next, columns = ['Name_total', 'year', 'google']) # Put in Dataframe\n",
    "\n",
    "    return data_google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayern_hist_a = beautiful_soup(bayern_hist_4['Name_compl'], bayern_hist_4['Jahr'], bayern_hist_4['Stadt'])\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayern_hist_b = beautiful_soup(bayern_hist_5['Name_compl'], bayern_hist_5['Jahr'], bayern_hist_5['Stadt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayern_hist_c = beautiful_soup(bayern_hist_6['Name_compl'], bayern_hist_6['Jahr'], bayern_hist_6['Stadt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayern_hist_d = beautiful_soup(bayern_hist_7['Name_compl'], bayern_hist_7['Jahr'], bayern_hist_7['Stadt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayern_hist_e = beautiful_soup(bayern_hist_8['Name_compl'], bayern_hist_8['Jahr'], bayern_hist_8['Stadt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayern_hist_10 = bayern_hist_a.append(bayern_hist_b, ignore_index = True).append(bayern_hist_c, ignore_index = True).append(bayern_hist_d, ignore_index = True).append(bayern_hist_e, ignore_index = True)\n",
    "frames = [bayern_hist_3, bayern_hist_10]\n",
    "bayern_hist_11 = pd.concat(frames, ignore_index=True, axis=1)\n",
    "bayern_hist_11.to_excel(r'C:\\Users\\mariu\\Documents\\Project Local Elections\\bayern_historisch_search.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler # scales variables to be mean=0, sd=1\n",
    "from sklearn.pipeline import Pipeline # in case multiple things are performed at the same time\n",
    "from sklearn.linear_model import LinearRegression # Baseline\n",
    "from sklearn.ensemble import GradientBoostingRegressor # Model 3\n",
    "from sklearn.ensemble import RandomForestRegressor # Model 2\n",
    "from sklearn.linear_model import LassoCV # Model 1\n",
    "from sklearn.linear_model import Lasso # To save time and not do the Cross-Validation every time\n",
    "from sklearn.svm import LinearSVR # Model 4\n",
    "from sklearn.model_selection import GridSearchCV # Hypertuning of parameters\n",
    "from sklearn.ensemble import VotingRegressor # Ensemble estimator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Since, regularized regression, as a training method, is able to detect 'useless' variables there is little need for a \n",
    "    rigorous pre-selection of variables. This function creates interaction terms of every variable. Further, it creates \n",
    "    second-order polynomials for each variable.\"\"\"\n",
    "\n",
    "def CombineAttributes(data, var_list):\n",
    "    for i in var_list:\n",
    "        for j in var_list:\n",
    "            if i == j:\n",
    "                name = str(i)+ '_square'\n",
    "                data[name] = data.loc[:, i] * data.loc[:, i]\n",
    "\n",
    "            else:\n",
    "                name =  str(i)+ '_' +str(j)\n",
    "                data[name] = data.loc[:, i] * data.loc[:, j]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataframe = pd.read_excel(r'C:\\Users\\mariu\\Desktop\\Project\\All_Data_BW_1.xlsx')\n",
    "\n",
    "X_name = ['place_list', 'incumbent', 'woman', 'doctor', 'time', 'federal_election', 'aristocracy', 'google_stan', 'population',\n",
    "          'share_students', 'unemployment', 'share_old','CDU', 'SPD', 'Linke', 'FDP', 'Grüne', 'AfD', 'share_youth',\n",
    "          'share_migrants', 'share_pupils', 'FW', 'local_list', 'federal_difference', 'youth_list', 'green_alt_list', \n",
    "          'muslim_migrant', 'non_muslim_migrant', 'double_name', 'first_time'\n",
    "         ]\n",
    "\n",
    "# Generate training (pre-2019) and test (2019) datasets\n",
    "dataframe_test = dataframe[dataframe['year'] == 2019].reset_index().drop(['index'], axis=1)\n",
    "dataframe_train = dataframe[dataframe['year'] != 2019].dropna(subset = ['incumbent']).reset_index().drop(['index'], axis=1)\n",
    "\n",
    "X_train = dataframe_train[['place_list', 'incumbent', 'woman', 'doctor', 'time', 'federal_election', 'aristocracy', 'google_stan', 'population',\n",
    "              'share_students', 'unemployment', 'share_old','CDU', 'SPD', 'Linke', 'FDP', 'Grüne', 'AfD', 'share_youth',\n",
    "              'share_migrants', 'share_pupils', 'FW', 'local_list', 'federal_difference', 'youth_list', 'green_alt_list', \n",
    "              'muslim_migrant', 'non_muslim_migrant', 'double_name', 'first_time'\n",
    "              ]]\n",
    "\n",
    "y_train = dataframe_train['votes']\n",
    "\n",
    "X_test = dataframe_test[['place_list', 'incumbent', 'woman', 'doctor', 'time', 'federal_election', 'aristocracy', 'google_stan', 'population',\n",
    "              'share_students', 'unemployment', 'share_old','CDU', 'SPD', 'Linke', 'FDP', 'Grüne', 'AfD', 'share_youth',\n",
    "              'share_migrants', 'share_pupils', 'FW', 'local_list', 'federal_difference', 'youth_list', 'green_alt_list', \n",
    "              'muslim_migrant', 'non_muslim_migrant', 'double_name', 'first_time'\n",
    "              ]]\n",
    "\n",
    "y_test = dataframe_test['votes']\n",
    "\n",
    "rand_for_train = dataframe_train[['place_list', 'incumbent', 'woman', 'doctor', 'time', 'federal_election', 'aristocracy', 'google_stan', 'population',\n",
    "              'share_students', 'unemployment', 'share_old','CDU', 'SPD', 'Linke', 'FDP', 'Grüne', 'AfD', 'share_youth',\n",
    "              'share_migrants', 'share_pupils', 'FW', 'local_list', 'federal_difference', 'youth_list', 'green_alt_list', \n",
    "              'muslim_migrant', 'non_muslim_migrant', 'double_name', 'first_time'\n",
    "              ]]\n",
    "\n",
    "rand_for_test = dataframe_test[['place_list', 'incumbent', 'woman', 'doctor', 'time', 'federal_election', 'aristocracy', 'google_stan', 'population',\n",
    "              'share_students', 'unemployment', 'share_old','CDU', 'SPD', 'Linke', 'FDP', 'Grüne', 'AfD', 'share_youth',\n",
    "              'share_migrants', 'share_pupils', 'FW', 'local_list', 'federal_difference', 'youth_list', 'green_alt_list', \n",
    "              'muslim_migrant', 'non_muslim_migrant', 'double_name', 'first_time'\n",
    "              ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>place_list</th>\n",
       "      <th>incumbent</th>\n",
       "      <th>woman</th>\n",
       "      <th>doctor</th>\n",
       "      <th>time</th>\n",
       "      <th>federal_election</th>\n",
       "      <th>aristocracy</th>\n",
       "      <th>google_stan</th>\n",
       "      <th>population</th>\n",
       "      <th>share_students</th>\n",
       "      <th>...</th>\n",
       "      <th>first_time_share_pupils</th>\n",
       "      <th>first_time_FW</th>\n",
       "      <th>first_time_local_list</th>\n",
       "      <th>first_time_federal_difference</th>\n",
       "      <th>first_time_youth_list</th>\n",
       "      <th>first_time_green_alt_list</th>\n",
       "      <th>first_time_muslim_migrant</th>\n",
       "      <th>first_time_non_muslim_migrant</th>\n",
       "      <th>first_time_double_name</th>\n",
       "      <th>first_time_square</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.028109</td>\n",
       "      <td>154715</td>\n",
       "      <td>0.238516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095886</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.119646</td>\n",
       "      <td>154715</td>\n",
       "      <td>0.238516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095886</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.119578</td>\n",
       "      <td>154715</td>\n",
       "      <td>0.238516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095886</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.119346</td>\n",
       "      <td>154715</td>\n",
       "      <td>0.238516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095886</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.119562</td>\n",
       "      <td>154715</td>\n",
       "      <td>0.238516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095886</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.119536</td>\n",
       "      <td>154715</td>\n",
       "      <td>0.238516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095886</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>39</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.081079</td>\n",
       "      <td>154715</td>\n",
       "      <td>0.238516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095886</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.119229</td>\n",
       "      <td>154715</td>\n",
       "      <td>0.238516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095886</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.119609</td>\n",
       "      <td>154715</td>\n",
       "      <td>0.238516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095886</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.119335</td>\n",
       "      <td>154715</td>\n",
       "      <td>0.238516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095886</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.063717</td>\n",
       "      <td>154715</td>\n",
       "      <td>0.238516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095886</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.119625</td>\n",
       "      <td>154715</td>\n",
       "      <td>0.238516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095886</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.102189</td>\n",
       "      <td>154715</td>\n",
       "      <td>0.238516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095886</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.069575</td>\n",
       "      <td>154715</td>\n",
       "      <td>0.238516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095886</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.068519</td>\n",
       "      <td>154715</td>\n",
       "      <td>0.238516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095886</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>41</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.119319</td>\n",
       "      <td>154715</td>\n",
       "      <td>0.238516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095886</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.024664</td>\n",
       "      <td>154715</td>\n",
       "      <td>0.238516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095886</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.119530</td>\n",
       "      <td>154715</td>\n",
       "      <td>0.238516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095886</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.062134</td>\n",
       "      <td>154715</td>\n",
       "      <td>0.238516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095886</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.045774</td>\n",
       "      <td>154715</td>\n",
       "      <td>0.238516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095886</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.119483</td>\n",
       "      <td>154715</td>\n",
       "      <td>0.238516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095886</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.032053</td>\n",
       "      <td>154715</td>\n",
       "      <td>0.238516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095886</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.851377</td>\n",
       "      <td>154715</td>\n",
       "      <td>0.238516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095886</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.096753</td>\n",
       "      <td>154715</td>\n",
       "      <td>0.238516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095886</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.119472</td>\n",
       "      <td>154715</td>\n",
       "      <td>0.238516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095886</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.050274</td>\n",
       "      <td>154715</td>\n",
       "      <td>0.238516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095886</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>42</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.119631</td>\n",
       "      <td>154715</td>\n",
       "      <td>0.238516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095886</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.169015</td>\n",
       "      <td>154715</td>\n",
       "      <td>0.238516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095886</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.119620</td>\n",
       "      <td>154715</td>\n",
       "      <td>0.238516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095886</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.119161</td>\n",
       "      <td>154715</td>\n",
       "      <td>0.238516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095886</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7532</th>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.119620</td>\n",
       "      <td>66196</td>\n",
       "      <td>0.057919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7533</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.119609</td>\n",
       "      <td>66196</td>\n",
       "      <td>0.057919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7534</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.119441</td>\n",
       "      <td>66196</td>\n",
       "      <td>0.057919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7535</th>\n",
       "      <td>13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.019387</td>\n",
       "      <td>67079</td>\n",
       "      <td>0.083111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7536</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.119551</td>\n",
       "      <td>67079</td>\n",
       "      <td>0.083111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7537</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.041024</td>\n",
       "      <td>67079</td>\n",
       "      <td>0.083111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7538</th>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.119520</td>\n",
       "      <td>67079</td>\n",
       "      <td>0.083111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7539</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.119546</td>\n",
       "      <td>67079</td>\n",
       "      <td>0.083111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7540</th>\n",
       "      <td>25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.119636</td>\n",
       "      <td>67079</td>\n",
       "      <td>0.083111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7541</th>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.119340</td>\n",
       "      <td>67079</td>\n",
       "      <td>0.083111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7542</th>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.119557</td>\n",
       "      <td>67079</td>\n",
       "      <td>0.083111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7543</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.119404</td>\n",
       "      <td>67079</td>\n",
       "      <td>0.083111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7544</th>\n",
       "      <td>19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.119525</td>\n",
       "      <td>67079</td>\n",
       "      <td>0.083111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7545</th>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.089207</td>\n",
       "      <td>67079</td>\n",
       "      <td>0.083111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7546</th>\n",
       "      <td>22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.199096</td>\n",
       "      <td>67079</td>\n",
       "      <td>0.083111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7547</th>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.119520</td>\n",
       "      <td>67079</td>\n",
       "      <td>0.083111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7548</th>\n",
       "      <td>24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.119620</td>\n",
       "      <td>67079</td>\n",
       "      <td>0.083111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7549</th>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.083402</td>\n",
       "      <td>67079</td>\n",
       "      <td>0.083111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7550</th>\n",
       "      <td>27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.119578</td>\n",
       "      <td>67079</td>\n",
       "      <td>0.083111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7551</th>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.119493</td>\n",
       "      <td>67079</td>\n",
       "      <td>0.083111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7552</th>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.119451</td>\n",
       "      <td>67079</td>\n",
       "      <td>0.083111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7553</th>\n",
       "      <td>15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.119625</td>\n",
       "      <td>67079</td>\n",
       "      <td>0.083111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7554</th>\n",
       "      <td>18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.119546</td>\n",
       "      <td>67079</td>\n",
       "      <td>0.083111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7555</th>\n",
       "      <td>17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.059495</td>\n",
       "      <td>67079</td>\n",
       "      <td>0.083111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7556</th>\n",
       "      <td>21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.061606</td>\n",
       "      <td>67079</td>\n",
       "      <td>0.083111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7557</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.119361</td>\n",
       "      <td>67079</td>\n",
       "      <td>0.083111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7558</th>\n",
       "      <td>14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.119483</td>\n",
       "      <td>67079</td>\n",
       "      <td>0.083111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7559</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.119483</td>\n",
       "      <td>67079</td>\n",
       "      <td>0.083111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7560</th>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.119435</td>\n",
       "      <td>67079</td>\n",
       "      <td>0.083111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7561</th>\n",
       "      <td>26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.119588</td>\n",
       "      <td>67079</td>\n",
       "      <td>0.083111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7562 rows × 930 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      place_list  incumbent  woman  doctor  time  federal_election  \\\n",
       "0             24        0.0      0       1     2               5.2   \n",
       "1             33        0.0      1       0     1               5.2   \n",
       "2             38        0.0      0       0     2               5.2   \n",
       "3             47        0.0      1       0     2               5.2   \n",
       "4             18        0.0      0       0     2               5.2   \n",
       "5             11        0.0      0       0     2               5.2   \n",
       "6             39        0.0      1       0     2               5.2   \n",
       "7              2        0.0      1       0     2               5.2   \n",
       "8             31        0.0      0       0     1               5.2   \n",
       "9             23        0.0      1       0     2               5.2   \n",
       "10             8        0.0      0       0     2               5.2   \n",
       "11            44        0.0      1       1     1               5.2   \n",
       "12            32        0.0      0       0     2               5.2   \n",
       "13            16        0.0      0       1     2               5.2   \n",
       "14             9        0.0      0       1     2               5.2   \n",
       "15            41        0.0      0       0     2               5.2   \n",
       "16            17        0.0      1       0     2               5.2   \n",
       "17            37        0.0      0       1     2               5.2   \n",
       "18            27        0.0      0       0     2               5.2   \n",
       "19            13        0.0      0       0     2               5.2   \n",
       "20            40        0.0      1       0     2               5.2   \n",
       "21            35        0.0      1       0     2               5.2   \n",
       "22            22        0.0      0       1     2               5.2   \n",
       "23            48        0.0      0       0     2               5.2   \n",
       "24             4        0.0      0       0     2               5.2   \n",
       "25             6        0.0      0       1     2               5.2   \n",
       "26            42        0.0      0       0     1               5.2   \n",
       "27            28        0.0      0       0     2               5.2   \n",
       "28            45        0.0      0       0     1               5.2   \n",
       "29            12        0.0      0       0     2               5.2   \n",
       "...          ...        ...    ...     ...   ...               ...   \n",
       "7532          11        0.0      0       0     1               0.0   \n",
       "7533           2        0.0      1       0     1               0.0   \n",
       "7534           6        0.0      0       0     1               0.0   \n",
       "7535          13        0.0      0       0     2               0.0   \n",
       "7536           6        0.0      0       0     2               0.0   \n",
       "7537           1        0.0      0       1     2               0.0   \n",
       "7538           8        0.0      1       0     2               0.0   \n",
       "7539           2        0.0      1       0     2               0.0   \n",
       "7540          25        0.0      1       0     2               0.0   \n",
       "7541          20        0.0      1       0     2               0.0   \n",
       "7542           9        0.0      1       0     2               0.0   \n",
       "7543           5        0.0      0       0     2               0.0   \n",
       "7544          19        0.0      0       0     2               0.0   \n",
       "7545          10        0.0      0       0     2               0.0   \n",
       "7546          22        0.0      0       1     2               0.0   \n",
       "7547          16        0.0      0       0     2               0.0   \n",
       "7548          24        0.0      0       0     2               0.0   \n",
       "7549          11        0.0      0       0     2               0.0   \n",
       "7550          27        0.0      1       0     2               0.0   \n",
       "7551           7        0.0      0       0     2               0.0   \n",
       "7552          23        0.0      1       0     2               0.0   \n",
       "7553          15        0.0      1       0     2               0.0   \n",
       "7554          18        0.0      1       0     2               0.0   \n",
       "7555          17        0.0      0       0     2               0.0   \n",
       "7556          21        0.0      1       0     2               0.0   \n",
       "7557           4        0.0      1       0     2               0.0   \n",
       "7558          14        0.0      0       0     2               0.0   \n",
       "7559           3        0.0      0       0     2               0.0   \n",
       "7560          12        0.0      0       0     2               0.0   \n",
       "7561          26        0.0      0       0     2               0.0   \n",
       "\n",
       "      aristocracy  google_stan  population  share_students  ...  \\\n",
       "0               0     0.028109      154715        0.238516  ...   \n",
       "1               0    -0.119646      154715        0.238516  ...   \n",
       "2               0    -0.119578      154715        0.238516  ...   \n",
       "3               0    -0.119346      154715        0.238516  ...   \n",
       "4               0    -0.119562      154715        0.238516  ...   \n",
       "5               0    -0.119536      154715        0.238516  ...   \n",
       "6               0    -0.081079      154715        0.238516  ...   \n",
       "7               0    -0.119229      154715        0.238516  ...   \n",
       "8               0    -0.119609      154715        0.238516  ...   \n",
       "9               0    -0.119335      154715        0.238516  ...   \n",
       "10              0    -0.063717      154715        0.238516  ...   \n",
       "11              0    -0.119625      154715        0.238516  ...   \n",
       "12              0    -0.102189      154715        0.238516  ...   \n",
       "13              0    -0.069575      154715        0.238516  ...   \n",
       "14              0    -0.068519      154715        0.238516  ...   \n",
       "15              0    -0.119319      154715        0.238516  ...   \n",
       "16              0    -0.024664      154715        0.238516  ...   \n",
       "17              0    -0.119530      154715        0.238516  ...   \n",
       "18              0    -0.062134      154715        0.238516  ...   \n",
       "19              0    -0.045774      154715        0.238516  ...   \n",
       "20              0    -0.119483      154715        0.238516  ...   \n",
       "21              0    -0.032053      154715        0.238516  ...   \n",
       "22              0     0.851377      154715        0.238516  ...   \n",
       "23              0    -0.096753      154715        0.238516  ...   \n",
       "24              0    -0.119472      154715        0.238516  ...   \n",
       "25              0     0.050274      154715        0.238516  ...   \n",
       "26              0    -0.119631      154715        0.238516  ...   \n",
       "27              0     0.169015      154715        0.238516  ...   \n",
       "28              0    -0.119620      154715        0.238516  ...   \n",
       "29              0    -0.119161      154715        0.238516  ...   \n",
       "...           ...          ...         ...             ...  ...   \n",
       "7532            0    -0.119620       66196        0.057919  ...   \n",
       "7533            0    -0.119609       66196        0.057919  ...   \n",
       "7534            0    -0.119441       66196        0.057919  ...   \n",
       "7535            0    -0.019387       67079        0.083111  ...   \n",
       "7536            0    -0.119551       67079        0.083111  ...   \n",
       "7537            0    -0.041024       67079        0.083111  ...   \n",
       "7538            0    -0.119520       67079        0.083111  ...   \n",
       "7539            0    -0.119546       67079        0.083111  ...   \n",
       "7540            0    -0.119636       67079        0.083111  ...   \n",
       "7541            0    -0.119340       67079        0.083111  ...   \n",
       "7542            0    -0.119557       67079        0.083111  ...   \n",
       "7543            0    -0.119404       67079        0.083111  ...   \n",
       "7544            0    -0.119525       67079        0.083111  ...   \n",
       "7545            0    -0.089207       67079        0.083111  ...   \n",
       "7546            0     0.199096       67079        0.083111  ...   \n",
       "7547            0    -0.119520       67079        0.083111  ...   \n",
       "7548            0    -0.119620       67079        0.083111  ...   \n",
       "7549            0    -0.083402       67079        0.083111  ...   \n",
       "7550            0    -0.119578       67079        0.083111  ...   \n",
       "7551            0    -0.119493       67079        0.083111  ...   \n",
       "7552            0    -0.119451       67079        0.083111  ...   \n",
       "7553            0    -0.119625       67079        0.083111  ...   \n",
       "7554            0    -0.119546       67079        0.083111  ...   \n",
       "7555            0    -0.059495       67079        0.083111  ...   \n",
       "7556            0    -0.061606       67079        0.083111  ...   \n",
       "7557            0    -0.119361       67079        0.083111  ...   \n",
       "7558            0    -0.119483       67079        0.083111  ...   \n",
       "7559            0    -0.119483       67079        0.083111  ...   \n",
       "7560            0    -0.119435       67079        0.083111  ...   \n",
       "7561            0    -0.119588       67079        0.083111  ...   \n",
       "\n",
       "      first_time_share_pupils  first_time_FW  first_time_local_list  \\\n",
       "0                    0.095886              0                      0   \n",
       "1                    0.095886              0                      0   \n",
       "2                    0.095886              0                      0   \n",
       "3                    0.095886              0                      0   \n",
       "4                    0.095886              0                      0   \n",
       "5                    0.095886              0                      0   \n",
       "6                    0.095886              0                      0   \n",
       "7                    0.095886              0                      0   \n",
       "8                    0.095886              0                      0   \n",
       "9                    0.095886              0                      0   \n",
       "10                   0.095886              0                      0   \n",
       "11                   0.095886              0                      0   \n",
       "12                   0.095886              0                      0   \n",
       "13                   0.095886              0                      0   \n",
       "14                   0.095886              0                      0   \n",
       "15                   0.095886              0                      0   \n",
       "16                   0.095886              0                      0   \n",
       "17                   0.095886              0                      0   \n",
       "18                   0.095886              0                      0   \n",
       "19                   0.095886              0                      0   \n",
       "20                   0.095886              0                      0   \n",
       "21                   0.095886              0                      0   \n",
       "22                   0.095886              0                      0   \n",
       "23                   0.095886              0                      0   \n",
       "24                   0.095886              0                      0   \n",
       "25                   0.095886              0                      0   \n",
       "26                   0.095886              0                      0   \n",
       "27                   0.095886              0                      0   \n",
       "28                   0.095886              0                      0   \n",
       "29                   0.095886              0                      0   \n",
       "...                       ...            ...                    ...   \n",
       "7532                 0.000000              0                      0   \n",
       "7533                 0.000000              0                      0   \n",
       "7534                 0.000000              0                      0   \n",
       "7535                 0.000000              0                      0   \n",
       "7536                 0.000000              0                      0   \n",
       "7537                 0.000000              0                      0   \n",
       "7538                 0.000000              0                      0   \n",
       "7539                 0.000000              0                      0   \n",
       "7540                 0.000000              0                      0   \n",
       "7541                 0.000000              0                      0   \n",
       "7542                 0.000000              0                      0   \n",
       "7543                 0.000000              0                      0   \n",
       "7544                 0.000000              0                      0   \n",
       "7545                 0.000000              0                      0   \n",
       "7546                 0.000000              0                      0   \n",
       "7547                 0.000000              0                      0   \n",
       "7548                 0.000000              0                      0   \n",
       "7549                 0.000000              0                      0   \n",
       "7550                 0.000000              0                      0   \n",
       "7551                 0.000000              0                      0   \n",
       "7552                 0.000000              0                      0   \n",
       "7553                 0.000000              0                      0   \n",
       "7554                 0.000000              0                      0   \n",
       "7555                 0.000000              0                      0   \n",
       "7556                 0.000000              0                      0   \n",
       "7557                 0.000000              0                      0   \n",
       "7558                 0.000000              0                      0   \n",
       "7559                 0.000000              0                      0   \n",
       "7560                 0.000000              0                      0   \n",
       "7561                 0.000000              0                      0   \n",
       "\n",
       "      first_time_federal_difference  first_time_youth_list  \\\n",
       "0                               0.2                      0   \n",
       "1                               0.2                      0   \n",
       "2                               0.2                      0   \n",
       "3                               0.2                      0   \n",
       "4                               0.2                      0   \n",
       "5                               0.2                      0   \n",
       "6                               0.2                      0   \n",
       "7                               0.2                      0   \n",
       "8                               0.2                      0   \n",
       "9                               0.2                      0   \n",
       "10                              0.2                      0   \n",
       "11                              0.2                      0   \n",
       "12                              0.2                      0   \n",
       "13                              0.2                      0   \n",
       "14                              0.2                      0   \n",
       "15                              0.2                      0   \n",
       "16                              0.2                      0   \n",
       "17                              0.2                      0   \n",
       "18                              0.2                      0   \n",
       "19                              0.2                      0   \n",
       "20                              0.2                      0   \n",
       "21                              0.2                      0   \n",
       "22                              0.2                      0   \n",
       "23                              0.2                      0   \n",
       "24                              0.2                      0   \n",
       "25                              0.2                      0   \n",
       "26                              0.2                      0   \n",
       "27                              0.2                      0   \n",
       "28                              0.2                      0   \n",
       "29                              0.2                      0   \n",
       "...                             ...                    ...   \n",
       "7532                            0.0                      0   \n",
       "7533                            0.0                      0   \n",
       "7534                            0.0                      0   \n",
       "7535                            0.0                      0   \n",
       "7536                            0.0                      0   \n",
       "7537                            0.0                      0   \n",
       "7538                            0.0                      0   \n",
       "7539                            0.0                      0   \n",
       "7540                            0.0                      0   \n",
       "7541                            0.0                      0   \n",
       "7542                            0.0                      0   \n",
       "7543                            0.0                      0   \n",
       "7544                            0.0                      0   \n",
       "7545                            0.0                      0   \n",
       "7546                            0.0                      0   \n",
       "7547                            0.0                      0   \n",
       "7548                            0.0                      0   \n",
       "7549                            0.0                      0   \n",
       "7550                            0.0                      0   \n",
       "7551                            0.0                      0   \n",
       "7552                            0.0                      0   \n",
       "7553                            0.0                      0   \n",
       "7554                            0.0                      0   \n",
       "7555                            0.0                      0   \n",
       "7556                            0.0                      0   \n",
       "7557                            0.0                      0   \n",
       "7558                            0.0                      0   \n",
       "7559                            0.0                      0   \n",
       "7560                            0.0                      0   \n",
       "7561                            0.0                      0   \n",
       "\n",
       "      first_time_green_alt_list  first_time_muslim_migrant  \\\n",
       "0                             0                          0   \n",
       "1                             0                          0   \n",
       "2                             0                          0   \n",
       "3                             0                          0   \n",
       "4                             0                          0   \n",
       "5                             0                          0   \n",
       "6                             0                          0   \n",
       "7                             0                          0   \n",
       "8                             0                          0   \n",
       "9                             0                          0   \n",
       "10                            0                          0   \n",
       "11                            0                          0   \n",
       "12                            0                          0   \n",
       "13                            0                          0   \n",
       "14                            0                          0   \n",
       "15                            0                          0   \n",
       "16                            0                          0   \n",
       "17                            0                          0   \n",
       "18                            0                          0   \n",
       "19                            0                          0   \n",
       "20                            0                          0   \n",
       "21                            0                          0   \n",
       "22                            0                          0   \n",
       "23                            0                          0   \n",
       "24                            0                          0   \n",
       "25                            0                          0   \n",
       "26                            0                          0   \n",
       "27                            0                          0   \n",
       "28                            0                          0   \n",
       "29                            0                          0   \n",
       "...                         ...                        ...   \n",
       "7532                          0                          0   \n",
       "7533                          0                          0   \n",
       "7534                          0                          0   \n",
       "7535                          0                          0   \n",
       "7536                          0                          0   \n",
       "7537                          0                          0   \n",
       "7538                          0                          0   \n",
       "7539                          0                          0   \n",
       "7540                          0                          0   \n",
       "7541                          0                          0   \n",
       "7542                          0                          0   \n",
       "7543                          0                          0   \n",
       "7544                          0                          0   \n",
       "7545                          0                          0   \n",
       "7546                          0                          0   \n",
       "7547                          0                          0   \n",
       "7548                          0                          0   \n",
       "7549                          0                          0   \n",
       "7550                          0                          0   \n",
       "7551                          0                          0   \n",
       "7552                          0                          0   \n",
       "7553                          0                          0   \n",
       "7554                          0                          0   \n",
       "7555                          0                          0   \n",
       "7556                          0                          0   \n",
       "7557                          0                          0   \n",
       "7558                          0                          0   \n",
       "7559                          0                          0   \n",
       "7560                          0                          0   \n",
       "7561                          0                          0   \n",
       "\n",
       "      first_time_non_muslim_migrant  first_time_double_name  first_time_square  \n",
       "0                                 0                       0                  1  \n",
       "1                                 0                       0                  1  \n",
       "2                                 0                       0                  1  \n",
       "3                                 0                       0                  1  \n",
       "4                                 0                       0                  1  \n",
       "5                                 0                       0                  1  \n",
       "6                                 0                       0                  1  \n",
       "7                                 0                       0                  1  \n",
       "8                                 0                       0                  1  \n",
       "9                                 0                       0                  1  \n",
       "10                                0                       0                  1  \n",
       "11                                1                       0                  1  \n",
       "12                                0                       0                  1  \n",
       "13                                0                       1                  1  \n",
       "14                                0                       0                  1  \n",
       "15                                0                       0                  1  \n",
       "16                                0                       0                  1  \n",
       "17                                0                       0                  1  \n",
       "18                                0                       0                  1  \n",
       "19                                0                       0                  1  \n",
       "20                                0                       0                  1  \n",
       "21                                0                       0                  1  \n",
       "22                                0                       0                  1  \n",
       "23                                0                       0                  1  \n",
       "24                                0                       0                  1  \n",
       "25                                0                       0                  1  \n",
       "26                                0                       0                  1  \n",
       "27                                0                       0                  1  \n",
       "28                                1                       0                  1  \n",
       "29                                0                       1                  1  \n",
       "...                             ...                     ...                ...  \n",
       "7532                              0                       0                  0  \n",
       "7533                              0                       0                  0  \n",
       "7534                              0                       0                  0  \n",
       "7535                              0                       0                  0  \n",
       "7536                              0                       0                  0  \n",
       "7537                              0                       0                  0  \n",
       "7538                              0                       0                  0  \n",
       "7539                              0                       0                  0  \n",
       "7540                              0                       0                  0  \n",
       "7541                              0                       0                  0  \n",
       "7542                              0                       0                  0  \n",
       "7543                              0                       0                  0  \n",
       "7544                              0                       0                  0  \n",
       "7545                              0                       0                  0  \n",
       "7546                              0                       0                  0  \n",
       "7547                              0                       0                  0  \n",
       "7548                              0                       0                  0  \n",
       "7549                              0                       0                  0  \n",
       "7550                              0                       0                  0  \n",
       "7551                              0                       0                  0  \n",
       "7552                              0                       0                  0  \n",
       "7553                              0                       0                  0  \n",
       "7554                              0                       0                  0  \n",
       "7555                              0                       0                  0  \n",
       "7556                              0                       0                  0  \n",
       "7557                              0                       0                  0  \n",
       "7558                              0                       0                  0  \n",
       "7559                              0                       0                  0  \n",
       "7560                              0                       0                  0  \n",
       "7561                              0                       0                  0  \n",
       "\n",
       "[7562 rows x 930 columns]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "CombineAttributes(X_train, X_name)\n",
    "CombineAttributes(X_test, X_name)\n",
    "\n",
    "X_train_scaled = StandardScaler().fit_transform(X_train)\n",
    "X_test_scaled = StandardScaler().fit(X_train).transform(X_test)\n",
    "\n",
    "X2 = X_train.columns.values\n",
    "X2_rand_for = rand_for_test.columns.values\n",
    "\n",
    "rand_for_train_scaled = StandardScaler().fit_transform(rand_for_train)\n",
    "rand_for_test_scaled = StandardScaler().fit(rand_for_train).transform(rand_for_test)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 89216805.08242798, tolerance: 59407373.10091065\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 70341091.83377075, tolerance: 59502110.65332249\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 77190877.59396362, tolerance: 56095863.05405889\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 84856028.84317017, tolerance: 59407373.10091065\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 91732687.940094, tolerance: 59009705.08597426\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 63617628.21324158, tolerance: 45112447.274434686\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66987339.516204834, tolerance: 59397810.59774071\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 113399470.30000305, tolerance: 55988288.90230411\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 72558763.11534119, tolerance: 59502110.65332249\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 89500009.18360901, tolerance: 45112447.274434686\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 101112820.45367432, tolerance: 55988288.90230411\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 63605567.74815369, tolerance: 58963311.17147437\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 69099772.56842041, tolerance: 59009705.08597426\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 373406279.6681824, tolerance: 59502110.65332249\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 141630269.79077148, tolerance: 59397810.59774071\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 94982580.42218018, tolerance: 55988288.90230411\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 342096949.5786438, tolerance: 56095863.05405889\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 226742436.05207825, tolerance: 59407373.10091065\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 200490692.43878174, tolerance: 58963311.17147437\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 70974087.27700806, tolerance: 59009705.08597426\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 667897235.3267212, tolerance: 59502110.65332249\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 315212655.00964355, tolerance: 55988288.90230411\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 736168242.4745331, tolerance: 59397810.59774071\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 704847483.153717, tolerance: 56095863.05405889\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 502840113.5686035, tolerance: 59407373.10091065\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 239054511.9725647, tolerance: 59009705.08597426\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 234143418.97735596, tolerance: 58963311.17147437\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1053099187.9407654, tolerance: 55988288.90230411\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 321736259.5024414, tolerance: 59502110.65332249\n",
      "  tol, rng, random, positive)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 813680190.5507965, tolerance: 56095863.05405889\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 313741342.6029663, tolerance: 59397810.59774071\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 464777819.4874115, tolerance: 59009705.08597426\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 359992611.33406067, tolerance: 59407373.10091065\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1143425238.461853, tolerance: 55988288.90230411\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 109549818.35217285, tolerance: 45112447.274434686\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1071243566.8338165, tolerance: 59502110.65332249\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 469784348.94589233, tolerance: 58963311.17147437\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 747994437.8272095, tolerance: 56095863.05405889\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 759440568.8494873, tolerance: 59397810.59774071\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 954273572.9441528, tolerance: 59407373.10091065\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 747134211.5046387, tolerance: 59009705.08597426\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1125835640.3001862, tolerance: 55988288.90230411\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 827029605.6582794, tolerance: 59502110.65332249\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 828479113.7351379, tolerance: 58963311.17147437\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 786244589.2960815, tolerance: 56095863.05405889\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1007529837.7081909, tolerance: 59397810.59774071\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 574321628.6977539, tolerance: 59407373.10091065\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1127344084.9884949, tolerance: 59009705.08597426\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1172973698.7330322, tolerance: 55988288.90230411\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1060833360.9862823, tolerance: 59502110.65332249\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1359633933.5281525, tolerance: 58963311.17147437\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 999305859.4251556, tolerance: 56095863.05405889\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1677198474.980896, tolerance: 59397810.59774071\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1024449429.1798096, tolerance: 59407373.10091065\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1595423944.9213104, tolerance: 59009705.08597426\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 137303076.31915283, tolerance: 57982497.17856447\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 68690891.54814148, tolerance: 57483034.06143239\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 74163255.55526733, tolerance: 59344866.12018862\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 134890094.77001953, tolerance: 57483034.06143239\n",
      "  tol, rng, random, positive)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 62878200.21333313, tolerance: 59350802.23465354\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 211643454.77069092, tolerance: 59341040.587068595\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 73551226.41983032, tolerance: 59344866.12018862\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 306098722.48742676, tolerance: 57982497.17856447\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 103083444.58935547, tolerance: 58373353.417881496\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 73458646.81794739, tolerance: 59350802.23465354\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 180494150.66519165, tolerance: 57433039.781943984\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 225645629.25778198, tolerance: 49191378.56737303\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 83926478.15559387, tolerance: 59344866.12018862\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 81174984.05737305, tolerance: 57483034.06143239\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 762565448.2585449, tolerance: 57982497.17856447\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 171594411.7828064, tolerance: 58373353.417881496\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 372177028.086853, tolerance: 59344866.12018862\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 657278777.8926239, tolerance: 57433039.781943984\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 119678243.85873413, tolerance: 49191378.56737303\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 227035809.32037354, tolerance: 59341040.587068595\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 103354760.90849304, tolerance: 57483034.06143239\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 254955479.34539795, tolerance: 59350802.23465354\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 216905956.35899353, tolerance: 58373353.417881496\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 419092601.1943512, tolerance: 57982497.17856447\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 355186503.8776703, tolerance: 59344866.12018862\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 606678995.5113525, tolerance: 59341040.587068595\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 232100431.38453674, tolerance: 57433039.781943984\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 147720756.44177246, tolerance: 49191378.56737303\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 165897097.9019165, tolerance: 57483034.06143239\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 467710761.9994812, tolerance: 59350802.23465354\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 638245237.9331665, tolerance: 58373353.417881496\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 523250919.87194824, tolerance: 57982497.17856447\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 290047628.4566803, tolerance: 59344866.12018862\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 338318692.5609131, tolerance: 59341040.587068595\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 651278951.1571045, tolerance: 57433039.781943984\n",
      "  tol, rng, random, positive)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 218632324.84446716, tolerance: 49191378.56737303\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 291939603.9486389, tolerance: 59350802.23465354\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271424880.92681885, tolerance: 57483034.06143239\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 983924642.3039398, tolerance: 57982497.17856447\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 240808438.18978882, tolerance: 58373353.417881496\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 919588532.6184998, tolerance: 59344866.12018862\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 608861900.1003418, tolerance: 59341040.587068595\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 573458118.398529, tolerance: 57433039.781943984\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 652763489.5824585, tolerance: 59350802.23465354\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 219548328.98739624, tolerance: 49191378.56737303\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 362267145.9233093, tolerance: 57483034.06143239\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 788708745.3690643, tolerance: 58373353.417881496\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1228167552.4766235, tolerance: 59344866.12018862\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1312265316.5206604, tolerance: 59341040.587068595\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 814664724.6672668, tolerance: 57433039.781943984\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1157796953.959137, tolerance: 59350802.23465354\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 773222251.6433411, tolerance: 58373353.417881496\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 465233822.3582611, tolerance: 57483034.06143239\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1395770533.4344788, tolerance: 59344866.12018862\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1572480454.4614258, tolerance: 59341040.587068595\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 863324731.2428894, tolerance: 58373353.417881496\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1143033560.4986877, tolerance: 59350802.23465354\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 62626502.1953125, tolerance: 55458408.200804114\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105383365.45761108, tolerance: 56650151.22535942\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 69639002.73730469, tolerance: 58867744.5020334\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 72091959.41062927, tolerance: 59215035.96989756\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 151031101.35160828, tolerance: 55458408.200804114\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 215080568.53781128, tolerance: 55458408.200804114\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105698959.64233398, tolerance: 59215035.96989756\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 139962167.50898743, tolerance: 56650151.22535942\n",
      "  tol, rng, random, positive)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 215886046.55136108, tolerance: 55458408.200804114\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 63076425.209121704, tolerance: 59215035.96989756\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 364861440.7958679, tolerance: 56650151.22535942\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 905429765.6066589, tolerance: 55458408.200804114\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 222570899.61727905, tolerance: 59215035.96989756\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 108554580.74859619, tolerance: 58867744.5020334\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 302416232.3404541, tolerance: 56650151.22535942\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 707724078.8943176, tolerance: 55458408.200804114\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 328376518.35798645, tolerance: 59215035.96989756\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 291052035.004364, tolerance: 58867744.5020334\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1095853870.0785065, tolerance: 56650151.22535942\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 503685065.02415466, tolerance: 55458408.200804114\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 272697422.0208435, tolerance: 59215035.96989756\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 205451063.49073792, tolerance: 58867744.5020334\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1287627833.8002014, tolerance: 56650151.22535942\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1100470169.5924683, tolerance: 55458408.200804114\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 855794584.539917, tolerance: 59215035.96989756\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 545382105.8924103, tolerance: 58867744.5020334\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1907361678.0624084, tolerance: 55458408.200804114\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 893489421.5069885, tolerance: 56650151.22535942\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1214508997.0621338, tolerance: 59215035.96989756\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 827766908.0744019, tolerance: 58867744.5020334\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2552663227.693062, tolerance: 55458408.200804114\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1282010610.5556335, tolerance: 59215035.96989756\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 952603359.5448151, tolerance: 58867744.5020334\n",
      "  tol, rng, random, positive)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7322741586238914,\n",
       " array([-3.25390411e+02,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  1.68807480e+02,\n",
       "         0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00, -0.00000000e+00,  5.82701926e+02, -0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -8.09906549e+02,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -1.41205598e+02, -0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "         1.39729596e+03,  1.47446448e+02,  9.21201253e+02,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "         3.39417094e+03,  2.92321051e+02,  3.01211582e+02,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  3.36021871e+03,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -3.81103427e+02, -4.65984311e+01,  1.55837158e+03, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        -7.74527017e+01, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00,  0.00000000e+00, -5.72984805e+02,\n",
       "        -7.81241668e+01,  1.80539841e-02, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "         1.40990638e+02, -3.39898064e+01, -0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -4.88316179e-01,\n",
       "         0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  6.86776604e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  7.42316716e+01,\n",
       "        -0.00000000e+00, -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  2.94233492e+02,\n",
       "        -0.00000000e+00, -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -8.03217923e+02,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  2.83424149e+01,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -5.18067998e-01, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "        -2.65376201e+01, -2.70158803e+01, -0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -1.08326235e+02,\n",
       "        -0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00,  1.22540583e+01,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  2.11138782e+01,  0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  4.74559810e-04,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -1.10093433e+02,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -1.34146404e+00,\n",
       "         0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  2.47157926e+01,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -1.00130575e+02, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -7.56131940e-02,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "         1.92405866e-02,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "        -2.86557233e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00]),\n",
       " 302.1091452688529)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "t1 = time.time()\n",
    "res_lasso_cv = LassoCV(cv=20, n_alphas=50, n_jobs = -1)\n",
    "res_lasso_cv.fit(X_train_scaled, y_train)\n",
    "t_lasso = time.time() - t1\n",
    "\n",
    "# Display results\n",
    "#res_lasso_cv_alphas = -np.log10(res_lasso_cv.alphas_ + EPSILON)\n",
    "\n",
    "res_lasso_cv.score(X_train_scaled, y_train), res_lasso_cv.coef_, res_lasso_cv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(      Prediction  Votes        city party                    name\n",
       " 0    6253.035992   8163    Freiburg   AfD          Bernhard Lukau\n",
       " 1    6344.340350   7711    Freiburg   AfD        Michael Theuring\n",
       " 2    6070.427278   6504    Freiburg   AfD            Elmar Ertmer\n",
       " 3    6161.731635   7703    Freiburg   AfD          Jens Bellemann\n",
       " 4    5522.601135   8149    Freiburg   AfD          Martin Polheim\n",
       " 5    5157.383706   4260    Freiburg   AfD         Robert Hagerman\n",
       " 6    4792.166277   4721    Freiburg   AfD           Michael Braun\n",
       " 7    5887.818564   6985    Freiburg   AfD             Bernd Fulde\n",
       " 8    6618.253421   8931    Freiburg   AfD            Karl Schwarz\n",
       " 9    5796.514206   6176    Freiburg   AfD             Jonas Möhle\n",
       " 10   5066.079349   5674    Freiburg   AfD   Alessandro Bertonasco\n",
       " 11   5979.122921   8008    Freiburg   AfD           Bernd Domnick\n",
       " 12   6435.644707   8263    Freiburg   AfD            Jack Gelfort\n",
       " 13   4974.774992   5585    Freiburg   AfD        Michael Pfeiffer\n",
       " 14   5705.209849   8607    Freiburg   AfD           Tilman Mehler\n",
       " 15   4700.861920   3979    Freiburg   AfD          Sandro Schüler\n",
       " 16   6800.862135   9278    Freiburg   AfD      Andreas Schumacher\n",
       " 17   4883.470635   4680    Freiburg   AfD           Thomas Müller\n",
       " 18   5248.688063   4934    Freiburg   AfD   Katharina Maria Lukau\n",
       " 19   5431.296778   7827    Freiburg   AfD     Heinz-Jürgen Schlag\n",
       " 20   6709.557778  12073    Freiburg   AfD            Detlef Huber\n",
       " 21   6526.949064   9948    Freiburg   AfD         Dubravko Mandic\n",
       " 22   5339.992421   4020    Freiburg   AfD  Marie-Luise Gatzweiler\n",
       " 23   5613.905492   5607    Freiburg   AfD              Marco Erat\n",
       " 24   5093.070066   2149  Heidelberg   AfD         Maximilian Wolf\n",
       " 25   4482.951654   2386  Heidelberg   AfD       Cornelia Degeratu\n",
       " 26   6679.377935   5280  Heidelberg   AfD       Jan Michael Kröhl\n",
       " 27   5459.141112   2249  Heidelberg   AfD           Florian Epple\n",
       " 28   4360.927972   2016  Heidelberg   AfD         Simone Kammerer\n",
       " 29   5703.188477   1977  Heidelberg   AfD        Ilona Schmittova\n",
       " 30   2774.620103   2319  Heidelberg   AfD              Liane Wolf\n",
       " 31   2652.596421   2235  Heidelberg   AfD      Helmut Fleischmann\n",
       " 32   5215.093748   2137  Heidelberg   AfD           Wilhelm Wabel\n",
       " 33   2530.572739   1698  Heidelberg   AfD           Helga Striehl\n",
       " 34   7533.543710   6856  Heidelberg   AfD        Timethy Bartesch\n",
       " 35   3018.667468   2116  Heidelberg   AfD              Bernd Wolf\n",
       " 36   6557.354252   5295  Heidelberg   AfD            Martin Jacob\n",
       " 37  14201.458749   9375  Heidelberg   AfD         Matthias Niebel\n",
       " 38   5947.235841   5316  Heidelberg   AfD           Manfred Hanke\n",
       " 39   2896.643785   2065  Heidelberg   AfD     Dieter-Eugen Glaser\n",
       " 40   2408.549057   1947  Heidelberg   AfD          Claudia Gregor\n",
       " 41   4971.046383   2473  Heidelberg   AfD    Dieter Bowe-Karamann\n",
       " 42   5825.212159   2979  Heidelberg   AfD         Stefan Holzmann\n",
       " 43   6313.306888   4708  Heidelberg   AfD            Klaus Blanck\n",
       " 44   3384.738514   1692  Heidelberg   AfD     Kai Gunther Lehmann\n",
       " 45   5581.164795   2735  Heidelberg   AfD             Jens Riedel\n",
       " 46   7655.567392   6986  Heidelberg   AfD         Sven Geschinski\n",
       " 47   2042.478010   1921  Heidelberg   AfD          Arthur Leibham\n",
       " 48   6801.401617   4691  Heidelberg   AfD      Birgit Fleischmann\n",
       " 49   6923.425299   5033  Heidelberg   AfD            Volker Kunze,\n",
       "                          Variable  Coefficient\n",
       " 0                      place_list  -325.390411\n",
       " 1                       incumbent     0.000000\n",
       " 2                           woman     0.000000\n",
       " 3                          doctor     0.000000\n",
       " 4                            time    -0.000000\n",
       " 5                federal_election     0.000000\n",
       " 6                     aristocracy    -0.000000\n",
       " 7                     google_stan    -0.000000\n",
       " 8                      population    -0.000000\n",
       " 9                  share_students    -0.000000\n",
       " 10                   unemployment     0.000000\n",
       " 11                      share_old   168.807480\n",
       " 12                            CDU     0.000000\n",
       " 13                            SPD     0.000000\n",
       " 14                          Linke    -0.000000\n",
       " 15                            FDP    -0.000000\n",
       " 16                          Grüne     0.000000\n",
       " 17                            AfD    -0.000000\n",
       " 18                    share_youth   582.701926\n",
       " 19                 share_migrants    -0.000000\n",
       " 20                   share_pupils     0.000000\n",
       " 21                             FW     0.000000\n",
       " 22                     local_list    -0.000000\n",
       " 23             federal_difference    -0.000000\n",
       " 24                     youth_list    -0.000000\n",
       " 25                 green_alt_list    -0.000000\n",
       " 26                 muslim_migrant    -0.000000\n",
       " 27             non_muslim_migrant    -0.000000\n",
       " 28                    double_name    -0.000000\n",
       " 29                     first_time    -0.000000\n",
       " ..                            ...          ...\n",
       " 40        place_list_unemployment    -0.000000\n",
       " 41           place_list_share_old    -0.000000\n",
       " 42                 place_list_CDU    -0.000000\n",
       " 43                 place_list_SPD    -0.000000\n",
       " 44               place_list_Linke    -0.000000\n",
       " 45                 place_list_FDP    -0.000000\n",
       " 46               place_list_Grüne    -0.000000\n",
       " 47                 place_list_AfD    -0.000000\n",
       " 48         place_list_share_youth    -0.000000\n",
       " 49      place_list_share_migrants  -141.205598\n",
       " 50        place_list_share_pupils    -0.000000\n",
       " 51                  place_list_FW     0.000000\n",
       " 52          place_list_local_list    -0.000000\n",
       " 53  place_list_federal_difference    -0.000000\n",
       " 54          place_list_youth_list    -0.000000\n",
       " 55      place_list_green_alt_list    -0.000000\n",
       " 56      place_list_muslim_migrant    -0.000000\n",
       " 57  place_list_non_muslim_migrant    -0.000000\n",
       " 58         place_list_double_name    -0.000000\n",
       " 59          place_list_first_time    -0.000000\n",
       " 60           incumbent_place_list    -0.000000\n",
       " 61               incumbent_square     0.000000\n",
       " 62                incumbent_woman     0.000000\n",
       " 63               incumbent_doctor     0.000000\n",
       " 64                 incumbent_time     0.000000\n",
       " 65     incumbent_federal_election     0.000000\n",
       " 66          incumbent_aristocracy     0.000000\n",
       " 67          incumbent_google_stan    -0.000000\n",
       " 68           incumbent_population  1397.295959\n",
       " 69       incumbent_share_students   147.446448\n",
       " \n",
       " [70 rows x 2 columns])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "lasso = Lasso(alpha = 302.1091452688529)\n",
    "\n",
    "lasso_comp_pred = res_lasso_cv.predict(X_test_scaled)\n",
    "pred_coef_comp = res_lasso_cv.coef_\n",
    "X2 = X_train.columns.values\n",
    "\n",
    "coefficients_lasso = pd.DataFrame({'Variable': X2, 'Coefficient': pred_coef_comp}) # Create a dataset with the estimated coefficients\n",
    "coefficients_lasso.to_excel(r'C:\\Users\\mariu\\Desktop\\Project\\Coefficients_Lasso.xlsx')\n",
    "\n",
    "prediction_lasso = pd.DataFrame({'Prediction': lasso_comp_pred, 'Votes': dataframe_test['votes'], 'city': dataframe_test['city'], 'party': dataframe_test['party'], 'name': dataframe_test['Name_total']})\n",
    "#prediction_lasso.to_excel(r'C:\\Users\\mariu\\Desktop\\Project\\Prediction_Lasso.xlsx')\n",
    "\n",
    "prediction_lasso.head(50), coefficients_lasso.head(70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] bootstrap=True, max_depth=90, max_features=auto, min_samples_leaf=3, min_samples_split=10, n_estimators=86 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  bootstrap=True, max_depth=90, max_features=auto, min_samples_leaf=3, min_samples_split=10, n_estimators=86, score=0.699, total=   1.4s\n",
      "[CV] bootstrap=True, max_depth=90, max_features=auto, min_samples_leaf=3, min_samples_split=10, n_estimators=86 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  bootstrap=True, max_depth=90, max_features=auto, min_samples_leaf=3, min_samples_split=10, n_estimators=86, score=0.380, total=   1.4s\n",
      "[CV] bootstrap=True, max_depth=90, max_features=auto, min_samples_leaf=3, min_samples_split=10, n_estimators=86 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    2.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  bootstrap=True, max_depth=90, max_features=auto, min_samples_leaf=3, min_samples_split=10, n_estimators=86, score=0.329, total=   1.4s\n",
      "[CV] bootstrap=True, max_depth=90, max_features=auto, min_samples_leaf=3, min_samples_split=10, n_estimators=86 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    4.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  bootstrap=True, max_depth=90, max_features=auto, min_samples_leaf=3, min_samples_split=10, n_estimators=86, score=0.742, total=   1.4s\n",
      "[CV] bootstrap=True, max_depth=90, max_features=auto, min_samples_leaf=3, min_samples_split=10, n_estimators=86 \n",
      "[CV]  bootstrap=True, max_depth=90, max_features=auto, min_samples_leaf=3, min_samples_split=10, n_estimators=86, score=0.173, total=   1.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    7.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9479491792761721"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid_1 = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [90],\n",
    "    'min_samples_leaf': [3],\n",
    "    'min_samples_split': [10],\n",
    "    'n_estimators': [86],\n",
    "    'max_features':['auto']\n",
    "}\n",
    "rf = RandomForestRegressor()\n",
    "random_forest = GridSearchCV(estimator = rf, param_grid = param_grid_1, \n",
    "                          cv = 5, verbose = 4, n_jobs = 1)\n",
    "t2 = time.time()\n",
    "random_forest.fit(rand_for_train_scaled, y_train)\n",
    "t_random_forest = time.time() - t2\n",
    "\n",
    "\n",
    "random_forest.score(rand_for_train_scaled,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(     Prediction  Votes        city party                    name\n",
       " 0   5855.545558   8163    Freiburg   AfD          Bernhard Lukau\n",
       " 1   5947.302264   7711    Freiburg   AfD        Michael Theuring\n",
       " 2   5342.404372   6504    Freiburg   AfD            Elmar Ertmer\n",
       " 3   5705.850133   7703    Freiburg   AfD          Jens Bellemann\n",
       " 4   4673.110880   8149    Freiburg   AfD          Martin Polheim\n",
       " 5   3633.733045   4260    Freiburg   AfD         Robert Hagerman\n",
       " 6   3045.624296   4721    Freiburg   AfD           Michael Braun\n",
       " 7   5103.121878   6985    Freiburg   AfD             Bernd Fulde\n",
       " 8   7321.717519   8931    Freiburg   AfD            Karl Schwarz\n",
       " 9   5095.306755   6176    Freiburg   AfD             Jonas Möhle\n",
       " 10  3728.866625   5674    Freiburg   AfD   Alessandro Bertonasco\n",
       " 11  5165.838577   8008    Freiburg   AfD           Bernd Domnick\n",
       " 12  6405.900374   8263    Freiburg   AfD            Jack Gelfort\n",
       " 13  3076.932713   5585    Freiburg   AfD        Michael Pfeiffer\n",
       " 14  5456.513691   8607    Freiburg   AfD           Tilman Mehler\n",
       " 15  3020.893263   3979    Freiburg   AfD          Sandro Schüler\n",
       " 16  9499.050983   9278    Freiburg   AfD      Andreas Schumacher\n",
       " 17  3097.618145   4680    Freiburg   AfD           Thomas Müller\n",
       " 18  3703.322554   4934    Freiburg   AfD   Katharina Maria Lukau\n",
       " 19  4293.488403   7827    Freiburg   AfD     Heinz-Jürgen Schlag\n",
       " 20  8285.623943  12073    Freiburg   AfD            Detlef Huber\n",
       " 21  6844.915437   9948    Freiburg   AfD         Dubravko Mandic\n",
       " 22  3966.294951   4020    Freiburg   AfD  Marie-Luise Gatzweiler\n",
       " 23  4755.053137   5607    Freiburg   AfD              Marco Erat\n",
       " 24  1309.890209   2149  Heidelberg   AfD         Maximilian Wolf\n",
       " 25  1260.615200   2386  Heidelberg   AfD       Cornelia Degeratu\n",
       " 26  2556.157515   5280  Heidelberg   AfD       Jan Michael Kröhl\n",
       " 27  1630.558221   2249  Heidelberg   AfD           Florian Epple\n",
       " 28  1277.603014   2016  Heidelberg   AfD         Simone Kammerer\n",
       " 29  1519.313892   1977  Heidelberg   AfD        Ilona Schmittova\n",
       " 30  1294.155186   2319  Heidelberg   AfD              Liane Wolf\n",
       " 31  1187.770020   2235  Heidelberg   AfD      Helmut Fleischmann\n",
       " 32  1452.289382   2137  Heidelberg   AfD           Wilhelm Wabel\n",
       " 33  1206.030682   1698  Heidelberg   AfD           Helga Striehl\n",
       " 34  5124.724030   6856  Heidelberg   AfD        Timethy Bartesch\n",
       " 35  1101.720725   2116  Heidelberg   AfD              Bernd Wolf\n",
       " 36  2297.320405   5295  Heidelberg   AfD            Martin Jacob\n",
       " 37  9308.718607   9375  Heidelberg   AfD         Matthias Niebel\n",
       " 38  2062.908292   5316  Heidelberg   AfD           Manfred Hanke\n",
       " 39  1379.476477   2065  Heidelberg   AfD     Dieter-Eugen Glaser\n",
       " 40  1265.116874   1947  Heidelberg   AfD          Claudia Gregor\n",
       " 41  1247.504344   2473  Heidelberg   AfD    Dieter Bowe-Karamann\n",
       " 42  1579.472864   2979  Heidelberg   AfD         Stefan Holzmann\n",
       " 43  2255.852805   4708  Heidelberg   AfD            Klaus Blanck\n",
       " 44  1190.513135   1692  Heidelberg   AfD     Kai Gunther Lehmann\n",
       " 45  1436.877357   2735  Heidelberg   AfD             Jens Riedel\n",
       " 46  5549.936162   6986  Heidelberg   AfD         Sven Geschinski\n",
       " 47  1290.535193   1921  Heidelberg   AfD          Arthur Leibham\n",
       " 48  2655.469834   4691  Heidelberg   AfD      Birgit Fleischmann\n",
       " 49  2901.707066   5033  Heidelberg   AfD            Volker Kunze,\n",
       " RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=90,\n",
       "                       max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=3, min_samples_split=10,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=86,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False),\n",
       " array([8.56556661e-02, 9.63480561e-02, 1.15648289e-03, 7.96896102e-04,\n",
       "        3.76209731e-04, 2.62555048e-01, 3.01912108e-07, 1.20969039e-02,\n",
       "        3.19461352e-01, 1.28632974e-01, 4.55418116e-03, 8.17029465e-03,\n",
       "        1.26473728e-03, 1.44705698e-02, 2.64996803e-04, 8.05958384e-03,\n",
       "        2.92114657e-03, 1.83471138e-04, 4.69628715e-03, 1.47370679e-02,\n",
       "        5.63890866e-03, 9.51758158e-04, 3.87075408e-03, 1.89077749e-02,\n",
       "        2.11642807e-04, 1.86380732e-04, 5.39623847e-05, 9.41829218e-05,\n",
       "        1.80502837e-04, 3.50190528e-03]),\n",
       " 30)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rf_1 = RandomForestRegressor(bootstrap = True, max_depth = 90, min_samples_leaf = 3, min_samples_split = 10, n_estimators = 86)\n",
    "rf_1.fit(rand_for_train_scaled, y_train)\n",
    "\n",
    "random_forest_pred = rf_1.predict(rand_for_test_scaled)\n",
    "\n",
    "feature_importances_coef_comp = rf_1.feature_importances_\n",
    "\n",
    "list_coefficients = pd.DataFrame({'Variable': X2_rand_for, 'Coefficient': feature_importances_coef_comp}) # Create a dataset with the estimated coefficients\n",
    "list_coefficients.to_excel(r'C:\\Users\\mariu\\Desktop\\Project\\Coefficients_Random_Forest.xlsx')\n",
    "\n",
    "prediction_random_forest = pd.DataFrame({'Prediction': \n",
    "random_forest_pred, 'Votes': dataframe_test['votes'], 'city': dataframe_test['city'], 'party': dataframe_test['party'], 'name': dataframe_test['Name_total']})\n",
    "prediction_random_forest.to_excel(r'C:\\Users\\mariu\\Desktop\\Project\\Prediction_Random_Forest.xlsx')\n",
    "\n",
    "prediction_random_forest.head(50), random_forest.best_estimator_, rf_1.feature_importances_, rf_1.n_features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1    52203002.5074            2.50s\n",
      "         2    45876040.6743            2.57s\n",
      "         3    40987946.8284            2.59s\n",
      "         4    36168260.4335            2.68s\n",
      "         5    33260917.5246            2.69s\n",
      "         6    30361353.0735            2.72s\n",
      "         7    27694921.9411            2.76s\n",
      "         8    25808744.5165            2.74s\n",
      "         9    24075968.1492            2.73s\n",
      "        10    22768566.3191            2.73s\n",
      "        11    21330927.9576            2.73s\n",
      "        12    20254714.8180            2.72s\n",
      "        13    19268205.7238            2.73s\n",
      "        14    18371385.9078            2.73s\n",
      "        15    17704376.8239            2.73s\n",
      "        16    16924361.9389            2.73s\n",
      "        17    16302479.1410            2.71s\n",
      "        18    15774979.1828            2.67s\n",
      "        19    15225100.2784            2.66s\n",
      "        20    14701721.3556            2.65s\n",
      "        21    14311163.8910            2.64s\n",
      "        22    13873478.1450            2.63s\n",
      "        23    13566402.7451            2.61s\n",
      "        24    12940167.3383            2.59s\n",
      "        25    12546884.9038            2.57s\n",
      "        26    12111694.5985            2.56s\n",
      "        27    11864990.6533            2.54s\n",
      "        28    11545269.3250            2.53s\n",
      "        29    11248871.3073            2.51s\n",
      "        30    11033063.1673            2.49s\n",
      "        31    10865557.2963            2.47s\n",
      "        32    10694564.5582            2.45s\n",
      "        33    10518058.8377            2.43s\n",
      "        34    10378864.1408            2.42s\n",
      "        35    10200552.6543            2.40s\n",
      "        36    10047989.3789            2.38s\n",
      "        37     9972883.1453            2.37s\n",
      "        38     9836209.1966            2.35s\n",
      "        39     9752394.8795            2.33s\n",
      "        40     9654910.6099            2.32s\n",
      "        41     9530631.8493            2.30s\n",
      "        42     9443562.4658            2.28s\n",
      "        43     9392990.0300            2.27s\n",
      "        44     9305755.7510            2.25s\n",
      "        45     9234952.4174            2.22s\n",
      "        46     9180808.1917            2.21s\n",
      "        47     9075423.1835            2.19s\n",
      "        48     9003889.9719            2.17s\n",
      "        49     8933138.3121            2.15s\n",
      "        50     8804283.2261            2.13s\n",
      "        51     8757364.8647            2.13s\n",
      "        52     8698902.7151            2.11s\n",
      "        53     8614626.5041            2.10s\n",
      "        54     8529364.0051            2.08s\n",
      "        55     8482816.0665            2.06s\n",
      "        56     8405089.5816            2.04s\n",
      "        57     8341412.2023            2.02s\n",
      "        58     8311910.4358            2.00s\n",
      "        59     8177250.3984            1.99s\n",
      "        60     8128961.5594            1.97s\n",
      "        61     8075160.5038            1.95s\n",
      "        62     8054190.5757            1.94s\n",
      "        63     8030871.9006            1.92s\n",
      "        64     8002927.8979            1.91s\n",
      "        65     7987241.9969            1.89s\n",
      "        66     7950193.3054            1.87s\n",
      "        67     7929812.0680            1.86s\n",
      "        68     7881644.8481            1.84s\n",
      "        69     7863401.1470            1.82s\n",
      "        70     7819665.5703            1.81s\n",
      "        71     7763024.1566            1.79s\n",
      "        72     7736565.9769            1.78s\n",
      "        73     7672228.6551            1.76s\n",
      "        74     7629099.6714            1.74s\n",
      "        75     7609580.9971            1.73s\n",
      "        76     7589744.7834            1.71s\n",
      "        77     7566143.4522            1.69s\n",
      "        78     7540401.0322            1.68s\n",
      "        79     7521878.5708            1.66s\n",
      "        80     7506030.4921            1.65s\n",
      "        81     7477486.0040            1.63s\n",
      "        82     7465906.4922            1.61s\n",
      "        83     7442265.5206            1.60s\n",
      "        84     7431504.5952            1.58s\n",
      "        85     7415270.6284            1.56s\n",
      "        86     7398178.3678            1.55s\n",
      "        87     7377092.2071            1.53s\n",
      "        88     7357786.6439            1.52s\n",
      "        89     7344361.9167            1.50s\n",
      "        90     7308522.1320            1.48s\n",
      "        91     7296365.7410            1.47s\n",
      "        92     7227270.9691            1.45s\n",
      "        93     7211191.1359            1.44s\n",
      "        94     7196087.0635            1.42s\n",
      "        95     7188569.7952            1.40s\n",
      "        96     7149405.7649            1.39s\n",
      "        97     7136974.7316            1.37s\n",
      "        98     7128880.7168            1.36s\n",
      "        99     7111619.0217            1.34s\n",
      "       100     7063375.0990            1.33s\n",
      "       101     7040937.5285            1.31s\n",
      "       102     7028456.6981            1.29s\n",
      "       103     6997058.7946            1.28s\n",
      "       104     6981800.4157            1.26s\n",
      "       105     6969629.4361            1.24s\n",
      "       106     6963227.5227            1.23s\n",
      "       107     6948631.4240            1.21s\n",
      "       108     6936753.8480            1.20s\n",
      "       109     6929596.0273            1.18s\n",
      "       110     6919128.0485            1.16s\n",
      "       111     6908225.7766            1.15s\n",
      "       112     6890536.1413            1.13s\n",
      "       113     6884677.2466            1.12s\n",
      "       114     6876605.6642            1.10s\n",
      "       115     6868024.2811            1.08s\n",
      "       116     6847420.3151            1.07s\n",
      "       117     6820085.9019            1.05s\n",
      "       118     6784087.3973            1.04s\n",
      "       119     6762738.6889            1.02s\n",
      "       120     6739561.1034            1.00s\n",
      "       121     6728396.6220            0.99s\n",
      "       122     6709311.6709            0.97s\n",
      "       123     6690710.1955            0.96s\n",
      "       124     6674422.5158            0.94s\n",
      "       125     6652323.3558            0.92s\n",
      "       126     6636763.7648            0.90s\n",
      "       127     6629016.7705            0.89s\n",
      "       128     6622016.8233            0.87s\n",
      "       129     6614036.5485            0.85s\n",
      "       130     6605522.7350            0.84s\n",
      "       131     6582848.6311            0.82s\n",
      "       132     6578593.3600            0.80s\n",
      "       133     6564775.3365            0.79s\n",
      "       134     6542442.1586            0.77s\n",
      "       135     6534969.4933            0.75s\n",
      "       136     6526112.9771            0.74s\n",
      "       137     6521867.5539            0.72s\n",
      "       138     6511789.5249            0.70s\n",
      "       139     6503697.8813            0.69s\n",
      "       140     6495918.3502            0.67s\n",
      "       141     6476964.4379            0.65s\n",
      "       142     6460696.4802            0.64s\n",
      "       143     6443064.1960            0.62s\n",
      "       144     6434127.9369            0.60s\n",
      "       145     6428753.4652            0.59s\n",
      "       146     6413825.3142            0.57s\n",
      "       147     6408949.2828            0.55s\n",
      "       148     6392500.9304            0.54s\n",
      "       149     6388795.3437            0.52s\n",
      "       150     6380946.8039            0.50s\n",
      "       151     6364154.8145            0.48s\n",
      "       152     6357601.0696            0.47s\n",
      "       153     6347442.8460            0.45s\n",
      "       154     6342130.2806            0.43s\n",
      "       155     6332403.5391            0.42s\n",
      "       156     6328109.4279            0.40s\n",
      "       157     6324545.5173            0.38s\n",
      "       158     6308597.6499            0.37s\n",
      "       159     6295996.0775            0.35s\n",
      "       160     6290873.2510            0.33s\n",
      "       161     6288063.3795            0.32s\n",
      "       162     6272258.8154            0.30s\n",
      "       163     6263027.0341            0.28s\n",
      "       164     6258130.7905            0.27s\n",
      "       165     6252910.3641            0.25s\n",
      "       166     6248900.1803            0.23s\n",
      "       167     6244382.6821            0.22s\n",
      "       168     6236446.7152            0.20s\n",
      "       169     6220395.2548            0.18s\n",
      "       170     6216088.4066            0.17s\n",
      "       171     6213081.1714            0.15s\n",
      "       172     6207019.7924            0.13s\n",
      "       173     6198658.7086            0.12s\n",
      "       174     6187571.0369            0.10s\n",
      "       175     6175079.8255            0.08s\n",
      "       176     6170425.7973            0.07s\n",
      "       177     6165702.2843            0.05s\n",
      "       178     6160991.6210            0.03s\n",
      "       179     6156370.0249            0.02s\n",
      "       180     6151098.1486            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1    82614763.6095            2.68s\n",
      "         2    73579905.5773            2.57s\n",
      "         3    64779142.6556            2.59s\n",
      "         4    56609799.4682            2.63s\n",
      "         5    50307378.4242            2.62s\n",
      "         6    44991273.6902            2.63s\n",
      "         7    40228717.6020            2.66s\n",
      "         8    37177447.0613            2.66s\n",
      "         9    34409875.9904            2.65s\n",
      "        10    32005714.6888            2.66s\n",
      "        11    28761963.9749            2.67s\n",
      "        12    26932457.9754            2.67s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        13    25412796.9571            2.66s\n",
      "        14    23794952.2251            2.68s\n",
      "        15    22236654.5174            2.67s\n",
      "        16    21267888.9743            2.66s\n",
      "        17    20405796.1375            2.63s\n",
      "        18    19366856.6966            2.61s\n",
      "        19    18701471.6120            2.60s\n",
      "        20    17712437.4648            2.59s\n",
      "        21    16795250.9038            2.59s\n",
      "        22    16175459.3983            2.59s\n",
      "        23    15625566.7720            2.58s\n",
      "        24    15294393.0925            2.57s\n",
      "        25    14790702.9998            2.55s\n",
      "        26    14539936.2245            2.55s\n",
      "        27    14155331.9506            2.53s\n",
      "        28    13733854.0498            2.51s\n",
      "        29    13297288.5241            2.49s\n",
      "        30    13042676.2431            2.49s\n",
      "        31    12901214.0108            2.47s\n",
      "        32    12778350.5331            2.46s\n",
      "        33    12630056.3402            2.47s\n",
      "        34    12367887.5637            2.47s\n",
      "        35    12279106.7625            2.46s\n",
      "        36    11975575.6987            2.46s\n",
      "        37    11836132.5151            2.46s\n",
      "        38    11705919.0038            2.45s\n",
      "        39    11580872.7502            2.43s\n",
      "        40    11483289.4819            2.42s\n",
      "        41    11217443.2208            2.42s\n",
      "        42    11147927.4075            2.41s\n",
      "        43    11039258.8340            2.40s\n",
      "        44    10898496.8508            2.37s\n",
      "        45    10748487.7928            2.35s\n",
      "        46    10612249.9235            2.33s\n",
      "        47    10469152.8632            2.31s\n",
      "        48    10329936.1498            2.29s\n",
      "        49    10087597.9255            2.27s\n",
      "        50     9993148.4434            2.25s\n",
      "        51     9934000.3857            2.23s\n",
      "        52     9897241.0261            2.21s\n",
      "        53     9773443.7948            2.20s\n",
      "        54     9679122.2330            2.18s\n",
      "        55     9591089.6400            2.16s\n",
      "        56     9556415.9460            2.14s\n",
      "        57     9487078.7760            2.12s\n",
      "        58     9421446.0486            2.10s\n",
      "        59     9317530.0479            2.08s\n",
      "        60     9282762.5034            2.06s\n",
      "        61     9253949.0922            2.04s\n",
      "        62     9218373.7799            2.03s\n",
      "        63     9170672.0767            2.01s\n",
      "        64     9130037.3355            1.99s\n",
      "        65     9106441.7170            1.97s\n",
      "        66     9053022.9961            1.95s\n",
      "        67     9025210.5044            1.94s\n",
      "        68     8959456.0339            1.93s\n",
      "        69     8933342.4621            1.91s\n",
      "        70     8912451.2146            1.90s\n",
      "        71     8856858.0475            1.88s\n",
      "        72     8813452.3931            1.87s\n",
      "        73     8792881.4924            1.86s\n",
      "        74     8767844.1460            1.84s\n",
      "        75     8747692.5639            1.82s\n",
      "        76     8702346.9934            1.81s\n",
      "        77     8669932.8225            1.79s\n",
      "        78     8643272.5725            1.77s\n",
      "        79     8627225.1789            1.75s\n",
      "        80     8581083.4673            1.74s\n",
      "        81     8547302.8175            1.72s\n",
      "        82     8460316.2453            1.70s\n",
      "        83     8438829.7526            1.68s\n",
      "        84     8419254.4625            1.67s\n",
      "        85     8396848.0366            1.65s\n",
      "        86     8323063.9685            1.63s\n",
      "        87     8301385.8742            1.61s\n",
      "        88     8287683.9435            1.59s\n",
      "        89     8261302.3548            1.57s\n",
      "        90     8233844.3671            1.56s\n",
      "        91     8183089.8194            1.54s\n",
      "        92     8145427.8768            1.52s\n",
      "        93     8120141.9065            1.50s\n",
      "        94     8083307.4846            1.49s\n",
      "        95     8074859.8919            1.47s\n",
      "        96     8055651.3623            1.45s\n",
      "        97     8034301.8002            1.43s\n",
      "        98     8022884.3358            1.42s\n",
      "        99     8005768.5670            1.40s\n",
      "       100     7973820.7751            1.38s\n",
      "       101     7961324.6900            1.37s\n",
      "       102     7922364.6307            1.35s\n",
      "       103     7911140.8318            1.33s\n",
      "       104     7895905.5273            1.32s\n",
      "       105     7875589.5707            1.30s\n",
      "       106     7840371.5890            1.29s\n",
      "       107     7830396.4023            1.27s\n",
      "       108     7822063.4618            1.26s\n",
      "       109     7792552.6037            1.24s\n",
      "       110     7765154.4625            1.22s\n",
      "       111     7746059.5914            1.21s\n",
      "       112     7728872.2993            1.19s\n",
      "       113     7710536.0247            1.17s\n",
      "       114     7701659.0224            1.15s\n",
      "       115     7693734.9840            1.14s\n",
      "       116     7680747.4378            1.12s\n",
      "       117     7671116.1152            1.10s\n",
      "       118     7659389.9062            1.08s\n",
      "       119     7653001.9509            1.06s\n",
      "       120     7638785.4414            1.05s\n",
      "       121     7618998.4033            1.03s\n",
      "       122     7599857.3610            1.01s\n",
      "       123     7591923.3770            1.00s\n",
      "       124     7583431.2180            0.98s\n",
      "       125     7573531.3258            0.96s\n",
      "       126     7563492.9843            0.94s\n",
      "       127     7553812.2728            0.93s\n",
      "       128     7542477.5877            0.91s\n",
      "       129     7515917.9749            0.89s\n",
      "       130     7502406.6913            0.87s\n",
      "       131     7495250.9023            0.86s\n",
      "       132     7485232.7390            0.84s\n",
      "       133     7478795.9679            0.82s\n",
      "       134     7456425.1529            0.81s\n",
      "       135     7446190.4207            0.79s\n",
      "       136     7435034.1795            0.77s\n",
      "       137     7393724.6103            0.76s\n",
      "       138     7379323.2500            0.74s\n",
      "       139     7372775.7629            0.72s\n",
      "       140     7351008.9426            0.71s\n",
      "       141     7334391.6428            0.69s\n",
      "       142     7321888.2127            0.67s\n",
      "       143     7293444.0907            0.66s\n",
      "       144     7284489.9280            0.64s\n",
      "       145     7264293.6203            0.62s\n",
      "       146     7252390.3104            0.60s\n",
      "       147     7245566.9212            0.59s\n",
      "       148     7233258.3711            0.57s\n",
      "       149     7228117.5309            0.55s\n",
      "       150     7219359.6140            0.53s\n",
      "       151     7205642.0539            0.52s\n",
      "       152     7194904.4342            0.50s\n",
      "       153     7179228.4471            0.48s\n",
      "       154     7165244.2970            0.46s\n",
      "       155     7160679.8083            0.44s\n",
      "       156     7152519.0864            0.43s\n",
      "       157     7137455.0865            0.41s\n",
      "       158     7123263.7707            0.39s\n",
      "       159     7115873.2556            0.37s\n",
      "       160     7093620.8411            0.36s\n",
      "       161     7075787.3071            0.34s\n",
      "       162     7069910.9092            0.32s\n",
      "       163     7059986.0179            0.30s\n",
      "       164     7055346.5138            0.28s\n",
      "       165     7047600.8455            0.27s\n",
      "       166     7041701.0273            0.25s\n",
      "       167     7029244.5277            0.23s\n",
      "       168     7018442.9407            0.21s\n",
      "       169     7009477.2796            0.20s\n",
      "       170     7001292.5815            0.18s\n",
      "       171     6995781.3238            0.16s\n",
      "       172     6980861.3773            0.14s\n",
      "       173     6974010.7764            0.12s\n",
      "       174     6969598.0303            0.11s\n",
      "       175     6964757.9232            0.09s\n",
      "       176     6947742.8712            0.07s\n",
      "       177     6944986.9010            0.05s\n",
      "       178     6934876.7697            0.04s\n",
      "       179     6922151.6678            0.02s\n",
      "       180     6918631.9407            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1    61036679.3891            3.04s\n",
      "         2    52660638.9539            2.84s\n",
      "         3    46152725.0930            2.82s\n",
      "         4    41613073.3275            2.85s\n",
      "         5    36907446.4614            2.93s\n",
      "         6    33592119.9749            2.92s\n",
      "         7    30913607.1439            2.88s\n",
      "         8    27876661.0813            2.87s\n",
      "         9    25192694.2833            2.86s\n",
      "        10    23209403.0343            2.85s\n",
      "        11    21317716.4954            2.84s\n",
      "        12    19719706.2390            2.85s\n",
      "        13    18331049.4056            2.83s\n",
      "        14    17004300.7284            2.83s\n",
      "        15    16180650.5151            2.79s\n",
      "        16    15303460.4911            2.76s\n",
      "        17    14506341.3055            2.76s\n",
      "        18    13885062.8379            2.75s\n",
      "        19    13251575.1389            2.72s\n",
      "        20    12576586.1388            2.71s\n",
      "        21    11634277.3141            2.70s\n",
      "        22    11059753.2452            2.68s\n",
      "        23    10831349.7519            2.66s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        24    10476988.4210            2.65s\n",
      "        25    10107526.2183            2.63s\n",
      "        26     9805632.3039            2.62s\n",
      "        27     9428851.1394            2.61s\n",
      "        28     9202538.0112            2.58s\n",
      "        29     8940359.3969            2.56s\n",
      "        30     8496323.0517            2.54s\n",
      "        31     8349893.3379            2.52s\n",
      "        32     8255261.0943            2.50s\n",
      "        33     7929464.7363            2.47s\n",
      "        34     7756653.6688            2.48s\n",
      "        35     7554936.9234            2.46s\n",
      "        36     7460201.0152            2.45s\n",
      "        37     7332226.5719            2.44s\n",
      "        38     7272339.0940            2.42s\n",
      "        39     7074215.1960            2.39s\n",
      "        40     6983943.6664            2.38s\n",
      "        41     6828996.6028            2.36s\n",
      "        42     6653919.4490            2.34s\n",
      "        43     6591087.3079            2.33s\n",
      "        44     6548946.9206            2.31s\n",
      "        45     6316729.3446            2.29s\n",
      "        46     6249296.9220            2.28s\n",
      "        47     6146780.0420            2.26s\n",
      "        48     6054450.4784            2.24s\n",
      "        49     6006085.6968            2.22s\n",
      "        50     5892822.1278            2.21s\n",
      "        51     5858957.6132            2.19s\n",
      "        52     5817353.2451            2.17s\n",
      "        53     5750564.0538            2.16s\n",
      "        54     5705171.7494            2.14s\n",
      "        55     5636804.9738            2.12s\n",
      "        56     5579703.3204            2.10s\n",
      "        57     5544401.7158            2.08s\n",
      "        58     5479548.8799            2.07s\n",
      "        59     5439200.4502            2.05s\n",
      "        60     5407446.2396            2.04s\n",
      "        61     5345941.2994            2.02s\n",
      "        62     5302959.8253            2.00s\n",
      "        63     5250231.1599            1.99s\n",
      "        64     5198482.7650            1.97s\n",
      "        65     5134241.6504            1.96s\n",
      "        66     5065770.9219            1.94s\n",
      "        67     5027842.4705            1.92s\n",
      "        68     4993841.4529            1.90s\n",
      "        69     4963658.3655            1.89s\n",
      "        70     4940385.2731            1.87s\n",
      "        71     4889995.9798            1.87s\n",
      "        72     4857139.2575            1.87s\n",
      "        73     4837625.8652            1.87s\n",
      "        74     4810408.2703            1.86s\n",
      "        75     4772754.7118            1.85s\n",
      "        76     4755574.4458            1.84s\n",
      "        77     4719276.4357            1.84s\n",
      "        78     4701484.5536            1.83s\n",
      "        79     4687129.2203            1.81s\n",
      "        80     4663151.1523            1.80s\n",
      "        81     4642775.3577            1.78s\n",
      "        82     4627495.6623            1.76s\n",
      "        83     4602111.2100            1.74s\n",
      "        84     4578387.9819            1.72s\n",
      "        85     4563736.6198            1.70s\n",
      "        86     4538340.6665            1.68s\n",
      "        87     4509329.0940            1.66s\n",
      "        88     4490142.7047            1.64s\n",
      "        89     4454820.0798            1.63s\n",
      "        90     4418175.2008            1.61s\n",
      "        91     4406081.8949            1.59s\n",
      "        92     4382181.5794            1.57s\n",
      "        93     4369255.5543            1.56s\n",
      "        94     4355535.7554            1.54s\n",
      "        95     4348956.3409            1.52s\n",
      "        96     4341955.5529            1.50s\n",
      "        97     4327328.4848            1.48s\n",
      "        98     4309130.2134            1.46s\n",
      "        99     4291012.4401            1.45s\n",
      "       100     4266591.7539            1.43s\n",
      "       101     4251572.8392            1.41s\n",
      "       102     4239847.5489            1.39s\n",
      "       103     4218070.1812            1.38s\n",
      "       104     4201274.0124            1.36s\n",
      "       105     4171131.6321            1.34s\n",
      "       106     4141629.5208            1.32s\n",
      "       107     4106537.2739            1.30s\n",
      "       108     4095363.7552            1.29s\n",
      "       109     4081770.8389            1.27s\n",
      "       110     4071912.7295            1.25s\n",
      "       111     4045117.4644            1.23s\n",
      "       112     4037661.8778            1.21s\n",
      "       113     4023874.5170            1.20s\n",
      "       114     4015781.3074            1.18s\n",
      "       115     4008016.0134            1.16s\n",
      "       116     3999809.1813            1.14s\n",
      "       117     3993541.5572            1.12s\n",
      "       118     3973421.3929            1.11s\n",
      "       119     3956834.4191            1.09s\n",
      "       120     3948669.8830            1.07s\n",
      "       121     3943913.3635            1.05s\n",
      "       122     3922965.7926            1.03s\n",
      "       123     3916400.4347            1.01s\n",
      "       124     3888113.1050            0.99s\n",
      "       125     3862319.3030            0.97s\n",
      "       126     3845642.5283            0.96s\n",
      "       127     3835207.8804            0.94s\n",
      "       128     3827385.0604            0.92s\n",
      "       129     3821493.3013            0.90s\n",
      "       130     3813826.5001            0.88s\n",
      "       131     3808465.8967            0.87s\n",
      "       132     3794541.5562            0.85s\n",
      "       133     3779684.0054            0.83s\n",
      "       134     3768496.6521            0.81s\n",
      "       135     3761931.1871            0.79s\n",
      "       136     3753718.9066            0.78s\n",
      "       137     3739449.9964            0.76s\n",
      "       138     3728288.5399            0.74s\n",
      "       139     3724176.5665            0.72s\n",
      "       140     3716366.9793            0.71s\n",
      "       141     3708222.9573            0.69s\n",
      "       142     3702254.6683            0.67s\n",
      "       143     3696115.2424            0.65s\n",
      "       144     3692125.1606            0.64s\n",
      "       145     3673152.9906            0.62s\n",
      "       146     3665237.2857            0.60s\n",
      "       147     3657842.1902            0.58s\n",
      "       148     3651750.6568            0.57s\n",
      "       149     3638941.3941            0.55s\n",
      "       150     3628932.6224            0.53s\n",
      "       151     3624311.5473            0.51s\n",
      "       152     3610578.3029            0.50s\n",
      "       153     3606380.2155            0.48s\n",
      "       154     3603015.2365            0.46s\n",
      "       155     3576748.3649            0.44s\n",
      "       156     3571715.9547            0.42s\n",
      "       157     3563507.6757            0.41s\n",
      "       158     3556777.5670            0.39s\n",
      "       159     3549274.4036            0.37s\n",
      "       160     3537122.3234            0.35s\n",
      "       161     3521139.2119            0.34s\n",
      "       162     3507310.9743            0.32s\n",
      "       163     3500349.1012            0.30s\n",
      "       164     3494284.2620            0.28s\n",
      "       165     3486140.1756            0.26s\n",
      "       166     3482669.7542            0.25s\n",
      "       167     3478420.9555            0.23s\n",
      "       168     3470869.6709            0.21s\n",
      "       169     3466655.3896            0.19s\n",
      "       170     3456484.6307            0.18s\n",
      "       171     3444990.8719            0.16s\n",
      "       172     3442088.2460            0.14s\n",
      "       173     3435388.5840            0.12s\n",
      "       174     3431239.3357            0.11s\n",
      "       175     3415608.9447            0.09s\n",
      "       176     3411323.6279            0.07s\n",
      "       177     3408268.0761            0.05s\n",
      "       178     3405011.7782            0.04s\n",
      "       179     3401638.7255            0.02s\n",
      "       180     3391360.1381            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1    79969223.4884            2.68s\n",
      "         2    71857337.1980            2.57s\n",
      "         3    63921500.1655            2.53s\n",
      "         4    56222538.0234            2.59s\n",
      "         5    50848545.1375            2.62s\n",
      "         6    46072500.0701            2.63s\n",
      "         7    41216371.2693            2.66s\n",
      "         8    38066501.0567            2.68s\n",
      "         9    34797061.0651            2.69s\n",
      "        10    32097794.0317            2.70s\n",
      "        11    29946694.8049            2.68s\n",
      "        12    27746531.9176            2.67s\n",
      "        13    26193965.5137            2.66s\n",
      "        14    24660342.5344            2.67s\n",
      "        15    23360986.0127            2.68s\n",
      "        16    21703790.0407            2.67s\n",
      "        17    20462466.0445            2.67s\n",
      "        18    19786349.3902            2.66s\n",
      "        19    18699642.3388            2.65s\n",
      "        20    17938684.2457            2.66s\n",
      "        21    17166988.5122            2.64s\n",
      "        22    16527805.2620            2.64s\n",
      "        23    15979266.2728            2.62s\n",
      "        24    15335927.9127            2.60s\n",
      "        25    15086793.9941            2.58s\n",
      "        26    14555852.5004            2.57s\n",
      "        27    14013697.9483            2.55s\n",
      "        28    13787839.4844            2.53s\n",
      "        29    13421360.6309            2.52s\n",
      "        30    13200137.3959            2.50s\n",
      "        31    12992607.4302            2.49s\n",
      "        32    12733012.0290            2.47s\n",
      "        33    12472798.8780            2.45s\n",
      "        34    12262927.4247            2.45s\n",
      "        35    12148554.8838            2.42s\n",
      "        36    11940172.9518            2.41s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        37    11778843.5790            2.39s\n",
      "        38    11595011.7069            2.38s\n",
      "        39    11371686.0465            2.36s\n",
      "        40    11291120.7181            2.34s\n",
      "        41    11027236.5119            2.33s\n",
      "        42    10915718.5722            2.32s\n",
      "        43    10846724.6171            2.31s\n",
      "        44    10705225.1070            2.30s\n",
      "        45    10563392.7037            2.29s\n",
      "        46    10414461.3246            2.29s\n",
      "        47    10358604.8765            2.28s\n",
      "        48    10305499.4521            2.27s\n",
      "        49    10192698.5594            2.26s\n",
      "        50    10038471.0510            2.26s\n",
      "        51     9988483.9010            2.24s\n",
      "        52     9888364.2624            2.23s\n",
      "        53     9813948.5915            2.22s\n",
      "        54     9752253.6502            2.20s\n",
      "        55     9632364.5956            2.19s\n",
      "        56     9593616.4968            2.17s\n",
      "        57     9525209.8675            2.16s\n",
      "        58     9486836.3226            2.14s\n",
      "        59     9399859.6313            2.12s\n",
      "        60     9275308.6220            2.10s\n",
      "        61     9230682.4020            2.08s\n",
      "        62     9197271.4748            2.06s\n",
      "        63     9143784.8217            2.05s\n",
      "        64     9115221.5700            2.03s\n",
      "        65     9075335.8008            2.01s\n",
      "        66     8986214.1386            2.00s\n",
      "        67     8958438.2187            1.98s\n",
      "        68     8867661.3694            1.97s\n",
      "        69     8822512.7232            1.95s\n",
      "        70     8790617.2854            1.94s\n",
      "        71     8728471.9350            1.93s\n",
      "        72     8690425.4532            1.91s\n",
      "        73     8667569.3697            1.90s\n",
      "        74     8572965.0205            1.88s\n",
      "        75     8546776.5285            1.87s\n",
      "        76     8517760.8786            1.85s\n",
      "        77     8492354.0425            1.83s\n",
      "        78     8476683.9111            1.81s\n",
      "        79     8453689.1438            1.79s\n",
      "        80     8434429.9995            1.78s\n",
      "        81     8418742.3506            1.76s\n",
      "        82     8393694.6915            1.74s\n",
      "        83     8366122.2711            1.73s\n",
      "        84     8346408.1997            1.71s\n",
      "        85     8334383.0952            1.70s\n",
      "        86     8312083.6159            1.68s\n",
      "        87     8286286.6656            1.67s\n",
      "        88     8255447.5240            1.65s\n",
      "        89     8235377.1553            1.63s\n",
      "        90     8225302.7690            1.61s\n",
      "        91     8205743.7804            1.60s\n",
      "        92     8174304.1903            1.58s\n",
      "        93     8156302.0633            1.56s\n",
      "        94     8140639.3639            1.54s\n",
      "        95     8119034.8193            1.52s\n",
      "        96     8091871.8752            1.50s\n",
      "        97     8077540.8425            1.49s\n",
      "        98     8023582.7791            1.47s\n",
      "        99     7974522.7555            1.45s\n",
      "       100     7952588.5726            1.43s\n",
      "       101     7936469.9871            1.41s\n",
      "       102     7921404.4256            1.39s\n",
      "       103     7889578.7929            1.37s\n",
      "       104     7872710.1722            1.36s\n",
      "       105     7831167.9112            1.34s\n",
      "       106     7823188.1636            1.32s\n",
      "       107     7812398.4346            1.30s\n",
      "       108     7803259.3023            1.28s\n",
      "       109     7794365.8952            1.26s\n",
      "       110     7780938.1940            1.25s\n",
      "       111     7763153.2152            1.23s\n",
      "       112     7749174.2064            1.21s\n",
      "       113     7737565.1788            1.19s\n",
      "       114     7728830.6805            1.17s\n",
      "       115     7712269.5788            1.15s\n",
      "       116     7658029.7278            1.14s\n",
      "       117     7650780.2762            1.12s\n",
      "       118     7636504.5013            1.10s\n",
      "       119     7598682.6457            1.08s\n",
      "       120     7583741.3950            1.06s\n",
      "       121     7563480.4969            1.05s\n",
      "       122     7554987.4514            1.03s\n",
      "       123     7547797.8086            1.01s\n",
      "       124     7539841.6478            0.99s\n",
      "       125     7521517.1171            0.97s\n",
      "       126     7512678.6748            0.95s\n",
      "       127     7501837.7651            0.94s\n",
      "       128     7490935.5738            0.92s\n",
      "       129     7478611.2755            0.90s\n",
      "       130     7470511.9297            0.88s\n",
      "       131     7456089.4843            0.87s\n",
      "       132     7415855.9818            0.85s\n",
      "       133     7409033.0798            0.83s\n",
      "       134     7399280.6352            0.81s\n",
      "       135     7388841.8480            0.80s\n",
      "       136     7371798.8450            0.78s\n",
      "       137     7359325.3498            0.76s\n",
      "       138     7334952.6343            0.74s\n",
      "       139     7330438.7533            0.73s\n",
      "       140     7321886.1052            0.71s\n",
      "       141     7313436.8239            0.69s\n",
      "       142     7305079.7367            0.67s\n",
      "       143     7294791.3042            0.65s\n",
      "       144     7286297.3343            0.64s\n",
      "       145     7277373.3126            0.62s\n",
      "       146     7269110.6490            0.60s\n",
      "       147     7250693.8467            0.58s\n",
      "       148     7237254.5726            0.57s\n",
      "       149     7225238.0891            0.55s\n",
      "       150     7220942.1955            0.53s\n",
      "       151     7201162.2433            0.51s\n",
      "       152     7190357.6286            0.50s\n",
      "       153     7179392.3521            0.48s\n",
      "       154     7175160.7898            0.46s\n",
      "       155     7160157.2767            0.44s\n",
      "       156     7137224.0925            0.42s\n",
      "       157     7120648.8610            0.41s\n",
      "       158     7107460.3756            0.39s\n",
      "       159     7100650.1909            0.37s\n",
      "       160     7092980.5715            0.35s\n",
      "       161     7080039.1533            0.34s\n",
      "       162     7060335.4253            0.32s\n",
      "       163     7051050.2758            0.30s\n",
      "       164     7043830.1069            0.28s\n",
      "       165     7040474.7090            0.26s\n",
      "       166     7033686.7738            0.25s\n",
      "       167     7029010.4916            0.23s\n",
      "       168     7025685.5201            0.21s\n",
      "       169     7021166.8389            0.19s\n",
      "       170     7011986.1715            0.18s\n",
      "       171     6996336.3902            0.16s\n",
      "       172     6992404.7495            0.14s\n",
      "       173     6987746.4918            0.12s\n",
      "       174     6976934.9162            0.11s\n",
      "       175     6972890.8523            0.09s\n",
      "       176     6967521.0321            0.07s\n",
      "       177     6963337.8605            0.05s\n",
      "       178     6934734.6317            0.04s\n",
      "       179     6924554.6689            0.02s\n",
      "       180     6917038.2699            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1    74348374.7562            2.68s\n",
      "         2    66139929.4759            2.67s\n",
      "         3    57608458.0355            2.65s\n",
      "         4    50081951.7558            2.68s\n",
      "         5    45364669.8474            2.69s\n",
      "         6    40300985.2736            2.72s\n",
      "         7    36289172.6837            2.71s\n",
      "         8    33400234.4844            2.70s\n",
      "         9    30887389.4022            2.69s\n",
      "        10    28706187.9576            2.68s\n",
      "        11    26496772.6678            2.67s\n",
      "        12    24967827.7669            2.65s\n",
      "        13    23590601.9085            2.65s\n",
      "        14    22357422.6955            2.66s\n",
      "        15    21158227.6091            2.64s\n",
      "        16    20112760.0036            2.64s\n",
      "        17    19059895.8713            2.62s\n",
      "        18    17816342.7462            2.61s\n",
      "        19    16504299.6702            2.60s\n",
      "        20    15751080.7932            2.60s\n",
      "        21    15009122.0736            2.58s\n",
      "        22    14498138.1872            2.57s\n",
      "        23    13960094.9056            2.55s\n",
      "        24    13489425.6678            2.54s\n",
      "        25    13063572.5908            2.52s\n",
      "        26    12798888.5765            2.52s\n",
      "        27    12402894.6055            2.51s\n",
      "        28    11999331.8003            2.50s\n",
      "        29    11779545.4048            2.48s\n",
      "        30    11507209.5352            2.47s\n",
      "        31    11323476.4928            2.46s\n",
      "        32    11123298.3052            2.45s\n",
      "        33    10976659.5290            2.43s\n",
      "        34    10850690.3957            2.43s\n",
      "        35    10723651.8904            2.43s\n",
      "        36    10522288.2947            2.43s\n",
      "        37    10397121.5706            2.43s\n",
      "        38    10178035.7870            2.43s\n",
      "        39     9981333.9088            2.41s\n",
      "        40     9916239.6163            2.40s\n",
      "        41     9809916.2189            2.39s\n",
      "        42     9675130.8407            2.37s\n",
      "        43     9571086.4019            2.35s\n",
      "        44     9523851.3742            2.33s\n",
      "        45     9406913.2259            2.31s\n",
      "        46     9307057.8943            2.30s\n",
      "        47     9234391.8173            2.27s\n",
      "        48     9146615.0686            2.26s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        49     9055324.0728            2.24s\n",
      "        50     8963495.0164            2.22s\n",
      "        51     8899312.6909            2.21s\n",
      "        52     8849933.9329            2.19s\n",
      "        53     8679634.1267            2.18s\n",
      "        54     8630632.3426            2.17s\n",
      "        55     8534994.3786            2.16s\n",
      "        56     8479250.8423            2.15s\n",
      "        57     8451737.5771            2.14s\n",
      "        58     8401078.9212            2.13s\n",
      "        59     8307730.3606            2.12s\n",
      "        60     8264445.9414            2.11s\n",
      "        61     8235455.4542            2.10s\n",
      "        62     8198529.9692            2.08s\n",
      "        63     8164581.2288            2.07s\n",
      "        64     8135162.9013            2.05s\n",
      "        65     8087378.2519            2.04s\n",
      "        66     8054237.1336            2.02s\n",
      "        67     8027825.5213            2.00s\n",
      "        68     7952992.6025            1.98s\n",
      "        69     7913855.7852            1.97s\n",
      "        70     7885812.0567            1.95s\n",
      "        71     7838161.5641            1.93s\n",
      "        72     7813078.6104            1.91s\n",
      "        73     7757883.3819            1.89s\n",
      "        74     7742771.4938            1.87s\n",
      "        75     7721761.8192            1.85s\n",
      "        76     7709841.2831            1.83s\n",
      "        77     7684035.9762            1.81s\n",
      "        78     7624217.6446            1.79s\n",
      "        79     7601470.9028            1.78s\n",
      "        80     7560839.1976            1.76s\n",
      "        81     7539612.8351            1.74s\n",
      "        82     7521202.0525            1.72s\n",
      "        83     7509139.5751            1.70s\n",
      "        84     7492655.9246            1.69s\n",
      "        85     7481100.0650            1.67s\n",
      "        86     7429387.3380            1.65s\n",
      "        87     7416051.4641            1.63s\n",
      "        88     7394444.4873            1.62s\n",
      "        89     7356115.7706            1.60s\n",
      "        90     7328825.8224            1.58s\n",
      "        91     7302758.3584            1.56s\n",
      "        92     7286551.3007            1.55s\n",
      "        93     7274843.7872            1.53s\n",
      "        94     7262728.8107            1.51s\n",
      "        95     7237579.3618            1.50s\n",
      "        96     7226952.6750            1.48s\n",
      "        97     7213856.7006            1.46s\n",
      "        98     7204828.1566            1.45s\n",
      "        99     7192484.5357            1.43s\n",
      "       100     7168619.5255            1.41s\n",
      "       101     7158004.0291            1.40s\n",
      "       102     7146433.2749            1.38s\n",
      "       103     7137892.1264            1.36s\n",
      "       104     7112501.7857            1.35s\n",
      "       105     7092517.1199            1.33s\n",
      "       106     7072430.7486            1.31s\n",
      "       107     7065334.3700            1.29s\n",
      "       108     7058461.6625            1.28s\n",
      "       109     7039131.6040            1.26s\n",
      "       110     7031694.8417            1.24s\n",
      "       111     7014384.2155            1.22s\n",
      "       112     7001580.4780            1.20s\n",
      "       113     6993688.8976            1.18s\n",
      "       114     6985514.1725            1.17s\n",
      "       115     6969511.3305            1.15s\n",
      "       116     6951839.6743            1.13s\n",
      "       117     6944979.5572            1.11s\n",
      "       118     6932166.7909            1.09s\n",
      "       119     6924267.4412            1.07s\n",
      "       120     6916536.1065            1.06s\n",
      "       121     6906837.9806            1.04s\n",
      "       122     6890971.6904            1.02s\n",
      "       123     6884399.0909            1.00s\n",
      "       124     6865116.7388            0.99s\n",
      "       125     6858551.5944            0.97s\n",
      "       126     6840080.5147            0.95s\n",
      "       127     6819660.9065            0.93s\n",
      "       128     6810492.0170            0.92s\n",
      "       129     6794575.3568            0.90s\n",
      "       130     6788637.9750            0.88s\n",
      "       131     6774773.7937            0.86s\n",
      "       132     6742093.8676            0.85s\n",
      "       133     6733459.0125            0.83s\n",
      "       134     6729148.1159            0.81s\n",
      "       135     6724176.7058            0.79s\n",
      "       136     6718039.6944            0.77s\n",
      "       137     6712657.0943            0.75s\n",
      "       138     6695088.9758            0.74s\n",
      "       139     6678312.5832            0.72s\n",
      "       140     6665669.3855            0.70s\n",
      "       141     6660423.3091            0.68s\n",
      "       142     6653422.9548            0.67s\n",
      "       143     6634138.4628            0.65s\n",
      "       144     6623012.0380            0.63s\n",
      "       145     6616736.8711            0.62s\n",
      "       146     6609695.4332            0.60s\n",
      "       147     6596034.7809            0.58s\n",
      "       148     6591821.3170            0.56s\n",
      "       149     6587377.4737            0.54s\n",
      "       150     6568159.5606            0.53s\n",
      "       151     6562778.7332            0.51s\n",
      "       152     6552702.4118            0.49s\n",
      "       153     6546134.2667            0.47s\n",
      "       154     6540481.6260            0.46s\n",
      "       155     6532556.8194            0.44s\n",
      "       156     6527526.7203            0.42s\n",
      "       157     6515692.7402            0.40s\n",
      "       158     6508348.6797            0.39s\n",
      "       159     6499600.7588            0.37s\n",
      "       160     6494570.3330            0.35s\n",
      "       161     6484889.1450            0.33s\n",
      "       162     6477499.2254            0.31s\n",
      "       163     6464270.6433            0.30s\n",
      "       164     6459428.5207            0.28s\n",
      "       165     6455072.2472            0.26s\n",
      "       166     6447806.5215            0.24s\n",
      "       167     6434381.6298            0.23s\n",
      "       168     6428407.0038            0.21s\n",
      "       169     6408038.4055            0.19s\n",
      "       170     6402280.9092            0.18s\n",
      "       171     6395677.3447            0.16s\n",
      "       172     6390449.9662            0.14s\n",
      "       173     6387285.1596            0.12s\n",
      "       174     6379367.3712            0.11s\n",
      "       175     6362065.9611            0.09s\n",
      "       176     6357417.5329            0.07s\n",
      "       177     6347332.9854            0.05s\n",
      "       178     6334401.7934            0.04s\n",
      "       179     6330288.2059            0.02s\n",
      "       180     6325125.4193            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1    70733532.1339            1.43s\n",
      "         2    63217727.7487            1.42s\n",
      "         3    55927581.3378            1.35s\n",
      "         4    49079678.9323            1.40s\n",
      "         5    44041070.1302            1.36s\n",
      "         6    40362723.2276            1.39s\n",
      "         7    36665760.8005            1.40s\n",
      "         8    33989072.7657            1.37s\n",
      "         9    31554538.2614            1.35s\n",
      "        10    28656724.7455            1.36s\n",
      "        11    26018417.6513            1.35s\n",
      "        12    24481485.6989            1.33s\n",
      "        13    23077512.5397            1.31s\n",
      "        14    21767079.7378            1.29s\n",
      "        15    19958684.4925            1.29s\n",
      "        16    19142870.2268            1.28s\n",
      "        17    18302303.6075            1.26s\n",
      "        18    17226331.3017            1.26s\n",
      "        19    16318322.5363            1.24s\n",
      "        20    15554170.3911            1.26s\n",
      "        21    14667665.6650            1.25s\n",
      "        22    14099720.6208            1.26s\n",
      "        23    13557056.2808            1.25s\n",
      "        24    13196125.1247            1.25s\n",
      "        25    12783146.1931            1.24s\n",
      "        26    12523174.2919            1.22s\n",
      "        27    12037373.8523            1.21s\n",
      "        28    11630671.5672            1.20s\n",
      "        29    11363159.5158            1.19s\n",
      "        30    11154997.6704            1.20s\n",
      "        31    11004785.5377            1.18s\n",
      "        32    10818678.1611            1.18s\n",
      "        33    10545735.4647            1.16s\n",
      "        34    10386993.6246            1.18s\n",
      "        35    10260157.5600            1.17s\n",
      "        36    10005239.3050            1.15s\n",
      "        37     9888269.2016            1.15s\n",
      "        38     9792095.2610            1.14s\n",
      "        39     9667100.4335            1.13s\n",
      "        40     9529483.6642            1.12s\n",
      "        41     9372715.2285            1.13s\n",
      "        42     9236617.3574            1.11s\n",
      "        43     9158144.2840            1.12s\n",
      "        44     9053213.4589            1.10s\n",
      "        45     8912800.4595            1.09s\n",
      "        46     8863545.4057            1.08s\n",
      "        47     8750655.6126            1.07s\n",
      "        48     8652390.8622            1.06s\n",
      "        49     8567881.3465            1.05s\n",
      "        50     8489100.8470            1.04s\n",
      "        51     8440409.1256            1.02s\n",
      "        52     8406654.4094            1.01s\n",
      "        53     8328161.0893            0.99s\n",
      "        54     8264849.4391            0.98s\n",
      "        55     8190974.8085            0.97s\n",
      "        56     8143588.0601            0.97s\n",
      "        57     8102069.5470            0.96s\n",
      "        58     8049303.7874            0.95s\n",
      "        59     7957173.6313            0.94s\n",
      "        60     7922295.3302            0.93s\n",
      "        61     7870846.0930            0.92s\n",
      "        62     7838785.2881            0.91s\n",
      "        63     7803664.3901            0.89s\n",
      "        64     7778537.9960            0.88s\n",
      "        65     7761540.3976            0.87s\n",
      "        66     7726179.6815            0.86s\n",
      "        67     7703876.1858            0.85s\n",
      "        68     7642155.2997            0.84s\n",
      "        69     7607135.6478            0.83s\n",
      "        70     7557115.4723            0.83s\n",
      "        71     7513982.4081            0.83s\n",
      "        72     7485898.7840            0.82s\n",
      "        73     7454232.3980            0.81s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        74     7416755.1568            0.80s\n",
      "        75     7394423.9909            0.79s\n",
      "        76     7382030.5571            0.79s\n",
      "        77     7362843.6588            0.78s\n",
      "        78     7344523.2951            0.77s\n",
      "        79     7321372.8548            0.76s\n",
      "        80     7311315.8184            0.75s\n",
      "        81     7282611.8867            0.74s\n",
      "        82     7251293.8379            0.73s\n",
      "        83     7228323.1624            0.72s\n",
      "        84     7212722.6041            0.71s\n",
      "        85     7190648.9012            0.70s\n",
      "        86     7176915.5604            0.70s\n",
      "        87     7160185.6751            0.69s\n",
      "        88     7147086.1637            0.68s\n",
      "        89     7135001.0341            0.67s\n",
      "        90     7125259.7850            0.66s\n",
      "        91     7109356.0675            0.65s\n",
      "        92     7082414.3776            0.65s\n",
      "        93     7067219.9950            0.64s\n",
      "        94     7057443.9488            0.63s\n",
      "        95     7041562.9578            0.62s\n",
      "        96     7034029.7346            0.61s\n",
      "        97     7024944.8105            0.60s\n",
      "        98     7015377.8218            0.59s\n",
      "        99     7003216.5941            0.59s\n",
      "       100     6982271.4962            0.58s\n",
      "       101     6973732.1408            0.57s\n",
      "       102     6963562.8396            0.56s\n",
      "       103     6952658.0730            0.55s\n",
      "       104     6934386.8419            0.55s\n",
      "       105     6904043.2927            0.54s\n",
      "       106     6879507.3921            0.53s\n",
      "       107     6854687.4973            0.53s\n",
      "       108     6847891.2393            0.52s\n",
      "       109     6827946.1413            0.51s\n",
      "       110     6820268.9547            0.50s\n",
      "       111     6814667.6898            0.49s\n",
      "       112     6757166.7245            0.49s\n",
      "       113     6750323.6043            0.48s\n",
      "       114     6739704.3653            0.47s\n",
      "       115     6729505.6013            0.46s\n",
      "       116     6720667.4214            0.46s\n",
      "       117     6696604.6979            0.45s\n",
      "       118     6687309.5392            0.44s\n",
      "       119     6677819.8784            0.43s\n",
      "       120     6664582.9801            0.43s\n",
      "       121     6654262.1135            0.42s\n",
      "       122     6644480.4819            0.41s\n",
      "       123     6638057.6735            0.40s\n",
      "       124     6630072.4990            0.40s\n",
      "       125     6623589.6489            0.39s\n",
      "       126     6617960.9765            0.38s\n",
      "       127     6607446.7123            0.37s\n",
      "       128     6600687.1960            0.37s\n",
      "       129     6576037.3025            0.36s\n",
      "       130     6560117.9795            0.35s\n",
      "       131     6543453.1001            0.34s\n",
      "       132     6539069.5382            0.34s\n",
      "       133     6527481.9825            0.33s\n",
      "       134     6511004.7218            0.32s\n",
      "       135     6504929.1551            0.32s\n",
      "       136     6496139.7516            0.31s\n",
      "       137     6467719.6224            0.30s\n",
      "       138     6454506.0404            0.30s\n",
      "       139     6446015.1611            0.29s\n",
      "       140     6434402.0933            0.28s\n",
      "       141     6423967.7998            0.27s\n",
      "       142     6420142.4654            0.27s\n",
      "       143     6413205.2613            0.26s\n",
      "       144     6403150.0745            0.25s\n",
      "       145     6395515.1177            0.24s\n",
      "       146     6384750.9603            0.24s\n",
      "       147     6380766.9908            0.23s\n",
      "       148     6370058.9104            0.22s\n",
      "       149     6364010.5039            0.22s\n",
      "       150     6356499.4141            0.21s\n",
      "       151     6351256.2134            0.20s\n",
      "       152     6338763.9616            0.20s\n",
      "       153     6331198.3144            0.19s\n",
      "       154     6324878.2142            0.18s\n",
      "       155     6308126.4155            0.17s\n",
      "       156     6304122.2075            0.17s\n",
      "       157     6291189.9286            0.16s\n",
      "       158     6281193.6688            0.15s\n",
      "       159     6270477.6551            0.15s\n",
      "       160     6250041.3217            0.14s\n",
      "       161     6244307.8583            0.13s\n",
      "       162     6238267.9241            0.13s\n",
      "       163     6230195.2604            0.12s\n",
      "       164     6224612.3142            0.11s\n",
      "       165     6220026.3212            0.10s\n",
      "       166     6210522.1220            0.10s\n",
      "       167     6203726.0809            0.09s\n",
      "       168     6194289.9890            0.08s\n",
      "       169     6190801.3746            0.08s\n",
      "       170     6183231.4817            0.07s\n",
      "       171     6167224.9669            0.06s\n",
      "       172     6161194.9789            0.06s\n",
      "       173     6153546.5442            0.05s\n",
      "       174     6145588.8994            0.04s\n",
      "       175     6131148.7856            0.03s\n",
      "       176     6128531.5268            0.03s\n",
      "       177     6120060.1156            0.02s\n",
      "       178     6115695.6457            0.01s\n",
      "       179     6108578.0060            0.01s\n",
      "       180     6104972.1738            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'min_samples_leaf': 60}, 0.6367946263116123)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "param_test1 = {'n_estimators':range(20,200,20)}\n",
    "param_test2 = {'max_depth':range(5,16,2), 'min_samples_split':range(200,1001,100)}\n",
    "param_test3 = {'min_samples_leaf': [60] }\n",
    "\n",
    "gradient_boosting_cv = GridSearchCV(estimator = GradientBoostingRegressor(random_state=1, min_samples_leaf = 60,\n",
    "                                              n_estimators=180, loss = 'ls', learning_rate = 0.1, max_features = 'sqrt',\n",
    "                                              criterion = 'mse', verbose = 10, max_depth = 5, min_samples_split = 300)\n",
    "                                    , param_grid = param_test3, cv=5)\n",
    "\n",
    "t3 = time.time()\n",
    "gradient_boosting_cv.fit(X_train_scaled, y_train)\n",
    "t_gradient_boosting = time.time() - t3\n",
    "\n",
    "gradient_boosting_cv.best_params_, gradient_boosting_cv.best_score_\n",
    "#gradient_boosting_cv.score(X_train,y_train), gradient_boosting_cv.feature_importances_, gradient_boosting_cv.train_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1    70733532.1339            1.43s\n",
      "         2    63217727.7487            1.42s\n",
      "         3    55927581.3378            1.41s\n",
      "         4    49079678.9323            1.40s\n",
      "         5    44041070.1302            1.40s\n",
      "         6    40362723.2276            1.39s\n",
      "         7    36665760.8005            1.40s\n",
      "         8    33989072.7657            1.37s\n",
      "         9    31554538.2614            1.36s\n",
      "        10    28656724.7455            1.37s\n",
      "        11    26018417.6513            1.35s\n",
      "        12    24481485.6989            1.33s\n",
      "        13    23077512.5397            1.31s\n",
      "        14    21767079.7378            1.29s\n",
      "        15    19958684.4925            1.28s\n",
      "        16    19142870.2268            1.28s\n",
      "        17    18302303.6075            1.25s\n",
      "        18    17226331.3017            1.24s\n",
      "        19    16318322.5363            1.23s\n",
      "        20    15554170.3911            1.24s\n",
      "        21    14667665.6650            1.23s\n",
      "        22    14099720.6208            1.25s\n",
      "        23    13557056.2808            1.23s\n",
      "        24    13196125.1247            1.23s\n",
      "        25    12783146.1931            1.22s\n",
      "        26    12523174.2919            1.20s\n",
      "        27    12037373.8523            1.21s\n",
      "        28    11630671.5672            1.20s\n",
      "        29    11363159.5158            1.19s\n",
      "        30    11154997.6704            1.20s\n",
      "        31    11004785.5377            1.18s\n",
      "        32    10818678.1611            1.18s\n",
      "        33    10545735.4647            1.16s\n",
      "        34    10386993.6246            1.18s\n",
      "        35    10260157.5600            1.16s\n",
      "        36    10005239.3050            1.14s\n",
      "        37     9888269.2016            1.15s\n",
      "        38     9792095.2610            1.13s\n",
      "        39     9667100.4335            1.13s\n",
      "        40     9529483.6642            1.11s\n",
      "        41     9372715.2285            1.12s\n",
      "        42     9236617.3574            1.10s\n",
      "        43     9158144.2840            1.10s\n",
      "        44     9053213.4589            1.09s\n",
      "        45     8912800.4595            1.08s\n",
      "        46     8863545.4057            1.06s\n",
      "        47     8750655.6126            1.05s\n",
      "        48     8652390.8622            1.04s\n",
      "        49     8567881.3465            1.03s\n",
      "        50     8489100.8470            1.02s\n",
      "        51     8440409.1256            1.01s\n",
      "        52     8406654.4094            1.00s\n",
      "        53     8328161.0893            0.99s\n",
      "        54     8264849.4391            0.98s\n",
      "        55     8190974.8085            0.97s\n",
      "        56     8143588.0601            0.96s\n",
      "        57     8102069.5470            0.95s\n",
      "        58     8049303.7874            0.94s\n",
      "        59     7957173.6313            0.93s\n",
      "        60     7922295.3302            0.92s\n",
      "        61     7870846.0930            0.91s\n",
      "        62     7838785.2881            0.90s\n",
      "        63     7803664.3901            0.89s\n",
      "        64     7778537.9960            0.87s\n",
      "        65     7761540.3976            0.86s\n",
      "        66     7726179.6815            0.85s\n",
      "        67     7703876.1858            0.84s\n",
      "        68     7642155.2997            0.84s\n",
      "        69     7607135.6478            0.83s\n",
      "        70     7557115.4723            0.82s\n",
      "        71     7513982.4081            0.82s\n",
      "        72     7485898.7840            0.81s\n",
      "        73     7454232.3980            0.80s\n",
      "        74     7416755.1568            0.79s\n",
      "        75     7394423.9909            0.78s\n",
      "        76     7382030.5571            0.78s\n",
      "        77     7362843.6588            0.77s\n",
      "        78     7344523.2951            0.76s\n",
      "        79     7321372.8548            0.75s\n",
      "        80     7311315.8184            0.74s\n",
      "        81     7282611.8867            0.73s\n",
      "        82     7251293.8379            0.72s\n",
      "        83     7228323.1624            0.72s\n",
      "        84     7212722.6041            0.71s\n",
      "        85     7190648.9012            0.70s\n",
      "        86     7176915.5604            0.69s\n",
      "        87     7160185.6751            0.68s\n",
      "        88     7147086.1637            0.67s\n",
      "        89     7135001.0341            0.66s\n",
      "        90     7125259.7850            0.65s\n",
      "        91     7109356.0675            0.64s\n",
      "        92     7082414.3776            0.64s\n",
      "        93     7067219.9950            0.63s\n",
      "        94     7057443.9488            0.62s\n",
      "        95     7041562.9578            0.62s\n",
      "        96     7034029.7346            0.61s\n",
      "        97     7024944.8105            0.60s\n",
      "        98     7015377.8218            0.59s\n",
      "        99     7003216.5941            0.58s\n",
      "       100     6982271.4962            0.57s\n",
      "       101     6973732.1408            0.56s\n",
      "       102     6963562.8396            0.56s\n",
      "       103     6952658.0730            0.55s\n",
      "       104     6934386.8419            0.54s\n",
      "       105     6904043.2927            0.53s\n",
      "       106     6879507.3921            0.53s\n",
      "       107     6854687.4973            0.52s\n",
      "       108     6847891.2393            0.51s\n",
      "       109     6827946.1413            0.50s\n",
      "       110     6820268.9547            0.50s\n",
      "       111     6814667.6898            0.49s\n",
      "       112     6757166.7245            0.48s\n",
      "       113     6750323.6043            0.47s\n",
      "       114     6739704.3653            0.47s\n",
      "       115     6729505.6013            0.46s\n",
      "       116     6720667.4214            0.45s\n",
      "       117     6696604.6979            0.45s\n",
      "       118     6687309.5392            0.44s\n",
      "       119     6677819.8784            0.43s\n",
      "       120     6664582.9801            0.42s\n",
      "       121     6654262.1135            0.42s\n",
      "       122     6644480.4819            0.41s\n",
      "       123     6638057.6735            0.40s\n",
      "       124     6630072.4990            0.40s\n",
      "       125     6623589.6489            0.39s\n",
      "       126     6617960.9765            0.38s\n",
      "       127     6607446.7123            0.37s\n",
      "       128     6600687.1960            0.37s\n",
      "       129     6576037.3025            0.36s\n",
      "       130     6560117.9795            0.35s\n",
      "       131     6543453.1001            0.34s\n",
      "       132     6539069.5382            0.34s\n",
      "       133     6527481.9825            0.33s\n",
      "       134     6511004.7218            0.32s\n",
      "       135     6504929.1551            0.32s\n",
      "       136     6496139.7516            0.31s\n",
      "       137     6467719.6224            0.30s\n",
      "       138     6454506.0404            0.30s\n",
      "       139     6446015.1611            0.29s\n",
      "       140     6434402.0933            0.28s\n",
      "       141     6423967.7998            0.28s\n",
      "       142     6420142.4654            0.27s\n",
      "       143     6413205.2613            0.26s\n",
      "       144     6403150.0745            0.25s\n",
      "       145     6395515.1177            0.25s\n",
      "       146     6384750.9603            0.24s\n",
      "       147     6380766.9908            0.23s\n",
      "       148     6370058.9104            0.22s\n",
      "       149     6364010.5039            0.22s\n",
      "       150     6356499.4141            0.21s\n",
      "       151     6351256.2134            0.20s\n",
      "       152     6338763.9616            0.20s\n",
      "       153     6331198.3144            0.19s\n",
      "       154     6324878.2142            0.18s\n",
      "       155     6308126.4155            0.18s\n",
      "       156     6304122.2075            0.17s\n",
      "       157     6291189.9286            0.16s\n",
      "       158     6281193.6688            0.15s\n",
      "       159     6270477.6551            0.15s\n",
      "       160     6250041.3217            0.14s\n",
      "       161     6244307.8583            0.13s\n",
      "       162     6238267.9241            0.13s\n",
      "       163     6230195.2604            0.12s\n",
      "       164     6224612.3142            0.11s\n",
      "       165     6220026.3212            0.11s\n",
      "       166     6210522.1220            0.10s\n",
      "       167     6203726.0809            0.09s\n",
      "       168     6194289.9890            0.08s\n",
      "       169     6190801.3746            0.08s\n",
      "       170     6183231.4817            0.07s\n",
      "       171     6167224.9669            0.06s\n",
      "       172     6161194.9789            0.06s\n",
      "       173     6153546.5442            0.05s\n",
      "       174     6145588.8994            0.04s\n",
      "       175     6131148.7856            0.03s\n",
      "       176     6128531.5268            0.03s\n",
      "       177     6120060.1156            0.02s\n",
      "       178     6115695.6457            0.01s\n",
      "       179     6108578.0060            0.01s\n",
      "       180     6104972.1738            0.00s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Votes</th>\n",
       "      <th>city</th>\n",
       "      <th>party</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5358.045529</td>\n",
       "      <td>8163</td>\n",
       "      <td>Freiburg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Bernhard Lukau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6410.465060</td>\n",
       "      <td>7711</td>\n",
       "      <td>Freiburg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Michael Theuring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5226.157179</td>\n",
       "      <td>6504</td>\n",
       "      <td>Freiburg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Elmar Ertmer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5354.590421</td>\n",
       "      <td>7703</td>\n",
       "      <td>Freiburg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Jens Bellemann</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4608.469749</td>\n",
       "      <td>8149</td>\n",
       "      <td>Freiburg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Martin Polheim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4170.567824</td>\n",
       "      <td>4260</td>\n",
       "      <td>Freiburg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Robert Hagerman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3137.534641</td>\n",
       "      <td>4721</td>\n",
       "      <td>Freiburg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Michael Braun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4862.036928</td>\n",
       "      <td>6985</td>\n",
       "      <td>Freiburg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Bernd Fulde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6973.738649</td>\n",
       "      <td>8931</td>\n",
       "      <td>Freiburg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Karl Schwarz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4744.433272</td>\n",
       "      <td>6176</td>\n",
       "      <td>Freiburg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Jonas Möhle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4049.264364</td>\n",
       "      <td>5674</td>\n",
       "      <td>Freiburg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Alessandro Bertonasco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4821.162201</td>\n",
       "      <td>8008</td>\n",
       "      <td>Freiburg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Bernd Domnick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7117.130330</td>\n",
       "      <td>8263</td>\n",
       "      <td>Freiburg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Jack Gelfort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3592.131383</td>\n",
       "      <td>5585</td>\n",
       "      <td>Freiburg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Michael Pfeiffer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5318.370030</td>\n",
       "      <td>8607</td>\n",
       "      <td>Freiburg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Tilman Mehler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3503.619335</td>\n",
       "      <td>3979</td>\n",
       "      <td>Freiburg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Sandro Schüler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10943.176999</td>\n",
       "      <td>9278</td>\n",
       "      <td>Freiburg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Andreas Schumacher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3209.136084</td>\n",
       "      <td>4680</td>\n",
       "      <td>Freiburg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Thomas Müller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4101.996902</td>\n",
       "      <td>4934</td>\n",
       "      <td>Freiburg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Katharina Maria Lukau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4685.705238</td>\n",
       "      <td>7827</td>\n",
       "      <td>Freiburg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Heinz-Jürgen Schlag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8720.117867</td>\n",
       "      <td>12073</td>\n",
       "      <td>Freiburg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Detlef Huber</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>6789.007310</td>\n",
       "      <td>9948</td>\n",
       "      <td>Freiburg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Dubravko Mandic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4483.160717</td>\n",
       "      <td>4020</td>\n",
       "      <td>Freiburg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Marie-Luise Gatzweiler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4634.044715</td>\n",
       "      <td>5607</td>\n",
       "      <td>Freiburg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Marco Erat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3494.184895</td>\n",
       "      <td>2149</td>\n",
       "      <td>Heidelberg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Maximilian Wolf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3266.964175</td>\n",
       "      <td>2386</td>\n",
       "      <td>Heidelberg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Cornelia Degeratu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>6660.500377</td>\n",
       "      <td>5280</td>\n",
       "      <td>Heidelberg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Jan Michael Kröhl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3855.664876</td>\n",
       "      <td>2249</td>\n",
       "      <td>Heidelberg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Florian Epple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3544.427774</td>\n",
       "      <td>2016</td>\n",
       "      <td>Heidelberg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Simone Kammerer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3240.879916</td>\n",
       "      <td>1977</td>\n",
       "      <td>Heidelberg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Ilona Schmittova</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3274.491374</td>\n",
       "      <td>2319</td>\n",
       "      <td>Heidelberg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Liane Wolf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3293.578663</td>\n",
       "      <td>2235</td>\n",
       "      <td>Heidelberg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Helmut Fleischmann</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3756.108022</td>\n",
       "      <td>2137</td>\n",
       "      <td>Heidelberg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Wilhelm Wabel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3025.037644</td>\n",
       "      <td>1698</td>\n",
       "      <td>Heidelberg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Helga Striehl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>7812.249706</td>\n",
       "      <td>6856</td>\n",
       "      <td>Heidelberg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Timethy Bartesch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2915.899181</td>\n",
       "      <td>2116</td>\n",
       "      <td>Heidelberg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Bernd Wolf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4894.554627</td>\n",
       "      <td>5295</td>\n",
       "      <td>Heidelberg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Martin Jacob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>15109.714061</td>\n",
       "      <td>9375</td>\n",
       "      <td>Heidelberg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Matthias Niebel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4688.333922</td>\n",
       "      <td>5316</td>\n",
       "      <td>Heidelberg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Manfred Hanke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2384.087018</td>\n",
       "      <td>2065</td>\n",
       "      <td>Heidelberg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Dieter-Eugen Glaser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>3068.993173</td>\n",
       "      <td>1947</td>\n",
       "      <td>Heidelberg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Claudia Gregor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2803.962824</td>\n",
       "      <td>2473</td>\n",
       "      <td>Heidelberg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Dieter Bowe-Karamann</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4218.364131</td>\n",
       "      <td>2979</td>\n",
       "      <td>Heidelberg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Stefan Holzmann</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>4830.471880</td>\n",
       "      <td>4708</td>\n",
       "      <td>Heidelberg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Klaus Blanck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>3309.414872</td>\n",
       "      <td>1692</td>\n",
       "      <td>Heidelberg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Kai Gunther Lehmann</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>3812.458430</td>\n",
       "      <td>2735</td>\n",
       "      <td>Heidelberg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Jens Riedel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>9199.503501</td>\n",
       "      <td>6986</td>\n",
       "      <td>Heidelberg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Sven Geschinski</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>3008.012638</td>\n",
       "      <td>1921</td>\n",
       "      <td>Heidelberg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Arthur Leibham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>6783.030052</td>\n",
       "      <td>4691</td>\n",
       "      <td>Heidelberg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Birgit Fleischmann</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>7298.790370</td>\n",
       "      <td>5033</td>\n",
       "      <td>Heidelberg</td>\n",
       "      <td>AfD</td>\n",
       "      <td>Volker Kunze</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Prediction  Votes        city party                    name\n",
       "0    5358.045529   8163    Freiburg   AfD          Bernhard Lukau\n",
       "1    6410.465060   7711    Freiburg   AfD        Michael Theuring\n",
       "2    5226.157179   6504    Freiburg   AfD            Elmar Ertmer\n",
       "3    5354.590421   7703    Freiburg   AfD          Jens Bellemann\n",
       "4    4608.469749   8149    Freiburg   AfD          Martin Polheim\n",
       "5    4170.567824   4260    Freiburg   AfD         Robert Hagerman\n",
       "6    3137.534641   4721    Freiburg   AfD           Michael Braun\n",
       "7    4862.036928   6985    Freiburg   AfD             Bernd Fulde\n",
       "8    6973.738649   8931    Freiburg   AfD            Karl Schwarz\n",
       "9    4744.433272   6176    Freiburg   AfD             Jonas Möhle\n",
       "10   4049.264364   5674    Freiburg   AfD   Alessandro Bertonasco\n",
       "11   4821.162201   8008    Freiburg   AfD           Bernd Domnick\n",
       "12   7117.130330   8263    Freiburg   AfD            Jack Gelfort\n",
       "13   3592.131383   5585    Freiburg   AfD        Michael Pfeiffer\n",
       "14   5318.370030   8607    Freiburg   AfD           Tilman Mehler\n",
       "15   3503.619335   3979    Freiburg   AfD          Sandro Schüler\n",
       "16  10943.176999   9278    Freiburg   AfD      Andreas Schumacher\n",
       "17   3209.136084   4680    Freiburg   AfD           Thomas Müller\n",
       "18   4101.996902   4934    Freiburg   AfD   Katharina Maria Lukau\n",
       "19   4685.705238   7827    Freiburg   AfD     Heinz-Jürgen Schlag\n",
       "20   8720.117867  12073    Freiburg   AfD            Detlef Huber\n",
       "21   6789.007310   9948    Freiburg   AfD         Dubravko Mandic\n",
       "22   4483.160717   4020    Freiburg   AfD  Marie-Luise Gatzweiler\n",
       "23   4634.044715   5607    Freiburg   AfD              Marco Erat\n",
       "24   3494.184895   2149  Heidelberg   AfD         Maximilian Wolf\n",
       "25   3266.964175   2386  Heidelberg   AfD       Cornelia Degeratu\n",
       "26   6660.500377   5280  Heidelberg   AfD       Jan Michael Kröhl\n",
       "27   3855.664876   2249  Heidelberg   AfD           Florian Epple\n",
       "28   3544.427774   2016  Heidelberg   AfD         Simone Kammerer\n",
       "29   3240.879916   1977  Heidelberg   AfD        Ilona Schmittova\n",
       "30   3274.491374   2319  Heidelberg   AfD              Liane Wolf\n",
       "31   3293.578663   2235  Heidelberg   AfD      Helmut Fleischmann\n",
       "32   3756.108022   2137  Heidelberg   AfD           Wilhelm Wabel\n",
       "33   3025.037644   1698  Heidelberg   AfD           Helga Striehl\n",
       "34   7812.249706   6856  Heidelberg   AfD        Timethy Bartesch\n",
       "35   2915.899181   2116  Heidelberg   AfD              Bernd Wolf\n",
       "36   4894.554627   5295  Heidelberg   AfD            Martin Jacob\n",
       "37  15109.714061   9375  Heidelberg   AfD         Matthias Niebel\n",
       "38   4688.333922   5316  Heidelberg   AfD           Manfred Hanke\n",
       "39   2384.087018   2065  Heidelberg   AfD     Dieter-Eugen Glaser\n",
       "40   3068.993173   1947  Heidelberg   AfD          Claudia Gregor\n",
       "41   2803.962824   2473  Heidelberg   AfD    Dieter Bowe-Karamann\n",
       "42   4218.364131   2979  Heidelberg   AfD         Stefan Holzmann\n",
       "43   4830.471880   4708  Heidelberg   AfD            Klaus Blanck\n",
       "44   3309.414872   1692  Heidelberg   AfD     Kai Gunther Lehmann\n",
       "45   3812.458430   2735  Heidelberg   AfD             Jens Riedel\n",
       "46   9199.503501   6986  Heidelberg   AfD         Sven Geschinski\n",
       "47   3008.012638   1921  Heidelberg   AfD          Arthur Leibham\n",
       "48   6783.030052   4691  Heidelberg   AfD      Birgit Fleischmann\n",
       "49   7298.790370   5033  Heidelberg   AfD            Volker Kunze"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "grad_boost = GradientBoostingRegressor(random_state=1, min_samples_leaf = 60,\n",
    "                                              n_estimators=180, loss = 'ls', learning_rate = 0.1, max_features = 'sqrt',\n",
    "                                              criterion = 'mse', verbose = 2, max_depth = 5, min_samples_split = 300)\n",
    "\n",
    "grad_boost.fit(X_train_scaled, y_train)\n",
    "\n",
    "gradient_boosting_pred = grad_boost.predict(X_test_scaled)\n",
    "feature_importances_gradient_boost = grad_boost.feature_importances_\n",
    "\n",
    "coef_gradient_boost = pd.DataFrame({'Variable': X2, 'Coefficient': feature_importances_gradient_boost}) # Create a dataset with the estimated coefficients\n",
    "coef_gradient_boost.to_excel(r'C:\\Users\\mariu\\Desktop\\Project\\Coefficients_Random_Forest.xlsx')\n",
    "\n",
    "prediction_gradient_boost = pd.DataFrame({'Prediction': \n",
    "gradient_boosting_pred, 'Votes': dataframe_test['votes'], 'city': dataframe_test['city'], 'party': dataframe_test['party'], 'name': dataframe_test['Name_total']})\n",
    "prediction_gradient_boost.to_excel(r'C:\\Users\\mariu\\Desktop\\Project\\Prediction_Gradient_Boost.xlsx')\n",
    "\n",
    "prediction_gradient_boost.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot clone object '<class 'sklearn.svm.classes.LinearSVR'>' (type <class 'abc.ABCMeta'>): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' methods.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-bd28905ce4d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0msvr_cv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinearSVR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparam_grid_svr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0msvr_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0msvr_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    630\u001b[0m         \u001b[0mn_splits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_n_splits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 632\u001b[1;33m         \u001b[0mbase_estimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m         parallel = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mclone\u001b[1;34m(estimator, safe)\u001b[0m\n\u001b[0;32m     58\u001b[0m                             \u001b[1;34m\"it does not seem to be a scikit-learn estimator \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m                             \u001b[1;34m\"as it does not implement a 'get_params' methods.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m                             % (repr(estimator), type(estimator)))\n\u001b[0m\u001b[0;32m     61\u001b[0m     \u001b[0mklass\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mnew_object_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot clone object '<class 'sklearn.svm.classes.LinearSVR'>' (type <class 'abc.ABCMeta'>): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' methods."
     ]
    }
   ],
   "source": [
    "Cs = [1, 10, 100, 1000, 2000, 3000, 4000]\n",
    "param_grid_svr = {'C': Cs}\n",
    "\n",
    "SVR = LinearSVR\n",
    "\n",
    "svr_cv = GridSearchCV(estimator = LinearSVR, param_grid = param_grid_svr, cv=5, n_jobs = -1, verbose = 10)\n",
    "svr_cv.fit(X_train_scaled, y_train)\n",
    "svr_cv.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.786516906590885,\n",
       "       Prediction  Votes        city party                    name\n",
       " 0   35857.326257   8163    Freiburg   AfD          Bernhard Lukau\n",
       " 1   36070.631003   7711    Freiburg   AfD        Michael Theuring\n",
       " 2   35477.644329   6504    Freiburg   AfD            Elmar Ertmer\n",
       " 3   35670.631190   7703    Freiburg   AfD          Jens Bellemann\n",
       " 4   34499.557690   8149    Freiburg   AfD          Martin Polheim\n",
       " 5   34033.282551   4260    Freiburg   AfD         Robert Hagerman\n",
       " 6   34477.430950   4721    Freiburg   AfD           Michael Braun\n",
       " 7   35112.160703   6985    Freiburg   AfD             Bernd Fulde\n",
       " 8   37197.414645   8931    Freiburg   AfD            Karl Schwarz\n",
       " 9   34947.407742   6176    Freiburg   AfD             Jonas Möhle\n",
       " 10  31304.568099   5674    Freiburg   AfD   Alessandro Bertonasco\n",
       " 11  35297.685631   8008    Freiburg   AfD           Bernd Domnick\n",
       " 12  36074.070146   8263    Freiburg   AfD            Jack Gelfort\n",
       " 13  34110.095704   5585    Freiburg   AfD        Michael Pfeiffer\n",
       " 14  34665.946942   8607    Freiburg   AfD           Tilman Mehler\n",
       " 15  33671.885346   3979    Freiburg   AfD          Sandro Schüler\n",
       " 16  37489.519445   9278    Freiburg   AfD      Andreas Schumacher\n",
       " 17  35831.491819   4680    Freiburg   AfD           Thomas Müller\n",
       " 18  34482.091420   4934    Freiburg   AfD   Katharina Maria Lukau\n",
       " 19  36450.360800   7827    Freiburg   AfD     Heinz-Jürgen Schlag\n",
       " 20  37028.090782  12073    Freiburg   AfD            Detlef Huber\n",
       " 21  34142.246754   9948    Freiburg   AfD         Dubravko Mandic\n",
       " 22  36874.813598   4020    Freiburg   AfD  Marie-Luise Gatzweiler\n",
       " 23  32249.896337   5607    Freiburg   AfD              Marco Erat\n",
       " 24   5525.522649   2149  Heidelberg   AfD         Maximilian Wolf\n",
       " 25   2712.794740   2386  Heidelberg   AfD       Cornelia Degeratu\n",
       " 26   7279.438483   5280  Heidelberg   AfD       Jan Michael Kröhl\n",
       " 27   5695.473632   2249  Heidelberg   AfD           Florian Epple\n",
       " 28   5116.098293   2016  Heidelberg   AfD         Simone Kammerer\n",
       " 29  14655.499769   1977  Heidelberg   AfD        Ilona Schmittova\n",
       " 30   5348.352663   2319  Heidelberg   AfD              Liane Wolf\n",
       " 31   5299.372590   2235  Heidelberg   AfD      Helmut Fleischmann\n",
       " 32   5447.513389   2137  Heidelberg   AfD           Wilhelm Wabel\n",
       " 33   5552.874873   1698  Heidelberg   AfD           Helga Striehl\n",
       " 34   6435.381359   6856  Heidelberg   AfD        Timethy Bartesch\n",
       " 35   5529.584180   2116  Heidelberg   AfD              Bernd Wolf\n",
       " 36   7417.741692   5295  Heidelberg   AfD            Martin Jacob\n",
       " 37  16353.822366   9375  Heidelberg   AfD         Matthias Niebel\n",
       " 38   5714.532408   5316  Heidelberg   AfD           Manfred Hanke\n",
       " 39  15991.951510   2065  Heidelberg   AfD     Dieter-Eugen Glaser\n",
       " 40   5571.563749   1947  Heidelberg   AfD          Claudia Gregor\n",
       " 41  15583.777284   2473  Heidelberg   AfD    Dieter Bowe-Karamann\n",
       " 42   6138.562729   2979  Heidelberg   AfD         Stefan Holzmann\n",
       " 43   6013.421489   4708  Heidelberg   AfD            Klaus Blanck\n",
       " 44   4955.717469   1692  Heidelberg   AfD     Kai Gunther Lehmann\n",
       " 45   5947.919059   2735  Heidelberg   AfD             Jens Riedel\n",
       " 46   9188.729656   6986  Heidelberg   AfD         Sven Geschinski\n",
       " 47   2857.571306   1921  Heidelberg   AfD          Arthur Leibham\n",
       " 48   7534.099695   4691  Heidelberg   AfD      Birgit Fleischmann\n",
       " 49   7785.627874   5033  Heidelberg   AfD            Volker Kunze)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svr = LinearSVR(C = 4150)\n",
    "\n",
    "svr.fit(X_train_scaled, y_train)\n",
    "svr_pred = svr.predict(X_test_scaled)\n",
    "\n",
    "svr_pred_1 = pd.DataFrame({'Prediction': \n",
    "svr_pred, 'Votes': dataframe_test['votes'], 'city': dataframe_test['city'], 'party': dataframe_test['party'], 'name': dataframe_test['Name_total']})\n",
    "\n",
    "svr.score(X_train_scaled, y_train), svr_pred_1.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5983915741164743"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svr.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weights': (1, 2, 2, 0)}"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Hypertuning for Ensemble regression\"\"\"\n",
    "\n",
    "ensemble_regression = VotingRegressor([('lasso', res_lasso_cv), ('random_forest', rf_1), ('gradient_boosting', grad_boost), ('SVR', svr)])\n",
    "\n",
    "param_ensemble = {'weights': [(1, 1, 2, 1), (1, 2, 1, 1), (2, 1, 1, 1), (1, 1, 1, 2), (2, 2, 1, 1), (2, 1, 2, 1), \n",
    "                             (2, 1, 1, 2), (1, 2, 2, 1), (1, 2, 1, 2), (1, 1, 2, 2), (2, 2, 2, 1), (2, 2, 1, 2), (1, 2, 2, 2),\n",
    "                             (1, 1, 1, 1), (1, 1, 1, 0), (1, 1, 0, 1), (1, 0, 1, 1), (0, 1, 1, 1), (1, 1, 0, 0), (1, 0, 1, 0),\n",
    "                             (1, 0, 0, 1), (0, 1, 1, 0), (0, 1, 0, 1), (0, 0, 1, 1), (2, 2, 1, 0), (2, 2, 0, 1), (2, 1, 0, 2),\n",
    "                             (2, 0, 1, 2), (1, 0, 2, 2), (0, 1, 2, 2), (0, 2, 1, 2), (1, 2, 0, 2), (2, 1, 2, 0), (2, 0, 2, 1),\n",
    "                             (1, 2, 2, 0), (0, 2, 2, 1), (2, 1, 1, 0), (2, 1, 0, 1), (2, 1, 0, 0), (2, 0, 1, 1), (2, 0, 1, 0),\n",
    "                             (2, 0, 0, 1), (1, 2, 1, 0), (1, 2, 0, 1), (0, 2, 1, 0), (1, 2, 0, 0), (0, 2, 1, 1), (0, 0, 2, 1),\n",
    "                             (0, 1, 2, 1), (0, 1, 2, 1), (1, 0, 2, 1), (1, 1, 2, 0), (1, 0, 2, 0), (0, 1, 2, 0), (0, 0, 1, 2),\n",
    "                             (0, 1, 0, 2), (1, 0, 0, 2), (1, 1, 0, 2), (1, 0, 1, 2), (0, 1, 1, 2)]}\n",
    "#ensemble_cv = GridSearchCV(estimator = ensemble_regression, param_grid = param_ensemble , cv=5, n_jobs = 4, verbose = 2)\n",
    "\n",
    "#ensemble_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "ensemble_cv.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9286630599047928,\n",
       "       Prediction  Votes        city party                    name\n",
       " 0    6530.858045   8163    Freiburg   AfD          Bernhard Lukau\n",
       " 1    6816.346236   7711    Freiburg   AfD        Michael Theuring\n",
       " 2    6051.535332   6504    Freiburg   AfD            Elmar Ertmer\n",
       " 3    6310.016404   7703    Freiburg   AfD          Jens Bellemann\n",
       " 4    5123.192437   8149    Freiburg   AfD          Martin Polheim\n",
       " 5    4442.378606   4260    Freiburg   AfD         Robert Hagerman\n",
       " 6    3700.756215   4721    Freiburg   AfD           Michael Braun\n",
       " 7    5665.457585   6985    Freiburg   AfD             Bernd Fulde\n",
       " 8    8172.968692   8931    Freiburg   AfD            Karl Schwarz\n",
       " 9    5513.302510   6176    Freiburg   AfD             Jonas Möhle\n",
       " 10   4332.775801   5674    Freiburg   AfD   Alessandro Bertonasco\n",
       " 11   5766.065518   8008    Freiburg   AfD           Bernd Domnick\n",
       " 12   7686.251733   8263    Freiburg   AfD            Jack Gelfort\n",
       " 13   3968.045543   5585    Freiburg   AfD        Michael Pfeiffer\n",
       " 14   5578.894326   8607    Freiburg   AfD           Tilman Mehler\n",
       " 15   3797.641590   3979    Freiburg   AfD          Sandro Schüler\n",
       " 16  10155.035625   9278    Freiburg   AfD      Andreas Schumacher\n",
       " 17   3775.673340   4680    Freiburg   AfD           Thomas Müller\n",
       " 18   4581.670255   4934    Freiburg   AfD   Katharina Maria Lukau\n",
       " 19   5060.216551   7827    Freiburg   AfD     Heinz-Jürgen Schlag\n",
       " 20   9144.928365  12073    Freiburg   AfD            Detlef Huber\n",
       " 21   7683.225790   9948    Freiburg   AfD         Dubravko Mandic\n",
       " 22   4785.493941   4020    Freiburg   AfD  Marie-Luise Gatzweiler\n",
       " 23   5149.058767   5607    Freiburg   AfD              Marco Erat\n",
       " 24   3833.240911   2149  Heidelberg   AfD         Maximilian Wolf\n",
       " 25   3536.343798   2386  Heidelberg   AfD       Cornelia Degeratu\n",
       " 26   6530.236536   5280  Heidelberg   AfD       Jan Michael Kröhl\n",
       " 27   4300.904832   2249  Heidelberg   AfD           Florian Epple\n",
       " 28   3534.860354   2016  Heidelberg   AfD         Simone Kammerer\n",
       " 29   4120.873825   1977  Heidelberg   AfD        Ilona Schmittova\n",
       " 30   3134.562094   2319  Heidelberg   AfD              Liane Wolf\n",
       " 31   3039.652086   2235  Heidelberg   AfD      Helmut Fleischmann\n",
       " 32   4063.991762   2137  Heidelberg   AfD           Wilhelm Wabel\n",
       " 33   3036.147709   1698  Heidelberg   AfD           Helga Striehl\n",
       " 34   8444.662419   6856  Heidelberg   AfD        Timethy Bartesch\n",
       " 35   2957.271747   2116  Heidelberg   AfD              Bernd Wolf\n",
       " 36   5703.039371   5295  Heidelberg   AfD            Martin Jacob\n",
       " 37  14846.176914   9375  Heidelberg   AfD         Matthias Niebel\n",
       " 38   4948.259466   5316  Heidelberg   AfD           Manfred Hanke\n",
       " 39   2702.069440   2065  Heidelberg   AfD     Dieter-Eugen Glaser\n",
       " 40   3001.114529   1947  Heidelberg   AfD          Claudia Gregor\n",
       " 41   3334.341698   2473  Heidelberg   AfD    Dieter Bowe-Karamann\n",
       " 42   4646.910859   2979  Heidelberg   AfD         Stefan Holzmann\n",
       " 43   5399.393519   4708  Heidelberg   AfD            Klaus Blanck\n",
       " 44   3174.304872   1692  Heidelberg   AfD     Kai Gunther Lehmann\n",
       " 45   4341.290064   2735  Heidelberg   AfD             Jens Riedel\n",
       " 46   9150.993474   6986  Heidelberg   AfD         Sven Geschinski\n",
       " 47   2827.173793   1921  Heidelberg   AfD          Arthur Leibham\n",
       " 48   6670.876399   4691  Heidelberg   AfD      Birgit Fleischmann\n",
       " 49   7026.390122   5033  Heidelberg   AfD            Volker Kunze)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Forming predictions\"\"\"\n",
    "\n",
    "final_ensemble_regression = VotingRegressor([('lasso', res_lasso_cv), ('random_forest', rf_1), ('gradient_boosting', grad_boost), ('SVR', svr)], weights = [1,2,2,0], n_jobs = 4)\n",
    "final_ensemble_regression.fit(X_train_scaled, y_train)\n",
    "ensemble_regression_pred = final_ensemble_regression.predict(X_test_scaled)\n",
    "\n",
    "ensemble_regression_1 = pd.DataFrame({'Prediction': \n",
    "ensemble_regression_pred, 'Votes': dataframe_test['votes'], 'city': dataframe_test['city'], 'party': dataframe_test['party'], 'name': dataframe_test['Name_total']})\n",
    "\n",
    "final_ensemble_regression.score(X_train_scaled, y_train), ensemble_regression_1.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weights': (0, 1, 1, 0)}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Reduced Ensemble regression\"\"\"\n",
    "\n",
    "ensemble_regression = VotingRegressor([('lasso', res_lasso_cv), ('random_forest', rf_1), ('gradient_boosting', grad_boost), ('SVR', svr)])\n",
    "red_param_ensemble = {'weights': [(1, 1, 1, 0), (1, 1, 0, 1), (1, 0, 1, 1), (0, 1, 1, 1), (1, 1, 0, 0), (1, 0, 1, 0),\n",
    "                             (1, 0, 0, 1), (0, 1, 1, 0), (0, 1, 0, 1), (0, 0, 1, 1)]}\n",
    "\n",
    "red_ensemble_cv = GridSearchCV(estimator = ensemble_regression, param_grid = red_param_ensemble , cv=5, n_jobs = 4, verbose = 2)\n",
    "\n",
    "red_ensemble_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "red_ensemble_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weights': (0, 1, 1, 0)}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_regression = VotingRegressor([('lasso', res_lasso_cv), ('random_forest', rf_1), ('gradient_boosting', grad_boost), ('SVR', svr)])\n",
    "red_param_ensemble = {'weights': [(0, 1, 1, 0), (2, 2, 2, 1)]}\n",
    "\n",
    "red_ensemble_cv_2 = GridSearchCV(estimator = ensemble_regression, param_grid = red_param_ensemble , cv=5, n_jobs = 4, verbose = 2)\n",
    "\n",
    "red_ensemble_cv_2.fit(X_train_scaled, y_train)\n",
    "\n",
    "red_ensemble_cv_2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9431958685239916,\n",
       "       Prediction  Votes        city party                    name\n",
       " 0    6534.314754   8163    Freiburg   AfD          Bernhard Lukau\n",
       " 1    6942.136410   7711    Freiburg   AfD        Michael Theuring\n",
       " 2    6004.694941   6504    Freiburg   AfD            Elmar Ertmer\n",
       " 3    6255.335921   7703    Freiburg   AfD          Jens Bellemann\n",
       " 4    5105.162494   8149    Freiburg   AfD          Martin Polheim\n",
       " 5    4304.600197   4260    Freiburg   AfD         Robert Hagerman\n",
       " 6    3371.145338   4721    Freiburg   AfD           Michael Braun\n",
       " 7    5614.606900   6985    Freiburg   AfD             Bernd Fulde\n",
       " 8    8713.135043   8931    Freiburg   AfD            Karl Schwarz\n",
       " 9    5488.256771   6176    Freiburg   AfD             Jonas Möhle\n",
       " 10   4150.257875   5674    Freiburg   AfD   Alessandro Bertonasco\n",
       " 11   5667.116998   8008    Freiburg   AfD           Bernd Domnick\n",
       " 12   8086.315549   8263    Freiburg   AfD            Jack Gelfort\n",
       " 13   3727.013724   5585    Freiburg   AfD        Michael Pfeiffer\n",
       " 14   5662.178045   8607    Freiburg   AfD           Tilman Mehler\n",
       " 15   3571.249496   3979    Freiburg   AfD          Sandro Schüler\n",
       " 16  11004.717455   9278    Freiburg   AfD      Andreas Schumacher\n",
       " 17   3410.774943   4680    Freiburg   AfD           Thomas Müller\n",
       " 18   4538.295149   4934    Freiburg   AfD   Katharina Maria Lukau\n",
       " 19   5041.640992   7827    Freiburg   AfD     Heinz-Jürgen Schlag\n",
       " 20   9836.418568  12073    Freiburg   AfD            Detlef Huber\n",
       " 21   8014.986708   9948    Freiburg   AfD         Dubravko Mandic\n",
       " 22   4749.965480   4020    Freiburg   AfD  Marie-Luise Gatzweiler\n",
       " 23   5173.673955   5607    Freiburg   AfD              Marco Erat\n",
       " 24   3384.342052   2149  Heidelberg   AfD         Maximilian Wolf\n",
       " 25   3208.988386   2386  Heidelberg   AfD       Cornelia Degeratu\n",
       " 26   6577.638360   5280  Heidelberg   AfD       Jan Michael Kröhl\n",
       " 27   4173.492573   2249  Heidelberg   AfD           Florian Epple\n",
       " 28   3249.557768   2016  Heidelberg   AfD         Simone Kammerer\n",
       " 29   3721.206942   1977  Heidelberg   AfD        Ilona Schmittova\n",
       " 30   3177.171115   2319  Heidelberg   AfD              Liane Wolf\n",
       " 31   3140.703283   2235  Heidelberg   AfD      Helmut Fleischmann\n",
       " 32   3673.004610   2137  Heidelberg   AfD           Wilhelm Wabel\n",
       " 33   3107.355827   1698  Heidelberg   AfD           Helga Striehl\n",
       " 34   8628.581270   6856  Heidelberg   AfD        Timethy Bartesch\n",
       " 35   2919.007580   2116  Heidelberg   AfD              Bernd Wolf\n",
       " 36   5500.165674   5295  Heidelberg   AfD            Martin Jacob\n",
       " 37  15632.841279   9375  Heidelberg   AfD         Matthias Niebel\n",
       " 38   4883.968724   5316  Heidelberg   AfD           Manfred Hanke\n",
       " 39   2662.686700   2065  Heidelberg   AfD     Dieter-Eugen Glaser\n",
       " 40   3083.336215   1947  Heidelberg   AfD          Claudia Gregor\n",
       " 41   2995.114664   2473  Heidelberg   AfD    Dieter Bowe-Karamann\n",
       " 42   4505.553877   2979  Heidelberg   AfD         Stefan Holzmann\n",
       " 43   5303.379315   4708  Heidelberg   AfD            Klaus Blanck\n",
       " 44   3134.634078   1692  Heidelberg   AfD     Kai Gunther Lehmann\n",
       " 45   4135.858542   2735  Heidelberg   AfD             Jens Riedel\n",
       " 46   9636.855334   6986  Heidelberg   AfD         Sven Geschinski\n",
       " 47   3005.161631   1921  Heidelberg   AfD          Arthur Leibham\n",
       " 48   6642.574972   4691  Heidelberg   AfD      Birgit Fleischmann\n",
       " 49   7028.154526   5033  Heidelberg   AfD            Volker Kunze)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_ensemble_regression_2 = VotingRegressor([('lasso', res_lasso_cv), ('random_forest', rf_1), ('gradient_boosting', grad_boost), ('SVR', svr)], weights = [0, 1, 1, 0], n_jobs = 4)\n",
    "final_ensemble_regression_2.fit(X_train_scaled, y_train)\n",
    "ensemble_regression_pred = final_ensemble_regression_2.predict(X_test_scaled)\n",
    "\n",
    "ensemble_regression_2 = pd.DataFrame({'Prediction': \n",
    "ensemble_regression_pred, 'Votes': dataframe_test['votes'], 'city': dataframe_test['city'], 'party': dataframe_test['party'], 'name': dataframe_test['Name_total']})\n",
    "\n",
    "final_ensemble_regression_2.score(X_train_scaled, y_train), ensemble_regression_2.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" General function that returns the quality of our prediction\"\"\"\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "def display_score(reg, mse_reg, r2):\n",
    "    \n",
    "    \"\"\" Insert explanation \"\"\"\n",
    "    \n",
    "    reg_rmse = np.sqrt(-mse_reg)\n",
    "    \n",
    "    print('MSE                                  ')\n",
    "    print('Scores:', reg_rmse, reg)\n",
    "    print('Mean:', reg_rmse.mean(), reg)\n",
    "    print('standard Deviation:', reg_rmse.std(), reg)\n",
    "    print('R2                                   ')\n",
    "    print('Scores:', r2, reg)\n",
    "    print('Mean:', r2.mean(), reg)\n",
    "    print('standard Deviation:', r2.std(), reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE                                  \n",
      "Scores: [ 3663.59861395 19628.61658841  2796.70483513  3069.11464393\n",
      "  6065.74324494  9050.80418847  2426.0731311   3519.56668207\n",
      "  6280.80525216  2661.48474369] Lasso\n",
      "Mean: 5916.251192385376 Lasso\n",
      "standard Deviation: 4994.404655134523 Lasso\n",
      "R2                                   \n",
      "Scores: [ 0.78371829 -0.75050078  0.57160317  0.49800648  0.42446468  0.51381774\n",
      "  0.42001559  0.61290077  0.57940586  0.52808943] Lasso\n",
      "Mean: 0.4181521230818716 Lasso\n",
      "standard Deviation: 0.4018113272630606 Lasso\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Cross-Validating the Score Measures of training set\"\"\"\n",
    "\n",
    "mse_lasso_train = cross_val_score(res_lasso_cv, X_train_scaled, y_train, scoring = 'neg_mean_squared_error', cv = 10, n_jobs = -1)\n",
    "r2_lasso_train = cross_val_score(res_lasso_cv, X_train_scaled, y_train, scoring = 'r2', cv = 10, n_jobs = -1)\n",
    "display_score(\"Lasso\", mse_lasso_train, r2_lasso_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE                                  \n",
      "Scores: [17356.52344979  7023.00935682  4344.72518826  8048.03579191\n",
      "  7627.2860799   5382.49221688  3024.46174504  3219.12195261\n",
      "  5946.92825027  3050.95263777] Lasso\n",
      "Mean: 6502.353666925841 Lasso\n",
      "standard Deviation: 4034.1560415395393 Lasso\n",
      "R2                                   \n",
      "Scores: [-7.2927101   0.82113607  0.68507558  0.04748726  0.85618446  0.8883483\n",
      "  0.68901382  0.65877043  0.73271644  0.61226766] Lasso\n",
      "Mean: -0.13017100977113635 Lasso\n",
      "standard Deviation: 2.397972098130186 Lasso\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Cross-Validating the Score measure of test set\"\"\"\n",
    "\n",
    "mse_lasso_test = cross_val_score(res_lasso_cv, X_test_scaled, y_test, scoring = 'neg_mean_squared_error', cv = 10, n_jobs = -1)\n",
    "r2_lasso_test = cross_val_score(res_lasso_cv, X_test_scaled, y_test, scoring = 'r2', cv = 10, n_jobs = -1)\n",
    "display_score(\"Lasso\", mse_lasso_test, r2_lasso_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE                                  \n",
      "Scores: [53564.6182203  17510.12809621  8185.84358988  7545.87209823\n",
      " 46053.89009838  8155.29939474  2726.54992551  3486.69907599\n",
      "  6114.33225281 13170.63649877] SVR\n",
      "Mean: 16651.386925081795 SVR\n",
      "standard Deviation: 17160.844494895413 SVR\n",
      "R2                                   \n",
      "Scores: [-46.41601474  -0.3046565   -3.24490149  -2.29155243 -32.96861307\n",
      "   0.62768556   0.19911129   0.69980231   0.63717313  -9.67405572] SVR\n",
      "Mean: -9.273602166719616 SVR\n",
      "standard Deviation: 15.786605132023318 SVR\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Support Vector Regression Training Score \"\"\"\n",
    "\n",
    "mse_svr_train = cross_val_score(svr, X_train_scaled, y_train, scoring = 'neg_mean_squared_error', cv = 10, n_jobs = -1)\n",
    "r2_svr_train = cross_val_score(svr, X_train_scaled, y_train, scoring = 'r2', cv = 10, n_jobs = -1)\n",
    "display_score(\"SVR\", mse_svr_train, r2_svr_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE                                  \n",
      "Scores: [60005.66644762  6594.34122182 20739.49075087 13087.09571946\n",
      " 13688.54068833 14569.22566644  4352.71022392  7569.80308237\n",
      "  6631.12045485  2995.07909131] SVR\n",
      "Mean: 15023.307334698267 SVR\n",
      "standard Deviation: 15866.300782367385 SVR\n",
      "R2                                   \n",
      "Scores: [-96.07604979   0.8237574   -6.00358986  -1.55052681   0.53048069\n",
      "   0.21941247   0.40922257  -1.0677633    0.67264646   0.57088177] SVR\n",
      "Mean: -10.147152838249866 SVR\n",
      "standard Deviation: 28.709757102237056 SVR\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Support Vector Regression Test Score \"\"\"\n",
    "\n",
    "mse_svr_test = cross_val_score(svr, X_test_scaled, y_test, scoring = 'neg_mean_squared_error', cv = 10, n_jobs = -1)\n",
    "r2_svr_test = cross_val_score(svr, X_test_scaled, y_test, scoring = 'r2', cv = 10, n_jobs = -1)\n",
    "display_score(\"SVR\", mse_svr_test, r2_svr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE                                  \n",
      "Scores: [ 3370.15618287  8261.69326749  4047.38385773  2665.66380868\n",
      "  5771.10254905  9584.60388166  1912.01015909  2721.57484861\n",
      " 10165.6246253   2609.23569527] RF\n",
      "Mean: 5110.9048875737235 RF\n",
      "standard Deviation: 2970.9958586040125 RF\n",
      "R2                                   \n",
      "Scores: [ 0.80936854  0.6731196   0.10496928  0.61563711  0.47599266  0.44001036\n",
      "  0.60309173  0.76429308 -0.22533435  0.55695775] RF\n",
      "Mean: 0.4818105776172928 RF\n",
      "standard Deviation: 0.30096509751675077 RF\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Checking out the Random Forest. As usual, train data first\"\"\"\n",
    "\n",
    "mse_rf_train = cross_val_score(rf_1, rand_for_train_scaled, y_train, scoring = 'neg_mean_squared_error', cv = 10, n_jobs = -1)\n",
    "r2_rf_train = cross_val_score(rf_1, rand_for_train_scaled, y_train, scoring = 'r2', cv = 10, n_jobs = -1)\n",
    "display_score(\"RF\", mse_rf_train, r2_rf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE                                  \n",
      "Scores: [10213.16517969 10378.0727881   2984.09451997  7710.38086063\n",
      " 12200.21689223  6239.73409206  2668.40500057  4488.51958774\n",
      "  5264.20880652  3259.37226337] RF\n",
      "Mean: 6540.61699908741 RF\n",
      "standard Deviation: 3255.0482733559948 RF\n",
      "R2                                   \n",
      "Scores: [-1.29165285  0.59603696  0.83696084  0.13797658  0.61928616  0.83921672\n",
      "  0.75989696  0.35707118  0.78735407  0.55820468] RF\n",
      "Mean: 0.42003512961178063 RF\n",
      "standard Deviation: 0.6085339091829746 RF\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Here, we consider the test data for the Random Forest\"\"\"\n",
    "\n",
    "mse_rf_test = cross_val_score(rf_1, rand_for_test_scaled, y_test, scoring = 'neg_mean_squared_error', cv = 10, n_jobs = -1)\n",
    "r2_rf_test = cross_val_score(rf_1, rand_for_test_scaled, y_test, scoring = 'r2', cv = 10, n_jobs = -1)\n",
    "display_score(\"RF\", mse_rf_test, r2_rf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE                                  \n",
      "Scores: [2835.99273453 8797.56061668 2598.19260953 2476.22606351 5668.4506034\n",
      " 8726.43573711 1922.99695068 2258.14473168 4991.98868083 2299.0724652 ] Gradient Boosting\n",
      "Mean: 4257.506119315109 Gradient Boosting\n",
      "standard Deviation: 2536.6016123291806 Gradient Boosting\n",
      "R2                                   \n",
      "Scores: [0.87039724 0.64835256 0.63026065 0.67322226 0.49738822 0.5480415\n",
      " 0.6356106  0.84065169 0.73430722 0.64785862] Gradient Boosting\n",
      "Mean: 0.6726090560486385 Gradient Boosting\n",
      "standard Deviation: 0.11033956076050566 Gradient Boosting\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Gradient Boosting\"\"\"\n",
    "\n",
    "mse_gb_train = cross_val_score(grad_boost, X_train_scaled, y_train, scoring = 'neg_mean_squared_error', cv = 10, n_jobs = -1)\n",
    "r2_gb_train = cross_val_score(grad_boost, X_train_scaled, y_train, scoring = 'r2', cv = 10, n_jobs = -1)\n",
    "display_score(\"Gradient Boosting\", mse_gb_train, r2_gb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE                                  \n",
      "Scores: [6598.68648349 7861.73255229 3910.43733371 7051.05520693 7402.35112187\n",
      " 5924.54157562 3201.2327585  3162.62998626 6450.85037935 3047.80846545] Gradient Boosting\n",
      "Mean: 5461.132586346712 Gradient Boosting\n",
      "standard Deviation: 1821.1653445575541 Gradient Boosting\n",
      "R2                                   \n",
      "Scores: [-0.19863235  0.77586338  0.74488712  0.26886222  0.86454186  0.86472797\n",
      "  0.65159899  0.67064173  0.6854999   0.6130664 ] Gradient Boosting\n",
      "Mean: 0.5941057227397042 Gradient Boosting\n",
      "standard Deviation: 0.30890704222558873 Gradient Boosting\n"
     ]
    }
   ],
   "source": [
    "\"\"\" GB: test data\"\"\"\n",
    "\n",
    "mse_gb_test = cross_val_score(grad_boost, X_test_scaled, y_test, scoring = 'neg_mean_squared_error', cv = 10, n_jobs = -1)\n",
    "r2_gb_test = cross_val_score(grad_boost, X_test_scaled, y_test, scoring = 'r2', cv = 10, n_jobs = -1)\n",
    "display_score(\"Gradient Boosting\", mse_gb_test, r2_gb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE                                  \n",
      "Scores: [2645.00902907 9049.95474667 2558.1216969  2562.574041   5669.40501072\n",
      " 5954.85363946 2010.87487667 2282.14642394 5502.74865191 2310.06798041] Ensemble Regression\n",
      "Mean: 4054.575609673228 Ensemble Regression\n",
      "standard Deviation: 2237.9364195585285 Ensemble Regression\n",
      "R2                                   \n",
      "Scores: [0.88766635 0.62523138 0.63210663 0.64909959 0.49756902 0.78320267\n",
      " 0.61400487 0.83797359 0.67705152 0.64567969] Ensemble Regression\n",
      "Mean: 0.684958532154825 Ensemble Regression\n",
      "standard Deviation: 0.11115623945338213 Ensemble Regression\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Ensemble Regression with Training Data\"\"\"\n",
    "\n",
    "mse_er_train = cross_val_score(final_ensemble_regression, X_train_scaled, y_train, scoring = 'neg_mean_squared_error', cv = 10, n_jobs = -1)\n",
    "r2_er_train = cross_val_score(final_ensemble_regression, X_train_scaled, y_train, scoring = 'r2', cv = 10, n_jobs = -1)\n",
    "display_score(\"Ensemble Regression\", mse_er_train, r2_er_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE                                  \n",
      "Scores: [11773.43128437  8520.31233556  4311.41790932  7365.93560315\n",
      "  8013.33141934  4833.62878778  3438.85984179  3085.17240197\n",
      "  5871.76420383  3151.9337471 ] Ensemble Regression\n",
      "Mean: 6036.578753420143 Ensemble Regression\n",
      "standard Deviation: 2701.5866154234104 Ensemble Regression\n",
      "R2                                   \n",
      "Scores: [-2.81599654  0.73527128  0.70195935  0.2046823   0.83175279  0.90730482\n",
      "  0.59525322  0.68032597  0.74234957  0.59570276] Ensemble Regression\n",
      "Mean: 0.317860553195191 Ensemble Regression\n",
      "standard Deviation: 1.0598527275862153 Ensemble Regression\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Ensemble Regression with Test Data\"\"\"\n",
    "\n",
    "mse_er_test = cross_val_score(final_ensemble_regression, X_test_scaled, y_test, scoring = 'neg_mean_squared_error', cv = 10, n_jobs = -1)\n",
    "r2_er_test = cross_val_score(final_ensemble_regression, X_test_scaled, y_test, scoring = 'r2', cv = 10, n_jobs = -1)\n",
    "display_score(\"Ensemble Regression\", mse_er_test, r2_er_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE                                  \n",
      "Scores: [2602.80962658 9226.51180934 2846.45399533 2531.92918332 5689.22553706\n",
      " 5383.86034975 2089.52054822 2223.4765704  6275.46174942 2373.19565419] reduced Ensemble Regression\n",
      "Mean: 4124.244502361255 reduced Ensemble Regression\n",
      "standard Deviation: 2280.325374669567 reduced Ensemble Regression\n",
      "R2                                   \n",
      "Scores: [0.89068069 0.61576045 0.55987285 0.65649924 0.48972129 0.82861354\n",
      " 0.56627912 0.84047779 0.59432696 0.60861269] reduced Ensemble Regression\n",
      "Mean: 0.6650844626231208 reduced Ensemble Regression\n",
      "standard Deviation: 0.13064635766948673 reduced Ensemble Regression\n"
     ]
    }
   ],
   "source": [
    "mse_er2_train = cross_val_score(final_ensemble_regression_2, X_train_scaled, y_train, scoring = 'neg_mean_squared_error', cv = 10, n_jobs = -1)\n",
    "r2_er2_train = cross_val_score(final_ensemble_regression_2, X_train_scaled, y_train, scoring = 'r2', cv = 10, n_jobs = -1)\n",
    "display_score(\"reduced Ensemble Regression\", mse_er2_train, r2_er2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE                                  \n",
      "Scores: [10682.07571352  9693.71710647  4914.2259956   7247.84658337\n",
      "  8552.53374941  4990.51017039  3750.44909851  3283.60570279\n",
      "  6072.11149061  3226.7805082 ] Reduced Ensemble Regression\n",
      "Mean: 6241.385611888166 Reduced Ensemble Regression\n",
      "standard Deviation: 2557.116679170166 Reduced Ensemble Regression\n",
      "R2                                   \n",
      "Scores: [-2.22905035  0.66108981  0.61101546  0.228594    0.80901921  0.90592576\n",
      "  0.557606    0.63329608  0.72286228  0.56475755] Reduced Ensemble Regression\n",
      "Mean: 0.34651157893061696 Reduced Ensemble Regression\n",
      "standard Deviation: 0.8751731517625211 Reduced Ensemble Regression\n"
     ]
    }
   ],
   "source": [
    "mse_er2_test = cross_val_score(final_ensemble_regression_2, X_test_scaled, y_test, scoring = 'neg_mean_squared_error', cv = 10, n_jobs = -1)\n",
    "r2_er2_test = cross_val_score(final_ensemble_regression_2, X_test_scaled, y_test, scoring = 'r2', cv = 10, n_jobs = -1)\n",
    "display_score(\"Reduced Ensemble Regression\", mse_er2_test, r2_er2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE                                  \n",
      "Scores: [184682.56322492  15399.38175936   4138.69680795   6666.16576312\n",
      "  30327.23402986   9541.28372463   3190.64633222   4376.62534453\n",
      "   7003.48086228   4605.92100565] Linear Regression\n",
      "Mean: 26993.199885452275 Linear Regression\n",
      "standard Deviation: 53132.29134859821 Linear Regression\n",
      "R2                                   \n",
      "Scores: [-5.48611495e+02 -7.74310736e-02  6.18328846e-02 -1.36823124e+00\n",
      " -1.33869766e+01  4.59695741e-01 -3.15000431e-03  4.01419383e-01\n",
      "  4.77049651e-01 -4.13333697e-01] Linear Regression\n",
      "Mean: -56.24606195961583 Linear Regression\n",
      "standard Deviation: 164.17071635437767 Linear Regression\n"
     ]
    }
   ],
   "source": [
    "\"\"\" To complete things, the linear regression with the reduced coefficients \"\"\"\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "lin_reg.fit(rand_for_train_scaled, y_train)\n",
    "lin_pred = lin_reg.predict(rand_for_test_scaled)\n",
    "\n",
    "mse_linreg_train = cross_val_score(lin_reg, rand_for_train_scaled, y_train, scoring = 'neg_mean_squared_error', cv = 10, n_jobs = -1)\n",
    "r2_linreg_train = cross_val_score(lin_reg, rand_for_train_scaled, y_train, scoring = 'r2', cv = 10, n_jobs = -1)\n",
    "display_score(\"Linear Regression\", mse_linreg_train, r2_linreg_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:                  votes   R-squared (uncentered):                   0.354\n",
      "Model:                            OLS   Adj. R-squared (uncentered):              0.352\n",
      "Method:                 Least Squares   F-statistic:                              137.8\n",
      "Date:                Thu, 18 Jul 2019   Prob (F-statistic):                        0.00\n",
      "Time:                        12:56:36   Log-Likelihood:                         -79439.\n",
      "No. Observations:                7562   AIC:                                  1.589e+05\n",
      "Df Residuals:                    7532   BIC:                                  1.591e+05\n",
      "Df Model:                          30                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1         -1413.6102    110.543    -12.788      0.000   -1630.304   -1196.916\n",
      "x2          2137.9231    112.196     19.055      0.000    1917.988    2357.858\n",
      "x3           -35.4046    104.466     -0.339      0.735    -240.187     169.377\n",
      "x4           130.9154    104.688      1.251      0.211     -74.302     336.133\n",
      "x5           -45.3765    141.803     -0.320      0.749    -323.350     232.597\n",
      "x6          3394.4999   2164.570      1.568      0.117    -848.660    7637.660\n",
      "x7           -26.6909    102.086     -0.261      0.794    -226.807     173.425\n",
      "x8          -166.0518    102.289     -1.623      0.105    -366.567      34.464\n",
      "x9          5470.0670    236.390     23.140      0.000    5006.676    5933.458\n",
      "x10         3292.5333    279.211     11.792      0.000    2745.201    3839.866\n",
      "x11         1538.0581    170.016      9.047      0.000    1204.780    1871.336\n",
      "x12         1481.1338    283.908      5.217      0.000     924.594    2037.673\n",
      "x13          420.2750   1693.019      0.248      0.804   -2898.514    3739.064\n",
      "x14          702.3560    907.639      0.774      0.439   -1076.869    2481.581\n",
      "x15          -20.2611    300.011     -0.068      0.946    -608.365     567.843\n",
      "x16          272.9074    486.780      0.561      0.575    -681.318    1227.133\n",
      "x17         2104.6684    316.024      6.660      0.000    1485.174    2724.163\n",
      "x18          278.4974    266.952      1.043      0.297    -244.803     801.798\n",
      "x19          572.9309    294.515      1.945      0.052      -4.400    1150.262\n",
      "x20         -721.0593    187.491     -3.846      0.000   -1088.593    -353.525\n",
      "x21          104.7493    208.220      0.503      0.615    -303.420     512.919\n",
      "x22          883.6444    583.890      1.513      0.130    -260.942    2028.231\n",
      "x23          708.7408    894.711      0.792      0.428   -1045.142    2462.623\n",
      "x24         -806.1958    854.551     -0.943      0.345   -2481.355     868.963\n",
      "x25          -39.3865    106.162     -0.371      0.711    -247.493     168.720\n",
      "x26           57.0178    107.188      0.532      0.595    -153.101     267.136\n",
      "x27          -70.5376    103.156     -0.684      0.494    -272.753     131.678\n",
      "x28          -99.6660    102.306     -0.974      0.330    -300.214     100.881\n",
      "x29          -20.9057    104.090     -0.201      0.841    -224.951     183.140\n",
      "x30         -843.8958    138.677     -6.085      0.000   -1115.742    -572.050\n",
      "==============================================================================\n",
      "Omnibus:                     5391.016   Durbin-Watson:                   0.302\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           196319.656\n",
      "Skew:                           2.980   Prob(JB):                         0.00\n",
      "Kurtosis:                      27.240   Cond. No.                         59.4\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as smf\n",
    "results = smf.OLS(y_train, rand_for_train_scaled).fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE                                  \n",
      "Scores: [3.68000176e+14 7.92680148e+09 1.80502367e+14 1.95424353e+16\n",
      " 3.65120498e+16 2.61029003e+14 3.65448952e+03 5.16701358e+03\n",
      " 7.86512468e+03 6.26541240e+03] Linear Regression\n",
      "Mean: 5686402456527141.0 Linear Regression\n",
      "standard Deviation: 1.179797003958671e+16 Linear Regression\n",
      "R2                                   \n",
      "Scores: [-3.72792405e+21 -2.27862177e+11 -5.43560040e+20 -5.61627496e+24\n",
      " -3.29562792e+24 -2.62588703e+20  5.45955575e-01  1.20874239e-01\n",
      "  5.32482282e-01 -6.35160635e-01] Linear Regression\n",
      "Mean: -8.916436951175416e+23 Linear Regression\n",
      "standard Deviation: 1.85616371198575e+24 Linear Regression\n"
     ]
    }
   ],
   "source": [
    "mse_linreg_test = cross_val_score(lin_reg, rand_for_test_scaled, y_test, scoring = 'neg_mean_squared_error', cv = 10, n_jobs = -1)\n",
    "r2_linreg_test = cross_val_score(lin_reg, rand_for_test_scaled, y_test, scoring = 'r2', cv = 10, n_jobs = -1)\n",
    "display_score(\"Linear Regression\", mse_linreg_test, r2_linreg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Merge all predictions in one dataset \"\"\"\n",
    "\n",
    "all_pred = pd.DataFrame({'Ensemble Prediction': ensemble_regression_pred, 'Support Vector Prediction': svr_pred,\n",
    "                         'Gradient Boosting Prediction': gradient_boosting_pred, 'Random Forest Prediction': random_forest_pred,\n",
    "                         'Lasso Prediction': lasso_comp_pred, 'Linear Regression Prediction': lin_pred,\n",
    "                         'Votes': dataframe_test['votes'], 'city': dataframe_test['city'], 'party': dataframe_test['party'], 'name': dataframe_test['Name_total']})\n",
    "\n",
    "all_pred.to_excel(r'C:\\Users\\mariu\\Desktop\\Project\\all_predictions.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" All Features/Coefficients in one dataset \"\"\"\n",
    "\n",
    "coefficients_lasso = pd.DataFrame({'Variable': X2, 'Coefficients Lasso': res_lasso_cv.coef_, \n",
    "                                   'Coefficients Gradient Boosting': grad_boost.feature_importances_,\n",
    "                                   'Coefficients SVM': svr.coef_ ,\n",
    "                                  }) # Create a dataset with the estimated coefficients\n",
    "\n",
    "coefficients_lasso.to_excel(r'C:\\Users\\mariu\\Desktop\\Project\\Coefficients_Estimators.xlsx')\n",
    "\n",
    "list_coefficients = pd.DataFrame({'Variable': X2_rand_for, 'Coefficients RF': feature_importances_coef_comp,\n",
    "                                 'Coefficients LR': lin_reg.coef_,\n",
    "                                 }) # Create a dataset with the estimated coefficients - only the core variables (used for RF and LR)\n",
    "\n",
    "list_coefficients.to_excel(r'C:\\Users\\mariu\\Desktop\\Project\\Coefficients_Random_Forest_Lin_Reg.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE                                  \n",
      "Scores: [1.53323263 0.69730152 2.51026109 2.76513724 1.23844075 1.70881249\n",
      " 0.80096218 0.21982417 1.34298295 0.67475326] Ensemble\n",
      "Mean: 1.3491708281454362 Ensemble\n",
      "standard Deviation: 0.7735852252252899 Ensemble\n",
      "R2                                   \n",
      "Scores: [0.82689547 0.82876268 0.69326066 0.59521262 0.76822281 0.83390221\n",
      " 0.97929471 0.98982681 0.79819824 0.75513709] Ensemble\n",
      "Mean: 0.8068713298483136 Ensemble\n",
      "standard Deviation: 0.11259021000512717 Ensemble\n",
      "MSE                                  \n",
      "Scores: [1.37076549 1.07625199 2.53274236 2.05158662 1.22519696 2.29570449\n",
      " 2.20362547 1.14834725 1.50147623 0.60329823] Gradient Boosting\n",
      "Mean: 1.6008995095973588 Gradient Boosting\n",
      "standard Deviation: 0.5997289762539889 Gradient Boosting\n",
      "R2                                   \n",
      "Scores: [0.86163742 0.59207049 0.68774189 0.77717018 0.77315352 0.700217\n",
      " 0.84327697 0.72237865 0.74775599 0.8042521 ] Gradient Boosting\n",
      "Mean: 0.7509654209505499 Gradient Boosting\n",
      "standard Deviation: 0.07578647742617083 Gradient Boosting\n",
      "MSE                                  \n",
      "Scores: [2.65079338 1.03720317 3.41369432 2.82732262 0.94440595 2.68733035\n",
      " 2.99946594 1.64763078 1.90478951 1.49731566] Random Forest\n",
      "Mean: 2.1609951682396797 Random Forest\n",
      "standard Deviation: 0.8213321073113173 Random Forest\n",
      "R2                                   \n",
      "Scores: [ 0.48257896  0.62113468  0.43274157  0.5768013   0.86521621  0.58921257\n",
      "  0.70963442  0.42848691  0.59404497 -0.20575687] Random Forest\n",
      "Mean: 0.5094094722841406 Random Forest\n",
      "standard Deviation: 0.2686733244029276 Random Forest\n",
      "MSE                                  \n",
      "Scores: [1.71442163 0.53797541 2.64584546 1.8405146  1.12825889 2.56714803\n",
      " 2.3942396  0.86710208 1.80129008 1.21266903] Lasso\n",
      "Mean: 1.670946479976417 Lasso\n",
      "standard Deviation: 0.689833920201173 Lasso\n",
      "R2                                   \n",
      "Scores: [0.78356494 0.89807469 0.65923056 0.82066208 0.80762982 0.62513331\n",
      " 0.81499116 0.84171242 0.63696269 0.20910726] Lasso\n",
      "Mean: 0.7097068935718054 Lasso\n",
      "standard Deviation: 0.18892852789264114 Lasso\n",
      "MSE                                  \n",
      "Scores: [1.87071706 0.88489081 1.84462487 1.95735033 0.52316804 1.62638283\n",
      " 3.69801703 1.47481531 3.99418852 1.86800648] Linear Regression\n",
      "Mean: 1.9742161291419742 Linear Regression\n",
      "standard Deviation: 1.0369864164500937 Linear Regression\n",
      "R2                                   \n",
      "Scores: [ 0.74230348  0.72423665  0.83436664  0.79717069  0.95863789  0.84954016\n",
      "  0.55863786  0.54208838 -0.78501168 -0.87667803] Linear Regression\n",
      "Mean: 0.43452920369892667 Linear Regression\n",
      "standard Deviation: 0.6442417687942458 Linear Regression\n",
      "MSE                                  \n",
      "Scores: [2.89275314 1.37272759 3.5173774  2.93952766 1.54816172 2.4231093\n",
      " 3.22330113 1.43750216 0.9247721  1.76924244] Support Vector Machine\n",
      "Mean: 2.204847463084456 Support Vector Machine\n",
      "standard Deviation: 0.8579867885906127 Support Vector Machine\n",
      "R2                                   \n",
      "Scores: [ 0.38380938  0.33637016  0.39775995  0.54254467  0.6377963   0.66601956\n",
      "  0.66468034  0.5649658   0.9043129  -0.68347903] Support Vector Machine\n",
      "Mean: 0.4414780034101115 Support Vector Machine\n",
      "standard Deviation: 0.40746433359489886 Support Vector Machine\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Evaluate on party seats for each estimator \"\"\"\n",
    "\n",
    "import statsmodels.tools.tools as sm\n",
    "\n",
    "dataframe_seats = pd.read_excel(r'C:\\Users\\mariu\\Desktop\\Project\\Evaluation_seats.xlsx')\n",
    "\n",
    "estimators = ['Ensemble', 'Gradient Boosting', 'Random Forest', 'Lasso', 'Linear Regression', 'Support Vector Machine']\n",
    "\n",
    "score = []\n",
    "\n",
    "for i in range(len(estimators)):\n",
    "    \n",
    "    estimator = estimators[i]\n",
    "    data_estimator = dataframe_seats[dataframe_seats['Estimator'].str.match(estimator)]\n",
    "    \n",
    "    X_seats = data_estimator['Predicted Seats']\n",
    "    y_seats = data_estimator['True Seats']\n",
    "    X = sm.add_constant(X_seats)\n",
    "    \n",
    "    mse_eval_seats_estimator = cross_val_score(lin_reg, X, y_seats, scoring = 'neg_mean_squared_error', cv = 10, n_jobs = -1)\n",
    "    r2_eval_seats_estimator= cross_val_score(lin_reg, X, y_seats, scoring = 'r2', cv = 10, n_jobs = -1)\n",
    "    display_score(estimator, mse_eval_seats_estimator, r2_eval_seats_estimator)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly.tools as tls\n",
    "\n",
    "tools.set_credentials_file(username='marius92', api_key='4naxu3XiGZTGniWO02z1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\IPython\\core\\display.py:694: UserWarning:\n",
      "\n",
      "Consider using IPython.display.IFrame instead\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Make a nice graph \"\"\"\n",
    "\n",
    "dataframe_seats = pd.read_excel(r'C:\\Users\\mariu\\Desktop\\Project\\Evaluation_seats.xlsx')\n",
    "\n",
    "estimators = ['Ensemble', 'Gradient Boosting', 'Random Forest', 'Lasso', 'Linear Regression', 'Support Vector Machine']\n",
    "\n",
    "colors = ['#0D76BF', '#43B02A', '#F93822', '#3E332E', '#FFD700', '#D62598', '#8D3921', '#F06400', '#c1c0c0', '#c1c0c0', '#c1c0c0',\n",
    "         '#c1c0c0', '#c1c0c0', '#c1c0c0', '#c1c0c0', '#c1c0c0', '#c1c0c0',\n",
    "         '#c1c0c0', '#c1c0c0', '#c1c0c0', '#c1c0c0', '#c1c0c0', '#c1c0c0', '#c1c0c0', '#c1c0c0', '#c1c0c0',\n",
    "         '#c1c0c0', '#c1c0c0', '#c1c0c0', '#c1c0c0', '#c1c0c0', '#c1c0c0', '#c1c0c0',\n",
    "         '#c1c0c0', '#c1c0c0', '#c1c0c0', '#c1c0c0', '#c1c0c0', '#c1c0c0', '#c1c0c0', '#c1c0c0', '#c1c0c0', '#c1c0c0']\n",
    "\n",
    "parties = ['AfD', 'Grüne', 'SPD', 'CDU', 'FDP', 'Linke', 'Partei', 'FW', 'Piraten', 'UFF', 'Junges Freiburg',\n",
    "          'Bürger für Freiburg', 'Urbanes Freiburg', ' FL', 'Für Freiburg', 'GA Freiburg', 'Liste Teilhabe', 'nicht',\n",
    "          'Urbanes Freiburg', 'GA Heidelberg', 'Bunte Linke', 'heidelberger', 'HiB', 'Für KA', 'KAL', 'BIG', 'Mannheimer Volkspartei',\n",
    "          'Mittelstand für Mannheim', 'Tierschutzpartei', 'BZS 23', 'DIB', 'Fem. Liste', 'Junges Liste', 'Kein Fahrverbot',\n",
    "          'öpd', 'sös', 'SchUB', 'Stadtisten', 'BLO', 'UfA', 'UVL', 'WWG', 'UWS']\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(estimators)):\n",
    "    \n",
    "    estimator = estimators[i]\n",
    "    \n",
    "    data_estimator = dataframe_seats[dataframe_seats['Estimator'].str.match(estimator)]\n",
    "        \n",
    "    X_seats = data_estimator[['Predicted Seats', 'Party']]\n",
    "    y_seats = data_estimator[['True Seats', 'Party']]\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for i in range(len(parties)):\n",
    "        \n",
    "        party_now = parties[i]\n",
    "        color_now = colors[i]\n",
    "        X_data = X_seats[X_seats['Party'].str.match(party_now)]\n",
    "        y_data = y_seats[y_seats['Party'].str.match(party_now)]\n",
    "        \n",
    "        trace = go.Scatter(\n",
    "            x = X_data['Predicted Seats'],\n",
    "            y = y_data['True Seats'],\n",
    "            name = party_now,\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                color=color_now,\n",
    "                size=12,\n",
    "                line=dict(\n",
    "                    color='rgba(217, 217, 217, 0.14)',\n",
    "                    width=0.5),\n",
    "                opacity=0.8)\n",
    "        )\n",
    "        data.append(trace)\n",
    "    \n",
    "    trace_1 = go.Scatter(\n",
    "        x = [0, 5, 10, 16],\n",
    "        y = [0, 5, 10, 16],\n",
    "        name = 'Fit',\n",
    "        mode = 'lines',\n",
    "        showlegend=False,\n",
    "        marker = dict(\n",
    "            color = 'black')\n",
    "    )\n",
    "\n",
    "    data.append(trace_1)\n",
    "\n",
    "    filename = estimator\n",
    "    \n",
    "    layout = go.Layout(\n",
    "        legend=dict(orientation=\"h\")\n",
    "    )\n",
    "    fig = dict(data = data, layout = layout)\n",
    "    fig['layout'].update(width = 600, height = 600)\n",
    "    fig['layout'].update(title = filename, showlegend = False)\n",
    "    fig['layout'].update(xaxis = dict(\n",
    "                    title = 'Predicted Seats'))\n",
    "    fig['layout'].update(yaxis = dict(\n",
    "                    title = 'Actual Seats',\n",
    "                    tickmode = 'auto'))\n",
    "\n",
    "    py.iplot(fig, filename = filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~marius92/52.embed\" height=\"600px\" width=\"600px\"></iframe>"
      ],
      "text/plain": [
       "<chart_studio.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_line = [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "data_line = []\n",
    "\n",
    "trace_1 = go.Scatter(\n",
    "    x = X_line,\n",
    "    y = r2_lasso_test,\n",
    "    name = 'R2 Lasso',\n",
    "    mode = 'lines',\n",
    "    marker = dict(\n",
    "        color = 'purple')\n",
    ")\n",
    "\n",
    "data_line.append(trace_1)\n",
    "\n",
    "trace_2 = go.Scatter(\n",
    "    x = X_line,\n",
    "    y = r2_svr_test,\n",
    "    name = 'R2 SVM',\n",
    "    mode = 'lines',\n",
    "    marker = dict(\n",
    "        color = 'black')\n",
    ")\n",
    "\n",
    "#data_line.append(trace_2)\n",
    "\n",
    "trace_3 = go.Scatter(\n",
    "    x = X_line,\n",
    "    y = r2_rf_test,\n",
    "    name = 'R2 Random Forest',\n",
    "    mode = 'lines',\n",
    "    marker = dict(\n",
    "        color = 'green')\n",
    ")\n",
    "\n",
    "data_line.append(trace_3)\n",
    "\n",
    "trace_4 = go.Scatter(\n",
    "    x = X_line,\n",
    "    y = r2_gb_test,\n",
    "    name = 'R2 Gradient Boosting',\n",
    "    mode = 'lines',\n",
    "    marker = dict(\n",
    "        color = 'blue')\n",
    ")\n",
    "\n",
    "data_line.append(trace_4)\n",
    "\n",
    "trace_5 = go.Scatter(\n",
    "    x = X_line,\n",
    "    y = r2_er_test,\n",
    "    name = 'R2 Ensemble',\n",
    "    mode = 'lines',\n",
    "    marker = dict(\n",
    "        color = 'red')\n",
    ")\n",
    "\n",
    "data_line.append(trace_5)\n",
    "\n",
    "trace_6 = go.Scatter(\n",
    "    x = X_line,\n",
    "    y = r2_linreg_test,\n",
    "    name = 'R2 Linear Regression',\n",
    "    mode = 'lines',\n",
    "    marker = dict(\n",
    "        color = 'black')\n",
    ")\n",
    "\n",
    "#data_line.append(trace_6)\n",
    "\n",
    "\n",
    "filename = 'R2_path'\n",
    "    \n",
    "layout = go.Layout(\n",
    "    legend=dict(orientation=\"h\")\n",
    ")\n",
    "fig = dict(data = data_line, layout = layout)\n",
    "fig['layout'].update(width = 600, height = 600)\n",
    "fig['layout'].update(title = filename)\n",
    "fig['layout'].update(xaxis = dict(\n",
    "                title = 'Part of Cross Validation'))\n",
    "fig['layout'].update(yaxis = dict(\n",
    "                title = 'R2 Score',\n",
    "                tickmode = 'auto'))\n",
    "\n",
    "py.iplot(fig, filename = filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~marius92/54.embed\" height=\"600px\" width=\"600px\"></iframe>"
      ],
      "text/plain": [
       "<chart_studio.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_estimator = dataframe_seats[dataframe_seats['Estimator'].str.match('Linear Regression')]\n",
    "X_seats = data_estimator['Predicted Seats']\n",
    "y_seats = data_estimator['True Seats']\n",
    "X = sm.add_constant(X_seats)\n",
    "mse_eval_seats_lin_reg = cross_val_score(lin_reg, X, y_seats, scoring = 'neg_mean_squared_error', cv = 10, n_jobs = -1)\n",
    "r2_eval_seats_lin_reg = cross_val_score(lin_reg, X, y_seats, scoring = 'r2', cv = 10, n_jobs = -1)\n",
    "\n",
    "X_line = [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "data_line = []\n",
    "\n",
    "trace_1 = go.Scatter(\n",
    "    x = X_line,\n",
    "    y = r2_eval_seats_lin_reg,\n",
    "    name = 'R2 Linear Regression',\n",
    "    mode = 'lines',\n",
    "    marker = dict(\n",
    "        color = 'purple')\n",
    ")\n",
    "\n",
    "data_line.append(trace_1)\n",
    "\n",
    "data_estimator = dataframe_seats[dataframe_seats['Estimator'].str.match('Lasso')]\n",
    "X_seats = data_estimator['Predicted Seats']\n",
    "y_seats = data_estimator['True Seats']\n",
    "X = sm.add_constant(X_seats)\n",
    "mse_eval_seats_lasso = cross_val_score(lin_reg, X, y_seats, scoring = 'neg_mean_squared_error', cv = 10, n_jobs = -1)\n",
    "r2_eval_seats_lasso = cross_val_score(lin_reg, X, y_seats, scoring = 'r2', cv = 10, n_jobs = -1)\n",
    "\n",
    "\n",
    "trace_2 = go.Scatter(\n",
    "    x = X_line,\n",
    "    y = r2_eval_seats_lasso,\n",
    "    name = 'R2 Lasso',\n",
    "    mode = 'lines',\n",
    "    marker = dict(\n",
    "        color = 'black')\n",
    ")\n",
    "\n",
    "data_line.append(trace_2)\n",
    "\n",
    "data_estimator = dataframe_seats[dataframe_seats['Estimator'].str.match('Support Vector Machine')]\n",
    "X_seats = data_estimator['Predicted Seats']\n",
    "y_seats = data_estimator['True Seats']\n",
    "X = sm.add_constant(X_seats)\n",
    "mse_eval_seats_svm = cross_val_score(lin_reg, X, y_seats, scoring = 'neg_mean_squared_error', cv = 10, n_jobs = -1)\n",
    "r2_eval_seats_svm = cross_val_score(lin_reg, X, y_seats, scoring = 'r2', cv = 10, n_jobs = -1)\n",
    "\n",
    "trace_3 = go.Scatter(\n",
    "    x = X_line,\n",
    "    y = r2_eval_seats_svm ,\n",
    "    name = 'R2 Support Vector Machine',\n",
    "    mode = 'lines',\n",
    "    marker = dict(\n",
    "        color = 'green')\n",
    ")\n",
    "\n",
    "data_line.append(trace_3)\n",
    "\n",
    "data_estimator = dataframe_seats[dataframe_seats['Estimator'].str.match('Random Forest')]\n",
    "X_seats = data_estimator['Predicted Seats']\n",
    "y_seats = data_estimator['True Seats']\n",
    "X = sm.add_constant(X_seats)\n",
    "mse_eval_seats_rf = cross_val_score(lin_reg, X, y_seats, scoring = 'neg_mean_squared_error', cv = 10, n_jobs = -1)\n",
    "r2_eval_seats_rf = cross_val_score(lin_reg, X, y_seats, scoring = 'r2', cv = 10, n_jobs = -1)\n",
    "\n",
    "trace_4 = go.Scatter(\n",
    "    x = X_line,\n",
    "    y = r2_eval_seats_rf,\n",
    "    name = 'R2 Random Forest',\n",
    "    mode = 'lines',\n",
    "    marker = dict(\n",
    "        color = 'blue')\n",
    ")\n",
    "\n",
    "data_line.append(trace_4)\n",
    "\n",
    "data_estimator = dataframe_seats[dataframe_seats['Estimator'].str.match('Gradient Boosting')]\n",
    "X_seats = data_estimator['Predicted Seats']\n",
    "y_seats = data_estimator['True Seats']\n",
    "X = sm.add_constant(X_seats)\n",
    "mse_eval_seats_gb = cross_val_score(lin_reg, X, y_seats, scoring = 'neg_mean_squared_error', cv = 10, n_jobs = -1)\n",
    "r2_eval_seats_gb = cross_val_score(lin_reg, X, y_seats, scoring = 'r2', cv = 10, n_jobs = -1)\n",
    "\n",
    "trace_5 = go.Scatter(\n",
    "    x = X_line,\n",
    "    y = r2_eval_seats_gb,\n",
    "    name = 'R2 Gradient Boosting',\n",
    "    mode = 'lines',\n",
    "    marker = dict(\n",
    "        color = 'red')\n",
    ")\n",
    "\n",
    "data_line.append(trace_5)\n",
    "\n",
    "data_estimator = dataframe_seats[dataframe_seats['Estimator'].str.match('Ensemble')]\n",
    "X_seats = data_estimator['Predicted Seats']\n",
    "y_seats = data_estimator['True Seats']\n",
    "X = sm.add_constant(X_seats)\n",
    "mse_eval_seats_er = cross_val_score(lin_reg, X, y_seats, scoring = 'neg_mean_squared_error', cv = 10, n_jobs = -1)\n",
    "r2_eval_seats_er = cross_val_score(lin_reg, X, y_seats, scoring = 'r2', cv = 10, n_jobs = -1)\n",
    "\n",
    "trace_6 = go.Scatter(\n",
    "    x = X_line,\n",
    "    y = r2_eval_seats_er,\n",
    "    name = 'R2 Ensemble',\n",
    "    mode = 'lines',\n",
    "    marker = dict(\n",
    "        color = 'orange')\n",
    ")\n",
    "\n",
    "data_line.append(trace_6)\n",
    "\n",
    "\n",
    "filename = 'R2_path_seats'\n",
    "    \n",
    "layout = go.Layout(\n",
    "    legend=dict(orientation=\"h\")\n",
    ")\n",
    "fig = dict(data = data_line, layout = layout)\n",
    "fig['layout'].update(width = 600, height = 600)\n",
    "fig['layout'].update(title = filename)\n",
    "fig['layout'].update(xaxis = dict(\n",
    "                title = 'Part of Cross Validation'))\n",
    "fig['layout'].update(yaxis = dict(\n",
    "                title = 'R2 Score Seats',\n",
    "                tickmode = 'auto'))\n",
    "\n",
    "py.iplot(fig, filename = filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>place_list</td>\n",
       "      <td>-325.390411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>incumbent</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>woman</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doctor</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>time</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>federal_election</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>aristocracy</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>google_stan</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>population</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>share_students</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>unemployment</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>share_old</td>\n",
       "      <td>168.807480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CDU</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SPD</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Linke</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>FDP</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Grüne</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>AfD</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>share_youth</td>\n",
       "      <td>582.701926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>share_migrants</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>share_pupils</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>FW</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>local_list</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>federal_difference</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>youth_list</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>green_alt_list</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>muslim_migrant</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>non_muslim_migrant</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>double_name</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>first_time</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>first_time_place_list</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>first_time_incumbent</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>first_time_woman</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>first_time_doctor</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>first_time_time</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>first_time_federal_election</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>first_time_aristocracy</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>first_time_google_stan</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>first_time_population</td>\n",
       "      <td>-2.865572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>first_time_share_students</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>first_time_unemployment</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>first_time_share_old</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>first_time_CDU</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>first_time_SPD</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>first_time_Linke</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>first_time_FDP</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>first_time_Grüne</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>first_time_AfD</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>first_time_share_youth</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919</th>\n",
       "      <td>first_time_share_migrants</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920</th>\n",
       "      <td>first_time_share_pupils</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>first_time_FW</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>first_time_local_list</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>first_time_federal_difference</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>first_time_youth_list</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>first_time_green_alt_list</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>first_time_muslim_migrant</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>first_time_non_muslim_migrant</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928</th>\n",
       "      <td>first_time_double_name</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>first_time_square</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>930 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Variable  Coefficient\n",
       "0                       place_list  -325.390411\n",
       "1                        incumbent     0.000000\n",
       "2                            woman     0.000000\n",
       "3                           doctor     0.000000\n",
       "4                             time    -0.000000\n",
       "5                 federal_election     0.000000\n",
       "6                      aristocracy    -0.000000\n",
       "7                      google_stan    -0.000000\n",
       "8                       population    -0.000000\n",
       "9                   share_students    -0.000000\n",
       "10                    unemployment     0.000000\n",
       "11                       share_old   168.807480\n",
       "12                             CDU     0.000000\n",
       "13                             SPD     0.000000\n",
       "14                           Linke    -0.000000\n",
       "15                             FDP    -0.000000\n",
       "16                           Grüne     0.000000\n",
       "17                             AfD    -0.000000\n",
       "18                     share_youth   582.701926\n",
       "19                  share_migrants    -0.000000\n",
       "20                    share_pupils     0.000000\n",
       "21                              FW     0.000000\n",
       "22                      local_list    -0.000000\n",
       "23              federal_difference    -0.000000\n",
       "24                      youth_list    -0.000000\n",
       "25                  green_alt_list    -0.000000\n",
       "26                  muslim_migrant    -0.000000\n",
       "27              non_muslim_migrant    -0.000000\n",
       "28                     double_name    -0.000000\n",
       "29                      first_time    -0.000000\n",
       "..                             ...          ...\n",
       "900          first_time_place_list    -0.000000\n",
       "901           first_time_incumbent    -0.000000\n",
       "902               first_time_woman    -0.000000\n",
       "903              first_time_doctor     0.000000\n",
       "904                first_time_time    -0.000000\n",
       "905    first_time_federal_election    -0.000000\n",
       "906         first_time_aristocracy     0.000000\n",
       "907         first_time_google_stan    -0.000000\n",
       "908          first_time_population    -2.865572\n",
       "909      first_time_share_students    -0.000000\n",
       "910        first_time_unemployment    -0.000000\n",
       "911           first_time_share_old    -0.000000\n",
       "912                 first_time_CDU     0.000000\n",
       "913                 first_time_SPD     0.000000\n",
       "914               first_time_Linke     0.000000\n",
       "915                 first_time_FDP     0.000000\n",
       "916               first_time_Grüne     0.000000\n",
       "917                 first_time_AfD    -0.000000\n",
       "918         first_time_share_youth    -0.000000\n",
       "919      first_time_share_migrants    -0.000000\n",
       "920        first_time_share_pupils    -0.000000\n",
       "921                  first_time_FW     0.000000\n",
       "922          first_time_local_list    -0.000000\n",
       "923  first_time_federal_difference     0.000000\n",
       "924          first_time_youth_list     0.000000\n",
       "925      first_time_green_alt_list     0.000000\n",
       "926      first_time_muslim_migrant    -0.000000\n",
       "927  first_time_non_muslim_migrant    -0.000000\n",
       "928         first_time_double_name    -0.000000\n",
       "929              first_time_square    -0.000000\n",
       "\n",
       "[930 rows x 2 columns]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficients_lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~marius92/54.embed\" height=\"525\" width=\"100%\"></iframe>'"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tls.get_embed('https://plot.ly/~marius92/54')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
